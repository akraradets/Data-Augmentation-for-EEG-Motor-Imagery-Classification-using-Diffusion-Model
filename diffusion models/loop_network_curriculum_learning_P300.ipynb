{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nutapolt/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from braindecode.preprocessing import (\n",
    "    exponential_moving_standardize,\n",
    "    preprocess,\n",
    "    Preprocessor)\n",
    "from numpy import multiply\n",
    "from braindecode.datasets import MOABBDataset\n",
    "from braindecode.preprocessing import create_windows_from_events\n",
    "import torch\n",
    "from braindecode.util import set_random_seeds\n",
    "\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from braindecode.visualization import plot_confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import os\n",
    "import sys\n",
    "from mne.datasets import eegbci\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from mne.datasets import eegbci\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import wandb\n",
    "import os\n",
    "import neptune.new.integrations.sklearn as npt_utils\n",
    "import neptune.new as neptune\n",
    "\n",
    "\n",
    "\n",
    "os.environ['http_proxy'] = \"http://192.41.170.23:3128\"\n",
    "os.environ['https_proxy'] = \"http://192.41.170.23:3128\"\n",
    "\n",
    "from braindecode.models import ShallowFBCSPNet, Deep4Net ,HybridNet,EEGInceptionMI,EEGITNet,ATCNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def create_dataloader(X, y, batch_size):\n",
    "    X_tensor = torch.tensor(X).float()\n",
    "    y_tensor = torch.tensor(y).long()\n",
    "    dataset_tensor = TensorDataset(X_tensor, y_tensor)\n",
    "    dl = torch.utils.data.DataLoader(dataset_tensor, batch_size=batch_size, shuffle=True)\n",
    "    return dl\n",
    "\n",
    "def early_stopping(train_loss, validation_loss, min_delta, tolerance):\n",
    "    counter = 0\n",
    "    if (validation_loss - train_loss) > min_delta:\n",
    "        counter +=1\n",
    "        if counter >= tolerance:\n",
    "          return True\n",
    "\n",
    "def extrack_dataset(dataset):\n",
    "    for x, y, window_ind in dataset:\n",
    "        x_shape = x.shape\n",
    "        y_shape = len(dataset.get_metadata().target)\n",
    "        break\n",
    "    X = np.zeros((y_shape,x_shape[0],x_shape[1]))\n",
    "    y_=[]\n",
    "    i=0\n",
    "    for x, y, window_ind in dataset:\n",
    "        X[i]=x\n",
    "        y_.append(y)\n",
    "        i+=1\n",
    "    #X2 = X[:, 7:8, :]\n",
    "    #X3= X[:, 11:12, :]\n",
    "    #(288, 22, 1125)\n",
    "    #X = np.concatenate((X2,X3), axis=1)\n",
    "    print(X.shape)\n",
    "\n",
    "    #X = X.reshape(y_shape,X.shape[2],X.shape[1])\n",
    "    return X,np.array(y_).T\n",
    "\n",
    "def extrack_dataset_types(dataset,types = 'NonTarget'):\n",
    "    for x, y, window_ind in dataset:\n",
    "\n",
    "        x_shape = x.shape\n",
    "        y_shape = len(dataset.get_metadata().target)\n",
    "        break\n",
    "    X = np.zeros((y_shape,x_shape[0],x_shape[1]))\n",
    "    y_re = np.zeros(())\n",
    "    y_=[]\n",
    "    i=0\n",
    "    for x, y, window_ind in dataset:\n",
    "        X[i]=x\n",
    "        y_.append(y)\n",
    "        i+=1\n",
    "    y = np.array(y_).T\n",
    "\n",
    "    if types == \"NonTarget\":\n",
    "        X = X[y == 0]\n",
    "    if types == \"Target\":\n",
    "        X = X[y == 1]\n",
    "    y_re = np.zeros((X.shape[0],))\n",
    "    return X,y_re\n",
    "\n",
    "def extrack_dataset_2class(dataset):\n",
    "    for x, y, window_ind in dataset:\n",
    "        x_shape = x.shape\n",
    "        y_shape = len(dataset.get_metadata().target)\n",
    "        break\n",
    "    X = np.zeros((y_shape,x_shape[0],x_shape[1]))\n",
    "    y_=[]\n",
    "    i=0\n",
    "    for x, y, window_ind in dataset:\n",
    "        X[i]=x\n",
    "        y_.append(y)\n",
    "        i+=1\n",
    "    y = np.array(y_).T\n",
    "    X_ = np.zeros((int(y_shape/2) ,x_shape[0],x_shape[1])) #for 2 class\n",
    "    y_for_2class =[]\n",
    "    j=0\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == 0 or y[i] == 1:\n",
    "            y_for_2class.append(y[i])\n",
    "            X_[j]=X[i]\n",
    "            j +=1\n",
    "    y_for_2class = np.array(y_for_2class).T\n",
    "    X_ = X_.reshape(y_for_2class.shape[0],X_.shape[2],X_.shape[1])\n",
    "    return X_,y_for_2class\n",
    "\n",
    "def extrack_dataset_2class_cut(dataset):\n",
    "    for x, y, window_ind in dataset:\n",
    "        x_shape = x.shape\n",
    "        y_shape = len(dataset.get_metadata().target)\n",
    "        break\n",
    "    X = np.zeros((y_shape,x_shape[0],x_shape[1]))\n",
    "    y_=[]\n",
    "    i=0\n",
    "    for x, y, window_ind in dataset:\n",
    "        X[i]=x\n",
    "        y_.append(y)\n",
    "        i+=1\n",
    "    y = np.array(y_).T\n",
    "    X_ = np.zeros((int(y_shape/2) ,x_shape[0],x_shape[1])) #for 2 class\n",
    "    y_for_2class =[]\n",
    "    j=0\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == 0 or y[i] == 1:\n",
    "            y_for_2class.append(y[i])\n",
    "            X_[j]=X[i]\n",
    "            j +=1\n",
    "    y_for_2class = np.array(y_for_2class).T\n",
    "    X_ = X_.reshape(y_for_2class.shape[0],X_.shape[2],X_.shape[1])\n",
    "    #100, 1, 576, 22\n",
    "    X_ = X_[:,64:320,:]\n",
    "    print(X_.shape)\n",
    "    return X_,y_for_2class\n",
    "\n",
    "\n",
    "def train(model,gpu_num,train_loader,test_loader,\n",
    "          weights_name=False,\n",
    "          optimizer = None,\n",
    "          criterion = None,\n",
    "          num_epochs=500,\n",
    "          vail_loader= None,\n",
    "          save_weights = False,\n",
    "          neptune = True,\n",
    "          lr = None\n",
    "         ):\n",
    "\n",
    "    # Train the model\n",
    "\n",
    "\n",
    "    train_loss = []\n",
    "    valid_loss = [10,11]\n",
    "    train_accuracy = []\n",
    "    valid_accuracy = []\n",
    "\n",
    "    old_loss = 100\n",
    "    old_acc = 0\n",
    "    valid_loss_vail = []\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        iter_loss = 0.0\n",
    "        correct = 0\n",
    "        iterations = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for i, (items, classes) in enumerate(train_loader):\n",
    "            items = Variable(items)\n",
    "            classes = classes.type(torch.LongTensor)\n",
    "            classes = Variable(classes)\n",
    "\n",
    "            if cuda.is_available():\n",
    "                items = items.cuda(gpu_num)\n",
    "                classes = classes.cuda(gpu_num)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(items)\n",
    "            #avg_pedic = torch.mean(outputs, 1, True).float()\n",
    "            #avg_pedic = avg_pedic.reshape(classes.shape[0])\n",
    "            #print(avg_pedic.shape)\n",
    "            #print(classes.shape)\n",
    "            loss = criterion(outputs, classes)\n",
    "\n",
    "            iter_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            metrics = {\"train/train_loss\": loss}\n",
    "\n",
    "            #print(loss)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == classes.data).sum()\n",
    "            iterations += 1\n",
    "\n",
    "        train_loss.append(iter_loss/iterations)\n",
    "\n",
    "\n",
    "        train_accuracy.append(( correct.float() / len(train_loader.dataset)))\n",
    "        train_metrics = {\"train/train_loss\": iter_loss/iterations,\n",
    "                       \"train/train_accuracy\": (100 * correct.float() / len(train_loader.dataset))}\n",
    "\n",
    "\n",
    "\n",
    "        loss = 0.0\n",
    "        correct = 0\n",
    "        iterations = 0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for i, (items, classes) in enumerate(test_loader):\n",
    "            classes = classes.type(torch.LongTensor)\n",
    "            items = Variable(items)\n",
    "            classes = Variable(classes)\n",
    "\n",
    "            if cuda.is_available():\n",
    "                items = items.cuda(gpu_num)\n",
    "                classes = classes.cuda(gpu_num)\n",
    "\n",
    "\n",
    "            outputs = model(items)\n",
    "\n",
    "            #avg_pedic = torch.mean(outputs, 1, True).float()\n",
    "            #avg_pedic = avg_pedic.reshape(classes.shape[0])\n",
    "\n",
    "            loss += criterion(outputs, classes).item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            correct += (predicted == classes.data).sum()\n",
    "            #print(\"correct : {}\".format(classes.data))\n",
    "            #print(\"predicted : {}\".format(predicted))\n",
    "            iterations += 1\n",
    "\n",
    "        valid_loss.append(loss/iterations)\n",
    "        correct_scalar = np.array([correct.clone().cpu()])[0]\n",
    "        valid_accuracy.append(correct_scalar / len(test_loader.dataset) )\n",
    "\n",
    "        test_metrics = {\"Test/Test_loss\": loss/iterations,\n",
    "                       \"Test/Test_accuracy\": correct_scalar / len(test_loader.dataset) }\n",
    "\n",
    "        if save_weights is True:\n",
    "            if epoch+1 > 2 and valid_loss[-1] < old_loss and old_acc <= valid_accuracy[-1] :\n",
    "                    newpath = r'./{}'.format(weights_name)\n",
    "                    if not os.path.exists(newpath):\n",
    "                        os.makedirs(newpath)\n",
    "                    torch.save(model.state_dict(),'./{}/{:.4f}_{}_{:.4f}_{:.4f}'.format(weights_name,valid_loss[-1],weights_name,valid_loss[-1],valid_accuracy[-1]))\n",
    "                    part_weights = './{}/{:.4f}_{}_{:.4f}_{:.4f}'.format(weights_name,valid_loss[-1],weights_name,valid_loss[-1],valid_accuracy[-1])\n",
    "                    old_loss = valid_loss[-1]\n",
    "                    old_acc = valid_accuracy[-1]\n",
    "\n",
    "        print ('Epoch %d/%d, Tr Loss: %.4f, Tr Acc: %.4f, Val Loss: %.4f, Val Acc: %.4f , le : %f'\n",
    "                       %(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], valid_loss[-1], valid_accuracy[-1],lr))\n",
    "        if early_stopping(train_loss[-1], valid_loss[-1], min_delta=10, tolerance = 20):\n",
    "            print(\"We are at epoch:\", epoch+1)\n",
    "            run[f\"epoch/valid_accuracy\"].append(valid_accuracy[-1])\n",
    "            run[f\"epoch/train_accuracy\"].append(train_accuracy[-1])\n",
    "            run[f\"epoch/train_loss\"].append(train_loss[-1])\n",
    "            run[f\"epoch/valid_loss\"].append(valid_loss[-1])\n",
    "\n",
    "            break\n",
    "        if neptune is True:\n",
    "            run[f\"epoch/valid_accuracy\"].append(valid_accuracy[-1])\n",
    "            run[f\"epoch/train_accuracy\"].append(train_accuracy[-1])\n",
    "            run[f\"epoch/train_loss\"].append(train_loss[-1])\n",
    "            run[f\"epoch/valid_loss\"].append(valid_loss[-1])\n",
    "        if epoch+1 == 1:\n",
    "            stop_loss = valid_loss[-1]\n",
    "\n",
    "        if  (epoch+1)//500 == 0 and epoch+1 > 1400 :\n",
    "            if valid_loss[-1] > valid_loss[-500]:\n",
    "                print(\"Stop\")\n",
    "                break\n",
    "\n",
    "    return train_loss,valid_loss,train_accuracy,valid_accuracy,part_weights\n",
    "def eval(model,\n",
    "         gpu_num,\n",
    "          valid_loader= None,\n",
    "         labels=None,\n",
    "\n",
    "         ):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    correct=0\n",
    "    evaluate_accuracy= []\n",
    "    for i, (items, classes) in enumerate(valid_loader):\n",
    "        classes = classes.type(torch.LongTensor)\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        if cuda.is_available():\n",
    "            items = items.cuda(gpu_num)\n",
    "            classes = classes.cuda(gpu_num)\n",
    "\n",
    "        outputs = model(items)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.append(predicted.clone().cpu().numpy())\n",
    "        y_true.append(classes.data.clone().cpu().numpy())\n",
    "        correct += (predicted == classes.data).sum()\n",
    "    correct_scalar = np.array([correct.clone().cpu()])[0]\n",
    "    evaluate_accuracy.append(correct_scalar / classes.shape[0] )\n",
    "\n",
    "    confusion_mat = confusion_matrix(np.array(y_true).T,np.array(y_pred).T )\n",
    "    NonTarget_eva_acc = confusion_mat[0][0]\n",
    "    Target_eva_acc = confusion_mat[1][1]\n",
    "\n",
    "\n",
    "    run[f\"NonTarget_eva_acc\"].append(NonTarget_eva_acc/480)\n",
    "    run[f\"Target_eva_acc\"].append(Target_eva_acc/96)\n",
    "\n",
    "\n",
    "\n",
    "    run[f\"epoch/eval_ACC\"].append(evaluate_accuracy[-1])\n",
    "    run[\"confusion matrices subject_id : {0}\".format(subject_id)].upload(plot_confusion_matrix(confusion_mat, class_names=labels,rotate_row_labels=0,rotate_col_labels=90,with_f1_score=True))\n",
    "    plot_confusion_matrix(confusion_mat, class_names=labels,rotate_row_labels=0,rotate_col_labels=90,with_f1_score=True).savefig('confusion_matrix/confusion matrices subject_id : {2}_{0}_of_{1}.png'.format(subject_id,params['Datasets'],network))\n",
    "    return y_pred,y_true,correct_scalar,evaluate_accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = 332\n",
    "\n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, 16), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "\n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "\n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "\n",
    "        # FC Layer\n",
    "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "        # I have 120 timepoints.\n",
    "        self.fc1 = nn.Linear(168, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = self.pooling2(x)\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = self.pooling3(x)\n",
    "\n",
    "        # FC Layer\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nutapolt/.local/lib/python3.8/site-packages/braindecode/preprocessing/preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
      "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
     ]
    }
   ],
   "source": [
    "subjects = list(range(1,11))\n",
    "\n",
    "low_cut_hz = 8.  # low cut frequency for filtering\n",
    "high_cut_hz = 35.  # high cut frequency for filtering\n",
    "resample = 250\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "# Factor to convert from V to uV\n",
    "factor = 1e6\n",
    "preprocessors = [\n",
    "                    Preprocessor('pick_types', eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
    "                    Preprocessor(lambda data: multiply(data, factor)),  # Convert from V to uV\n",
    "                    Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n",
    "                    #Preprocessor('resample', sfreq=resample),\n",
    "                    Preprocessor(exponential_moving_standardize,  # Exponential movin standardization\n",
    "                                factor_new=factor_new,\n",
    "                                 init_block_size=init_block_size)\n",
    "                    ]\n",
    "n_epochs = 2000\n",
    "lr = 0.0001\n",
    "\n",
    "weight_decay = 0.5 * 0.0001\n",
    "\n",
    "#task_list = ['foot'] ['ATCNet','EEGITNet','ShallowFBCSPNet',]\n",
    "network_list = [\"EEGnet\"]\n",
    "n_classes=2\n",
    "#network_list = ['Deep4Net']\n",
    "#percent_list = [72]\n",
    "percent_list = [18,36,54,72]\n",
    "gpu_num = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/AitBrainLab/P300/e/P-47\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "576 events found\n",
      "Event IDs: [1 2]\n",
      "576 events found\n",
      "Event IDs: [ 3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "576 events found\n",
      "Event IDs: [1 2]\n",
      "576 events found\n",
      "Event IDs: [ 3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "576 events found\n",
      "Event IDs: [1 2]\n",
      "576 events found\n",
      "Event IDs: [ 3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 423 samples (1.652 sec)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 423 samples (1.652 sec)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 423 samples (1.652 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256.0\n",
      "Used Annotations descriptions: ['NonTarget', 'Target']\n",
      "Used Annotations descriptions: ['NonTarget', 'Target']\n",
      "Used Annotations descriptions: ['NonTarget', 'Target']\n",
      "(576, 16, 332)\n",
      "(192, 16, 332)\n",
      "(192,)\n",
      "(192, 1, 332, 16)\n",
      "(576, 1, 332, 16)\n",
      "train size (134, 1, 332, 16) (134,)\n",
      "test size (58, 1, 332, 16) (58,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nutapolt/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000, Tr Loss: 0.5500, Tr Acc: 0.9701, Val Loss: 0.7043, Val Acc: 0.2241 , le : 0.000100\n",
      "Epoch 2/2000, Tr Loss: 0.5306, Tr Acc: 0.9851, Val Loss: 0.6950, Val Acc: 0.4655 , le : 0.000100\n",
      "Epoch 3/2000, Tr Loss: 0.5323, Tr Acc: 0.9776, Val Loss: 0.6914, Val Acc: 0.5517 , le : 0.000100\n",
      "Epoch 4/2000, Tr Loss: 0.5302, Tr Acc: 0.9776, Val Loss: 0.6850, Val Acc: 0.6552 , le : 0.000100\n",
      "Epoch 5/2000, Tr Loss: 0.5194, Tr Acc: 0.9925, Val Loss: 0.6769, Val Acc: 0.7759 , le : 0.000100\n",
      "Epoch 6/2000, Tr Loss: 0.5118, Tr Acc: 1.0000, Val Loss: 0.6715, Val Acc: 0.8103 , le : 0.000100\n",
      "Epoch 7/2000, Tr Loss: 0.5035, Tr Acc: 1.0000, Val Loss: 0.6620, Val Acc: 0.8966 , le : 0.000100\n",
      "Epoch 8/2000, Tr Loss: 0.5042, Tr Acc: 1.0000, Val Loss: 0.6520, Val Acc: 0.9483 , le : 0.000100\n",
      "Epoch 9/2000, Tr Loss: 0.4979, Tr Acc: 0.9925, Val Loss: 0.6520, Val Acc: 0.9483 , le : 0.000100\n",
      "Epoch 10/2000, Tr Loss: 0.4945, Tr Acc: 1.0000, Val Loss: 0.6423, Val Acc: 0.9655 , le : 0.000100\n",
      "Epoch 11/2000, Tr Loss: 0.4858, Tr Acc: 1.0000, Val Loss: 0.6283, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 12/2000, Tr Loss: 0.4919, Tr Acc: 0.9776, Val Loss: 0.6180, Val Acc: 0.9828 , le : 0.000100\n",
      "Epoch 13/2000, Tr Loss: 0.4805, Tr Acc: 1.0000, Val Loss: 0.6087, Val Acc: 0.9828 , le : 0.000100\n",
      "Epoch 14/2000, Tr Loss: 0.4751, Tr Acc: 1.0000, Val Loss: 0.6005, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 15/2000, Tr Loss: 0.4717, Tr Acc: 1.0000, Val Loss: 0.5881, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 16/2000, Tr Loss: 0.4678, Tr Acc: 1.0000, Val Loss: 0.5740, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 17/2000, Tr Loss: 0.4702, Tr Acc: 1.0000, Val Loss: 0.5667, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 18/2000, Tr Loss: 0.4617, Tr Acc: 1.0000, Val Loss: 0.5555, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 19/2000, Tr Loss: 0.4466, Tr Acc: 1.0000, Val Loss: 0.5465, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 20/2000, Tr Loss: 0.4510, Tr Acc: 1.0000, Val Loss: 0.5376, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 21/2000, Tr Loss: 0.4421, Tr Acc: 1.0000, Val Loss: 0.5308, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 22/2000, Tr Loss: 0.4446, Tr Acc: 1.0000, Val Loss: 0.5249, Val Acc: 0.9828 , le : 0.000100\n",
      "Epoch 23/2000, Tr Loss: 0.4392, Tr Acc: 1.0000, Val Loss: 0.5153, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 24/2000, Tr Loss: 0.4417, Tr Acc: 1.0000, Val Loss: 0.4972, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 25/2000, Tr Loss: 0.4387, Tr Acc: 1.0000, Val Loss: 0.4986, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 26/2000, Tr Loss: 0.4318, Tr Acc: 1.0000, Val Loss: 0.4904, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 27/2000, Tr Loss: 0.4248, Tr Acc: 1.0000, Val Loss: 0.4841, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 28/2000, Tr Loss: 0.4311, Tr Acc: 1.0000, Val Loss: 0.4693, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 29/2000, Tr Loss: 0.4190, Tr Acc: 1.0000, Val Loss: 0.4723, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 30/2000, Tr Loss: 0.4169, Tr Acc: 1.0000, Val Loss: 0.4578, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 31/2000, Tr Loss: 0.4154, Tr Acc: 1.0000, Val Loss: 0.4484, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 32/2000, Tr Loss: 0.4144, Tr Acc: 1.0000, Val Loss: 0.4519, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 33/2000, Tr Loss: 0.4081, Tr Acc: 1.0000, Val Loss: 0.4427, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 34/2000, Tr Loss: 0.4071, Tr Acc: 1.0000, Val Loss: 0.4380, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 35/2000, Tr Loss: 0.4080, Tr Acc: 1.0000, Val Loss: 0.4321, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 36/2000, Tr Loss: 0.4009, Tr Acc: 1.0000, Val Loss: 0.4244, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 37/2000, Tr Loss: 0.3948, Tr Acc: 1.0000, Val Loss: 0.4207, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 38/2000, Tr Loss: 0.4001, Tr Acc: 1.0000, Val Loss: 0.4189, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 39/2000, Tr Loss: 0.3980, Tr Acc: 1.0000, Val Loss: 0.4113, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 40/2000, Tr Loss: 0.3933, Tr Acc: 1.0000, Val Loss: 0.4063, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 41/2000, Tr Loss: 0.3883, Tr Acc: 1.0000, Val Loss: 0.4051, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 42/2000, Tr Loss: 0.3894, Tr Acc: 1.0000, Val Loss: 0.4028, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 43/2000, Tr Loss: 0.3938, Tr Acc: 1.0000, Val Loss: 0.3993, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 44/2000, Tr Loss: 0.3869, Tr Acc: 1.0000, Val Loss: 0.3969, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 45/2000, Tr Loss: 0.3863, Tr Acc: 1.0000, Val Loss: 0.3975, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 46/2000, Tr Loss: 0.3838, Tr Acc: 1.0000, Val Loss: 0.3876, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 47/2000, Tr Loss: 0.3833, Tr Acc: 1.0000, Val Loss: 0.3909, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 48/2000, Tr Loss: 0.3825, Tr Acc: 1.0000, Val Loss: 0.3858, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 49/2000, Tr Loss: 0.3794, Tr Acc: 1.0000, Val Loss: 0.3865, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 50/2000, Tr Loss: 0.3753, Tr Acc: 1.0000, Val Loss: 0.3847, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 51/2000, Tr Loss: 0.3753, Tr Acc: 1.0000, Val Loss: 0.3802, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 52/2000, Tr Loss: 0.3739, Tr Acc: 1.0000, Val Loss: 0.3862, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 53/2000, Tr Loss: 0.3752, Tr Acc: 1.0000, Val Loss: 0.3847, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 54/2000, Tr Loss: 0.3744, Tr Acc: 1.0000, Val Loss: 0.3805, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 55/2000, Tr Loss: 0.3724, Tr Acc: 1.0000, Val Loss: 0.3777, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 56/2000, Tr Loss: 0.3702, Tr Acc: 1.0000, Val Loss: 0.3724, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 57/2000, Tr Loss: 0.3674, Tr Acc: 1.0000, Val Loss: 0.3748, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 58/2000, Tr Loss: 0.3709, Tr Acc: 1.0000, Val Loss: 0.3737, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 59/2000, Tr Loss: 0.3660, Tr Acc: 1.0000, Val Loss: 0.3737, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 60/2000, Tr Loss: 0.3683, Tr Acc: 1.0000, Val Loss: 0.3706, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 61/2000, Tr Loss: 0.3656, Tr Acc: 1.0000, Val Loss: 0.3756, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 62/2000, Tr Loss: 0.3632, Tr Acc: 1.0000, Val Loss: 0.3672, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 63/2000, Tr Loss: 0.3649, Tr Acc: 1.0000, Val Loss: 0.3635, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 64/2000, Tr Loss: 0.3620, Tr Acc: 1.0000, Val Loss: 0.3658, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 65/2000, Tr Loss: 0.3628, Tr Acc: 1.0000, Val Loss: 0.3587, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 66/2000, Tr Loss: 0.3588, Tr Acc: 1.0000, Val Loss: 0.3667, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 67/2000, Tr Loss: 0.3589, Tr Acc: 1.0000, Val Loss: 0.3618, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 68/2000, Tr Loss: 0.3590, Tr Acc: 1.0000, Val Loss: 0.3587, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 69/2000, Tr Loss: 0.3571, Tr Acc: 1.0000, Val Loss: 0.3639, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 70/2000, Tr Loss: 0.3561, Tr Acc: 1.0000, Val Loss: 0.3557, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 71/2000, Tr Loss: 0.3575, Tr Acc: 1.0000, Val Loss: 0.3603, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 72/2000, Tr Loss: 0.3528, Tr Acc: 1.0000, Val Loss: 0.3537, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 73/2000, Tr Loss: 0.3553, Tr Acc: 1.0000, Val Loss: 0.3547, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 74/2000, Tr Loss: 0.3515, Tr Acc: 1.0000, Val Loss: 0.3515, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 75/2000, Tr Loss: 0.3525, Tr Acc: 1.0000, Val Loss: 0.3536, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 76/2000, Tr Loss: 0.3508, Tr Acc: 1.0000, Val Loss: 0.3537, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 77/2000, Tr Loss: 0.3489, Tr Acc: 1.0000, Val Loss: 0.3543, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 78/2000, Tr Loss: 0.3506, Tr Acc: 1.0000, Val Loss: 0.3534, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 79/2000, Tr Loss: 0.3516, Tr Acc: 1.0000, Val Loss: 0.3497, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 80/2000, Tr Loss: 0.3497, Tr Acc: 1.0000, Val Loss: 0.3486, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 81/2000, Tr Loss: 0.3472, Tr Acc: 1.0000, Val Loss: 0.3501, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 82/2000, Tr Loss: 0.3487, Tr Acc: 1.0000, Val Loss: 0.3490, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 83/2000, Tr Loss: 0.3447, Tr Acc: 1.0000, Val Loss: 0.3497, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 84/2000, Tr Loss: 0.3466, Tr Acc: 1.0000, Val Loss: 0.3449, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 85/2000, Tr Loss: 0.3465, Tr Acc: 1.0000, Val Loss: 0.3489, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 86/2000, Tr Loss: 0.3449, Tr Acc: 1.0000, Val Loss: 0.3462, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 87/2000, Tr Loss: 0.3476, Tr Acc: 1.0000, Val Loss: 0.3491, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 88/2000, Tr Loss: 0.3429, Tr Acc: 1.0000, Val Loss: 0.3433, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 89/2000, Tr Loss: 0.3437, Tr Acc: 1.0000, Val Loss: 0.3463, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 90/2000, Tr Loss: 0.3424, Tr Acc: 1.0000, Val Loss: 0.3470, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 91/2000, Tr Loss: 0.3418, Tr Acc: 1.0000, Val Loss: 0.3475, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 92/2000, Tr Loss: 0.3428, Tr Acc: 1.0000, Val Loss: 0.3445, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 93/2000, Tr Loss: 0.3405, Tr Acc: 1.0000, Val Loss: 0.3413, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 94/2000, Tr Loss: 0.3417, Tr Acc: 1.0000, Val Loss: 0.3426, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 95/2000, Tr Loss: 0.3403, Tr Acc: 1.0000, Val Loss: 0.3451, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 96/2000, Tr Loss: 0.3417, Tr Acc: 1.0000, Val Loss: 0.3424, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 97/2000, Tr Loss: 0.3406, Tr Acc: 1.0000, Val Loss: 0.3426, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 98/2000, Tr Loss: 0.3395, Tr Acc: 1.0000, Val Loss: 0.3388, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 99/2000, Tr Loss: 0.3400, Tr Acc: 1.0000, Val Loss: 0.3415, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 100/2000, Tr Loss: 0.3378, Tr Acc: 1.0000, Val Loss: 0.3434, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 101/2000, Tr Loss: 0.3386, Tr Acc: 1.0000, Val Loss: 0.3404, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 102/2000, Tr Loss: 0.3391, Tr Acc: 1.0000, Val Loss: 0.3414, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 103/2000, Tr Loss: 0.3362, Tr Acc: 1.0000, Val Loss: 0.3423, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 104/2000, Tr Loss: 0.3376, Tr Acc: 1.0000, Val Loss: 0.3388, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 105/2000, Tr Loss: 0.3361, Tr Acc: 1.0000, Val Loss: 0.3382, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 106/2000, Tr Loss: 0.3356, Tr Acc: 1.0000, Val Loss: 0.3398, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 107/2000, Tr Loss: 0.3365, Tr Acc: 1.0000, Val Loss: 0.3392, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 108/2000, Tr Loss: 0.3372, Tr Acc: 1.0000, Val Loss: 0.3365, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 109/2000, Tr Loss: 0.3359, Tr Acc: 1.0000, Val Loss: 0.3361, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 110/2000, Tr Loss: 0.3358, Tr Acc: 1.0000, Val Loss: 0.3368, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 111/2000, Tr Loss: 0.3358, Tr Acc: 1.0000, Val Loss: 0.3357, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 112/2000, Tr Loss: 0.3352, Tr Acc: 1.0000, Val Loss: 0.3374, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 113/2000, Tr Loss: 0.3343, Tr Acc: 1.0000, Val Loss: 0.3367, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 114/2000, Tr Loss: 0.3344, Tr Acc: 1.0000, Val Loss: 0.3366, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 115/2000, Tr Loss: 0.3344, Tr Acc: 1.0000, Val Loss: 0.3351, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 116/2000, Tr Loss: 0.3345, Tr Acc: 1.0000, Val Loss: 0.3335, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 117/2000, Tr Loss: 0.3342, Tr Acc: 1.0000, Val Loss: 0.3335, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 118/2000, Tr Loss: 0.3322, Tr Acc: 1.0000, Val Loss: 0.3343, Val Acc: 1.0000 , le : 0.000100\n",
      "Epoch 119/2000, Tr Loss: 0.3336, Tr Acc: 1.0000, Val Loss: 0.3352, Val Acc: 1.0000 , le : 0.000100\n"
     ]
    }
   ],
   "source": [
    "for subject_id in subjects:\n",
    "    for network in network_list:\n",
    "        run = neptune.init_run(\n",
    "project=\"AitBrainLab/P300\",\n",
    "api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhMTMyMzg0My02NzlhLTQ3N2ItYTdmMS0yNTcwNDBmM2QwM2QifQ==\",\n",
    ")\n",
    "\n",
    "        #torch.manual_seed(3407)\n",
    "        dataset = MOABBDataset(dataset_name=\"BNCI2014009\", subject_ids= [subject_id])\n",
    "        preprocess(dataset, preprocessors)\n",
    "        trial_start_offset_seconds = -0.5\n",
    "        # Extract sampling frequency, check that they are same in all datasets\n",
    "        sfreq = dataset.datasets[0].raw.info['sfreq']\n",
    "        print(sfreq)\n",
    "        assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
    "        # Calculate the trial start offset in samples.\n",
    "        trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
    "\n",
    "        # Create windows using braindecode function for this. It needs parameters to define how\n",
    "        # trials should be used.\n",
    "        windows_dataset = create_windows_from_events(\n",
    "            dataset,\n",
    "            trial_start_offset_samples=trial_start_offset_samples,\n",
    "            trial_stop_offset_samples=0,\n",
    "            preload=True,\n",
    "        )\n",
    "        splitted = windows_dataset.split('session')\n",
    "        #session_0 = splitted['session_0']\n",
    "        train_set = splitted['session_0']\n",
    "        valid_set = splitted['session_0']\n",
    "\n",
    "        input_window_samples = train_set[0][0].shape[1]\n",
    "\n",
    "        #X_train = X_train[:, np.newaxis,:,:]\n",
    "\n",
    "        X_valid,y_valid = extrack_dataset(valid_set)\n",
    "        X_train_NonTarget,y_train_NonTarget = extrack_dataset_types(train_set,types = 'NonTarget')\n",
    "        X_train_Target,y_train_Target = extrack_dataset_types(train_set,types = 'Target')\n",
    "        ran = np.random.randint(0,480,96)\n",
    "        X1 = X_train_NonTarget[ran]\n",
    "        y1 = y_train_NonTarget[ran]\n",
    "\n",
    "\n",
    "        X_train = np.concatenate((X_train_Target,X1),axis = 0)\n",
    "        print(X_train.shape)\n",
    "        y_train = np.concatenate((y_train_Target,y1),axis = 0)\n",
    "        print(y_train.shape)\n",
    "\n",
    "\n",
    "        #\n",
    "\n",
    "        if network == \"EEGnet\":\n",
    "            X_train = X_train.reshape(X_train.shape[0],X_train.shape[2],X_train.shape[1])\n",
    "            X_train = X_train[:, np.newaxis,:,:]\n",
    "            print(X_train.shape)\n",
    "            X_valid = X_valid.reshape(X_valid.shape[0],X_valid.shape[2],X_valid.shape[1])\n",
    "            X_valid = X_valid[:, np.newaxis,:,:]\n",
    "            model = EEGNet()\n",
    "            print(X_valid.shape)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_train,y_train, test_size=0.3,stratify=y_train)\n",
    "        label_dict = valid_set.datasets[0].windows.event_id.items()\n",
    "        labels = list(dict(sorted(list(label_dict), key=lambda kv: kv[1])).keys())\n",
    "        print('train size',X_train.shape, y_train.shape)\n",
    "        print('test size',X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "        train_loader = create_dataloader(X_train, y_train, batch_size=576)\n",
    "        test_loader = create_dataloader(X_test, y_test, batch_size=576)\n",
    "        valid_loader = create_dataloader(X_valid, y_valid, batch_size=576)\n",
    "\n",
    "        n_chans = X_train.shape[1]\n",
    "\n",
    "        #network_list = ['ATCNet', 'ShallowFBCSPNet', 'Deep4Net', 'EEGNetv4']\n",
    "        if network == 'ATCNet' :\n",
    "            model = ATCNet(\n",
    "                            n_chans,\n",
    "                            n_classes\n",
    "                            )\n",
    "        if network == \"ShallowFBCSPNet\":\n",
    "            model = ShallowFBCSPNet(\n",
    "            n_chans,\n",
    "            n_classes,\n",
    "            input_window_samples=input_window_samples,\n",
    "            final_conv_length=\"auto\",\n",
    "            )\n",
    "        if network == \"EEGITNet\":\n",
    "            model = EEGITNet(\n",
    "                n_classes,\n",
    "                n_chans,\n",
    "                input_window_samples=input_window_samples,\n",
    "                )\n",
    "        if network == \"Deep4Net\":\n",
    "            model = Deep4Net(\n",
    "                n_chans,\n",
    "                n_classes,\n",
    "                input_window_samples=input_window_samples,\n",
    "                final_conv_length='auto',\n",
    "                n_filters_time=25,\n",
    "                n_filters_spat=25,\n",
    "                filter_time_length=10,\n",
    "                pool_time_length=3,\n",
    "                pool_time_stride=3,\n",
    "                n_filters_2=50,\n",
    "                filter_length_2=10,\n",
    "                n_filters_3=100,\n",
    "                filter_length_3=10,\n",
    "                n_filters_4=200,\n",
    "                filter_length_4=10,\n",
    "                first_pool_mode=\"max\",\n",
    "                later_pool_mode=\"max\",\n",
    "                drop_prob=0.5,\n",
    "                #double_time_convs=False,\n",
    "                split_first_layer=True,\n",
    "                batch_norm=True,\n",
    "                batch_norm_alpha=0.1,\n",
    "                stride_before_pool=False\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "        params = {\"Subject number\":subject_id,\n",
    "                  \"learning_rate\": lr ,\n",
    "                  \"optimizer\": \"AdamW\" ,\n",
    "                  \"Network\": network,\n",
    "                  \"Datasets\":\"BNCI2014000\",\n",
    "                  \"sfreq\":dataset.datasets[0].raw.info['sfreq'],\n",
    "                  \"Class number\":n_classes,\n",
    "                  \"Channel number\": train_set[0][0].shape[0],\n",
    "                  \"samples point\" : X_train.shape[2]\n",
    "\n",
    "                  }\n",
    "        run[\"parameters\"] = params\n",
    "        net = model.cuda(gpu_num)\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()#nn.BCELoss()#\n",
    "        train_loss,valid_loss,train_accuracy,valid_accuracy,part_weights =train(\n",
    "                                                                    model = net,\n",
    "                                                                    gpu_num = gpu_num,\n",
    "                                                                    train_loader = train_loader,\n",
    "                                                                    test_loader = test_loader,\n",
    "                                                                    optimizer = optimizer  ,\n",
    "                                                                    criterion = criterion,\n",
    "                                                                    num_epochs=n_epochs,\n",
    "                                                                    save_weights= True,\n",
    "                                                                    lr=lr\n",
    "                                                                         )\n",
    "        model.load_state_dict(torch.load(part_weights))\n",
    "        eval(model = net,\n",
    "            gpu_num = gpu_num,\n",
    "            valid_loader= valid_loader,\n",
    "             labels=labels,\n",
    "             )\n",
    "\n",
    "\n",
    "        run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

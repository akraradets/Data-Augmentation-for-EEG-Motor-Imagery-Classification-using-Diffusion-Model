{
 "cells": [
  {
   "cell_type": "raw",
   "id": "edc0735b-62f8-4efa-b828-7f7a11868b90",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/pytorch-ignite/pytorch-ignite.ai/blob/gh-pages/how-to-guides/07-cross-validation.ipynb#scrollTo=iyrEEGHD9Ie5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d83b651-50a4-492e-bf4d-c28546b01977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#comment this if you are not using puffer?\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e59fe9-7be7-48c3-8612-02cbd5fc9742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nutapolt/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import os\n",
    "import sys\n",
    "from mne.datasets import eegbci\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from mne.datasets import eegbci\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader,SubsetRandomSampler\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8543d9f-1cda-4609-97d8-a02ad0208221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf7aecb-9c1f-4ad6-a018-1619a1a218de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG:\n",
    "    def __init__(self, path, base_url, subjects, runs):\n",
    "        self.subpath = ''\n",
    "        self.path = path\n",
    "        self.base_url = base_url\n",
    "        self.subjects = subjects\n",
    "        self.runs = runs\n",
    "        \n",
    "        # download data if does not exist in path.\n",
    "        # self.load_data()\n",
    "        self.data_to_raw()\n",
    "    \n",
    "    def load_data(self):\n",
    "        print(f\">>> Start download from: {self.base_url}.\")\n",
    "        print(f\"Downloading files to: {self.path}.\")\n",
    "        for subject in self.subjects:\n",
    "            eegbci.load_data(subject,self.runs,path=self.path,base_url=self.base_url)\n",
    "        print(\"Done.\")\n",
    "    \n",
    "    \n",
    "        \n",
    "        print(\"Done.\")\n",
    "        return self.raw\n",
    "    def filter(self, freq):\n",
    "        raw = self.raw\n",
    "        low, high = freq\n",
    "        print(f\">>> Apply filter.\")\n",
    "        self.raw.filter(low, high, fir_design='firwin', verbose=20)\n",
    "        return  raw\n",
    "    def raw_ica(self):\n",
    "        raw = self.raw\n",
    "        ica = mne.preprocessing.ICA(n_components=64, max_iter=100)\n",
    "        ica.fit(raw)\n",
    "        ica.exclude = [1, 2]  # details on how we picked these are omitted here\n",
    "        ica.plot_properties(raw, picks=ica.exclude)\n",
    "        ica.apply(raw)\n",
    "        print('ICA DONE ????')\n",
    "        return  raw\n",
    "        \n",
    "    def get_events(self):\n",
    "        event_id = dict(T1=0, T2=1) # the events we want to extract\n",
    "        events, event_id = mne.events_from_annotations(self.raw, event_id=event_id)\n",
    "        return events, event_id\n",
    "    \n",
    "    def get_epochs(self, events, event_id):\n",
    "        picks = mne.pick_types(self.raw.info, eeg=True, exclude='bads')\n",
    "        tmin = 0\n",
    "        tmax = 4\n",
    "        epochs = mne.Epochs(self.raw, events, event_id, tmin, tmax, proj=True, \n",
    "                            picks=picks, baseline=None, preload=True)\n",
    "        return epochs\n",
    "    \n",
    "    def create_epochs(self):\n",
    "        print(\">>> Create Epochs.\")\n",
    "        events, event_id = self.get_events()\n",
    "        self.epochs = self.get_epochs(events, event_id)\n",
    "        return events , event_id\n",
    "        \n",
    "        print(\"Done.\")\n",
    "    \n",
    "    def get_X_y(self):\n",
    "        if self.epochs is None:\n",
    "            events , event_id=self.create_epochs()\n",
    "        self.X = self.epochs.get_data()\n",
    "        self.y = self.epochs.events[:, -1]\n",
    "        return self.X, self.y\n",
    "    \n",
    "    \n",
    "    def data_to_raw(self):\n",
    "        fullpath = os.path.join(self.path, *self.subpath.split(sep='/'))\n",
    "        #print(f\">>> Extract all subjects from: {fullpath}.\")\n",
    "        extension = \"edf\"\n",
    "        raws = []\n",
    "        count = 1\n",
    "        for i, subject in enumerate(self.subjects):\n",
    "            sname = f\"S{str(subject).zfill(3)}\".upper()\n",
    "            \n",
    "            for j, run in enumerate(self.runs):\n",
    "                rname = f\"{sname}R{str(run).zfill(2)}\".upper()\n",
    "                path_file = os.path.join(fullpath, sname, f'{rname}.{extension}')\n",
    "                #print(path_file)\n",
    "                #print(f\"Loading file #{count}/{len(self.subjects)*len(self.runs)}: {f'{rname}.{extension}'}\")\n",
    "                raw = mne.io.read_raw_edf( path_file , preload=True, verbose='WARNING' )\n",
    "                raws.append(raw)\n",
    "                count += 1\n",
    "\n",
    "        raw = mne.io.concatenate_raws(raws)\n",
    "        eegbci.standardize(raw)\n",
    "        montage = mne.channels.make_standard_montage('standard_1005')\n",
    "        raw.set_montage(montage)\n",
    "        self.raw = raw\n",
    "        \n",
    "        \n",
    "        \n",
    "def do_plot(train_loss, valid_loss,ty):\n",
    "    if ty == \"loss\":\n",
    "        plt.figure(figsize=(10,10))\n",
    "        \n",
    "        plt.plot(train_loss, label='train_loss')\n",
    "        plt.plot(valid_loss, label='valid_loss')\n",
    "        plt.title('loss {}'.format(iter))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    if ty == \"acc\":\n",
    "        plt.figure(figsize=(10,10))\n",
    "        \n",
    "        plot_ty=torch.tensor(train_loss, device = 'cpu')\n",
    "        plat_va=torch.tensor(valid_loss, device = 'cpu')\n",
    "        plt.plot(plot_ty, label='train_acc')\n",
    "        plt.plot(plat_va, label='valid_acc')\n",
    "        plt.title('acc {}'.format(iter))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "def create_dataloader_cross(X, y, batch_size):\n",
    "    \n",
    "    X_tensor = torch.tensor(X).float()\n",
    "    y_tensor = torch.tensor(y).long()\n",
    "    dataset_tensor = TensorDataset(X_tensor, y_tensor)\n",
    "    \n",
    "    dl = torch.utils.data.DataLoader(dataset_tensor, batch_size=batch_size, shuffle=True)\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0dd9b5d-1ed9-4cf2-a34f-6626c35159db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw done\n",
      ">>> Apply filter.\n",
      "Filtering raw data in 237 contiguous segments\n",
      "Setting up band-pass filter from 1 - 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 529 samples (3.306 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter done\n",
      ">>> Create Epochs.\n",
      "Used Annotations descriptions: ['T1', 'T2']\n",
      "Not setting metadata\n",
      "3555 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 3555 events and 641 original time points ...\n",
      "16 bad epochs dropped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[    672,       0,       1],\n",
       "        [   2000,       0,       0],\n",
       "        [   3328,       0,       0],\n",
       "        ...,\n",
       "        [4676928,       0,       1],\n",
       "        [4678256,       0,       0],\n",
       "        [4679584,       0,       1]]),\n",
       " {'T1': 0, 'T2': 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# home directory + datasets folder\n",
    "#path = '/content/drive/MyDrive/MNE-eegbci-data/files/eegmmidb/'\n",
    "path = 'physionet.org/files/eegmmidb/1.0.0'\n",
    "base_url = 'https://physionet.org/files/eegmmidb/'\n",
    "# subjects = [1]\n",
    "#runs = [3, 4, 7, 8, 11, 12]\n",
    "runs = [4, 8, 12]\n",
    "subjects = [i for i in range(1,80)]\n",
    "#subjects = [1]\n",
    "# runs = [6,10,14]\n",
    "\n",
    "eeg = EEG(path, base_url, subjects, runs)\n",
    "raw=eeg.data_to_raw()\n",
    "print(\"Raw done\")\n",
    "# apply filter\n",
    "freq = (1., 40.)\n",
    "raw=eeg.filter(freq=freq)\n",
    "#raw=eeg.data_to_raw()\n",
    "print(\"Filter done\")\n",
    "#raw=eeg.raw_ica()\n",
    "eeg.create_epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a751921c-5ff5-4a08-ad1f-d503e6062002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3539, 64, 641) (3539,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3539, 64, 641)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = eeg.get_X_y()\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "X = X[:,:,:]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f08e360-9091-4c79-9a80-26303c8f20e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3539, 2, 641)\n"
     ]
    }
   ],
   "source": [
    "# pick 7 channels.\n",
    "#X2 = X[:, :14, :]\n",
    "# = X2\n",
    "\n",
    "# pick C3 and C4 channels.\n",
    "X2 = X[:, 1:2, :] \n",
    "X3= X[:, 5:6, :]\n",
    "X = np.concatenate((X2,X3), axis=1)\n",
    "# = X4\n",
    "#X = X[:, :,np.newaxis,:]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87f728da-0a9a-4875-927d-b012cba21e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size (3185, 2, 641) (3185,)\n",
      "Test size (354, 2, 641) (354,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "print('train size',X_train.shape, y_train.shape)\n",
    "print('Test size',X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b45b6f4-78e1-4c82-a544-bb2abed895cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset\n",
    "X_tensor = torch.tensor(X_train).float()\n",
    "y_tensor = torch.tensor(y_train).long()\n",
    "\n",
    "\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d940d-5dac-4cbd-a00b-d0d691f19c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc4f40c8-dac3-4b28-b20b-dad2fe492377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        #using sequential helps bind multiple operations together\n",
    "        self.layer1 = nn.Sequential(\n",
    "            #padding = (kernel_size - 1) / 2 = 2\n",
    "            nn.Conv1d(2, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc = nn.Linear(82048, 2)\n",
    "        self.drop_out = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.drop_out(out)\n",
    "        out = out.reshape(out.size(0), -1)   #can also use .view()\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fcd2047-d53e-4480-ad09-c9d7ec30b205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnutapol-1997\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nutapolt/eeg_mi/wandb/run-20220905_062331-y1t4w2yw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery/runs/y1t4w2yw\" target=\"_blank\">physionet_CNN_2ch_2class_cross</a></strong> to <a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "wand = wandb.init(\n",
    "        \n",
    "      # Set the project where this run will be logged\n",
    "      project=\"Motor-Imagery\", \n",
    "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"physionet_CNN_2ch_2class_cross\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      config={\n",
    "      \"learning_rate\": 0.00001,\n",
    "      \"architecture\": \"CNN\",\n",
    "      \"dataset\": \"Nutapol T.\",\n",
    "      \"epochs\": 1000,\n",
    "      \"weightname\":\"physionet_CNN_2ch_2class_cross\",\n",
    "       \n",
    "        \n",
    "      }\n",
    "    )\n",
    "\n",
    "config = wand.config\n",
    "#print(config.num_step_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e751a47-f30f-4187-a43d-698e941345a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import setup_dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44735353-f086-40c5-8035-1197643e4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 3\n",
    "#splits = KFold(n_splits=num_folds,shuffle=True,random_state=42)\n",
    "splits = KFold(n_splits=num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da7aa5d0-fd03-4e29-9f23-13664c413e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNet().cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25314bc9-2383-4ece-95ea-9818d33219f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd638e-4114-456f-b194-cd37b7447463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09b5210f-8b9c-4970-8c45-1159cafdea08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2123, 2, 641])\n",
      "torch.Size([2123])\n",
      "torch.Size([2123, 2, 641])\n",
      "torch.Size([2123])\n",
      "torch.Size([2124, 2, 641])\n",
      "torch.Size([2124])\n"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "        \n",
    "        X_tensor[train_idx]\n",
    "        y_tensor\n",
    "        \n",
    "        #print('Fold {}'.format(fold_idx + 1))\n",
    "        print(X_tensor[train_idx].shape)\n",
    "        print(y_tensor[train_idx].shape)\n",
    "        #train_loader, val_loader = setup_dataflow(dataset, train_idx, val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5dcbe0-a07f-445f-a245-bdc9b3c058de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026fd924-5ee7-473a-8721-49ff9a9846f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3efc55-d801-4041-ac5c-a8ac7ffb4a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2297074-b46a-40d9-b1fd-0522136e01ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef542f-6886-49be-b91b-32a7ad651b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/1000, Tr Loss: 0.7414, Tr Acc: 49.2699, Val Loss: 0.6932, Val Acc: 50.9416\n",
      "Epoch 11/1000, Tr Loss: 0.6468, Tr Acc: 63.6835, Val Loss: 0.6638, Val Acc: 60.0753\n",
      "Epoch 21/1000, Tr Loss: 0.5869, Tr Acc: 69.2887, Val Loss: 0.6319, Val Acc: 65.9134\n",
      "Epoch 31/1000, Tr Loss: 0.5454, Tr Acc: 73.2454, Val Loss: 0.6126, Val Acc: 67.1375\n",
      "Epoch 41/1000, Tr Loss: 0.5124, Tr Acc: 74.8940, Val Loss: 0.5935, Val Acc: 68.7382\n",
      "Epoch 51/1000, Tr Loss: 0.4836, Tr Acc: 77.1550, Val Loss: 0.6185, Val Acc: 65.1601\n",
      "Epoch 61/1000, Tr Loss: 0.4668, Tr Acc: 78.9449, Val Loss: 0.6318, Val Acc: 65.2542\n",
      "Epoch 71/1000, Tr Loss: 0.4499, Tr Acc: 79.8399, Val Loss: 0.6285, Val Acc: 66.6667\n",
      "Epoch 81/1000, Tr Loss: 0.4358, Tr Acc: 80.5464, Val Loss: 0.5959, Val Acc: 71.0923\n",
      "Epoch 91/1000, Tr Loss: 0.4262, Tr Acc: 81.2529, Val Loss: 0.6407, Val Acc: 65.6309\n",
      "Epoch 101/1000, Tr Loss: 0.4120, Tr Acc: 81.3000, Val Loss: 0.6943, Val Acc: 62.2411\n",
      "Epoch 111/1000, Tr Loss: 0.4042, Tr Acc: 81.6298, Val Loss: 0.6164, Val Acc: 68.1733\n",
      "Epoch 121/1000, Tr Loss: 0.4012, Tr Acc: 81.9595, Val Loss: 0.7094, Val Acc: 64.2185\n",
      "Epoch 131/1000, Tr Loss: 0.3888, Tr Acc: 82.3834, Val Loss: 0.6214, Val Acc: 68.3616\n",
      "Epoch 141/1000, Tr Loss: 0.3771, Tr Acc: 83.1371, Val Loss: 0.6175, Val Acc: 70.1507\n",
      "Epoch 151/1000, Tr Loss: 0.3638, Tr Acc: 84.0791, Val Loss: 0.6232, Val Acc: 67.7966\n",
      "Epoch 161/1000, Tr Loss: 0.3539, Tr Acc: 84.3147, Val Loss: 0.6322, Val Acc: 69.3974\n",
      "Epoch 171/1000, Tr Loss: 0.3466, Tr Acc: 84.9741, Val Loss: 0.6395, Val Acc: 68.7382\n",
      "Epoch 181/1000, Tr Loss: 0.3473, Tr Acc: 85.0683, Val Loss: 0.6647, Val Acc: 69.2090\n",
      "Epoch 191/1000, Tr Loss: 0.3347, Tr Acc: 86.0104, Val Loss: 0.6442, Val Acc: 68.3616\n",
      "Epoch 201/1000, Tr Loss: 0.3322, Tr Acc: 85.6806, Val Loss: 0.6655, Val Acc: 66.7608\n",
      "Epoch 211/1000, Tr Loss: 0.3210, Tr Acc: 86.9524, Val Loss: 0.6600, Val Acc: 66.0075\n",
      "Epoch 221/1000, Tr Loss: 0.3102, Tr Acc: 86.9995, Val Loss: 0.6433, Val Acc: 67.2316\n",
      "Epoch 231/1000, Tr Loss: 0.3133, Tr Acc: 87.0466, Val Loss: 0.7710, Val Acc: 62.2411\n",
      "Epoch 241/1000, Tr Loss: 0.2974, Tr Acc: 88.3184, Val Loss: 0.6664, Val Acc: 67.8908\n",
      "Epoch 251/1000, Tr Loss: 0.2942, Tr Acc: 88.3184, Val Loss: 0.7321, Val Acc: 66.6667\n",
      "Epoch 261/1000, Tr Loss: 0.2907, Tr Acc: 88.4597, Val Loss: 0.7832, Val Acc: 63.3710\n",
      "Epoch 271/1000, Tr Loss: 0.2879, Tr Acc: 87.5648, Val Loss: 0.7681, Val Acc: 63.6535\n",
      "Epoch 281/1000, Tr Loss: 0.2782, Tr Acc: 89.4489, Val Loss: 0.7834, Val Acc: 64.1243\n",
      "Epoch 291/1000, Tr Loss: 0.2724, Tr Acc: 89.5431, Val Loss: 0.8414, Val Acc: 61.3936\n",
      "Epoch 301/1000, Tr Loss: 0.2724, Tr Acc: 89.9199, Val Loss: 1.0872, Val Acc: 59.5104\n",
      "Epoch 311/1000, Tr Loss: 0.2552, Tr Acc: 90.2968, Val Loss: 0.7370, Val Acc: 67.2316\n",
      "Epoch 321/1000, Tr Loss: 0.2571, Tr Acc: 89.8728, Val Loss: 0.7303, Val Acc: 66.4783\n",
      "Epoch 331/1000, Tr Loss: 0.2483, Tr Acc: 90.2496, Val Loss: 0.8189, Val Acc: 62.9944\n",
      "Epoch 341/1000, Tr Loss: 0.2480, Tr Acc: 90.9091, Val Loss: 0.8284, Val Acc: 62.9944\n",
      "Epoch 351/1000, Tr Loss: 0.2375, Tr Acc: 91.1917, Val Loss: 0.7420, Val Acc: 67.1375\n",
      "Epoch 361/1000, Tr Loss: 0.2306, Tr Acc: 91.6156, Val Loss: 0.8704, Val Acc: 61.7702\n",
      "Epoch 371/1000, Tr Loss: 0.2373, Tr Acc: 90.3438, Val Loss: 0.9550, Val Acc: 59.8870\n",
      "Epoch 381/1000, Tr Loss: 0.2241, Tr Acc: 91.8040, Val Loss: 0.8277, Val Acc: 64.1243\n",
      "Epoch 391/1000, Tr Loss: 0.2239, Tr Acc: 91.5685, Val Loss: 1.0616, Val Acc: 60.0753\n",
      "Epoch 401/1000, Tr Loss: 0.2206, Tr Acc: 92.2280, Val Loss: 0.8020, Val Acc: 65.7250\n",
      "Epoch 411/1000, Tr Loss: 0.2235, Tr Acc: 91.4272, Val Loss: 0.8754, Val Acc: 64.9718\n",
      "Epoch 421/1000, Tr Loss: 0.2077, Tr Acc: 93.1700, Val Loss: 0.8900, Val Acc: 62.4294\n",
      "Epoch 431/1000, Tr Loss: 0.2042, Tr Acc: 92.5106, Val Loss: 0.7799, Val Acc: 65.3484\n",
      "Epoch 441/1000, Tr Loss: 0.2028, Tr Acc: 92.6519, Val Loss: 0.7995, Val Acc: 67.9849\n",
      "Epoch 451/1000, Tr Loss: 0.2141, Tr Acc: 92.0867, Val Loss: 1.1737, Val Acc: 59.5104\n",
      "Epoch 461/1000, Tr Loss: 0.1928, Tr Acc: 92.9345, Val Loss: 0.8753, Val Acc: 64.4068\n",
      "Epoch 471/1000, Tr Loss: 0.1962, Tr Acc: 92.7932, Val Loss: 0.8259, Val Acc: 66.4783\n",
      "Epoch 481/1000, Tr Loss: 0.1954, Tr Acc: 92.6990, Val Loss: 0.8092, Val Acc: 66.9492\n",
      "Epoch 491/1000, Tr Loss: 0.1849, Tr Acc: 93.0758, Val Loss: 0.8086, Val Acc: 66.0075\n",
      "Epoch 501/1000, Tr Loss: 0.1779, Tr Acc: 94.0650, Val Loss: 0.8390, Val Acc: 67.8908\n",
      "Epoch 511/1000, Tr Loss: 0.1790, Tr Acc: 93.9708, Val Loss: 1.8628, Val Acc: 56.6855\n",
      "Epoch 521/1000, Tr Loss: 0.1784, Tr Acc: 94.1121, Val Loss: 1.1193, Val Acc: 59.6987\n",
      "Epoch 531/1000, Tr Loss: 0.1782, Tr Acc: 93.9237, Val Loss: 1.1132, Val Acc: 62.9002\n",
      "Epoch 541/1000, Tr Loss: 0.1692, Tr Acc: 94.5360, Val Loss: 0.9050, Val Acc: 65.6309\n",
      "Epoch 551/1000, Tr Loss: 0.1634, Tr Acc: 94.3476, Val Loss: 1.0319, Val Acc: 61.6761\n",
      "Epoch 561/1000, Tr Loss: 0.1688, Tr Acc: 93.8295, Val Loss: 1.2772, Val Acc: 60.0753\n",
      "Epoch 571/1000, Tr Loss: 0.1565, Tr Acc: 95.1484, Val Loss: 1.1976, Val Acc: 60.0753\n",
      "Epoch 581/1000, Tr Loss: 0.1596, Tr Acc: 94.3476, Val Loss: 0.8735, Val Acc: 64.8776\n",
      "Epoch 591/1000, Tr Loss: 0.1570, Tr Acc: 94.9129, Val Loss: 1.7858, Val Acc: 56.4972\n",
      "Epoch 601/1000, Tr Loss: 0.1579, Tr Acc: 94.9129, Val Loss: 0.8619, Val Acc: 67.1375\n",
      "Epoch 611/1000, Tr Loss: 0.1528, Tr Acc: 94.8658, Val Loss: 1.0146, Val Acc: 64.0301\n",
      "Epoch 621/1000, Tr Loss: 0.1498, Tr Acc: 94.8186, Val Loss: 0.8810, Val Acc: 66.8550\n",
      "Epoch 631/1000, Tr Loss: 0.1487, Tr Acc: 94.9129, Val Loss: 1.0630, Val Acc: 63.9360\n",
      "Epoch 641/1000, Tr Loss: 0.1447, Tr Acc: 95.4310, Val Loss: 1.4499, Val Acc: 62.4294\n",
      "Epoch 651/1000, Tr Loss: 0.1463, Tr Acc: 95.0071, Val Loss: 1.1143, Val Acc: 62.3352\n",
      "Epoch 661/1000, Tr Loss: 0.1377, Tr Acc: 95.9491, Val Loss: 1.1770, Val Acc: 61.2053\n",
      "Epoch 671/1000, Tr Loss: 0.1395, Tr Acc: 95.6665, Val Loss: 1.1592, Val Acc: 58.9454\n",
      "Epoch 681/1000, Tr Loss: 0.1371, Tr Acc: 95.4310, Val Loss: 0.9362, Val Acc: 66.8550\n",
      "Epoch 691/1000, Tr Loss: 0.1374, Tr Acc: 95.7607, Val Loss: 1.0941, Val Acc: 62.0527\n",
      "Epoch 701/1000, Tr Loss: 0.1412, Tr Acc: 95.5723, Val Loss: 1.0393, Val Acc: 64.1243\n",
      "Epoch 711/1000, Tr Loss: 0.1325, Tr Acc: 95.7136, Val Loss: 1.0071, Val Acc: 63.8418\n",
      "Epoch 721/1000, Tr Loss: 0.1389, Tr Acc: 95.3368, Val Loss: 0.9059, Val Acc: 66.0075\n",
      "Epoch 731/1000, Tr Loss: 0.1345, Tr Acc: 95.4310, Val Loss: 1.1605, Val Acc: 62.4294\n",
      "Epoch 741/1000, Tr Loss: 0.1353, Tr Acc: 95.7607, Val Loss: 1.0826, Val Acc: 64.5009\n",
      "Epoch 751/1000, Tr Loss: 0.1282, Tr Acc: 95.9020, Val Loss: 2.0647, Val Acc: 56.2147\n",
      "Epoch 761/1000, Tr Loss: 0.1312, Tr Acc: 95.3839, Val Loss: 1.1039, Val Acc: 63.0885\n",
      "Epoch 771/1000, Tr Loss: 0.1156, Tr Acc: 96.6086, Val Loss: 0.9825, Val Acc: 67.0433\n",
      "Epoch 781/1000, Tr Loss: 0.1126, Tr Acc: 96.7028, Val Loss: 1.0726, Val Acc: 66.0075\n",
      "Epoch 791/1000, Tr Loss: 0.1162, Tr Acc: 96.4202, Val Loss: 0.9997, Val Acc: 66.7608\n",
      "Epoch 801/1000, Tr Loss: 0.1168, Tr Acc: 96.0904, Val Loss: 1.8646, Val Acc: 59.5104\n",
      "Epoch 811/1000, Tr Loss: 0.1214, Tr Acc: 95.8078, Val Loss: 0.9745, Val Acc: 67.5141\n",
      "Epoch 821/1000, Tr Loss: 0.1170, Tr Acc: 96.7499, Val Loss: 2.6086, Val Acc: 56.0264\n",
      "Epoch 831/1000, Tr Loss: 0.1128, Tr Acc: 96.7028, Val Loss: 1.0109, Val Acc: 67.3258\n",
      "Epoch 841/1000, Tr Loss: 0.1073, Tr Acc: 96.9383, Val Loss: 1.0535, Val Acc: 64.9718\n",
      "Epoch 851/1000, Tr Loss: 0.1105, Tr Acc: 96.5615, Val Loss: 2.6170, Val Acc: 55.2731\n",
      "Epoch 861/1000, Tr Loss: 0.1073, Tr Acc: 96.8912, Val Loss: 1.1799, Val Acc: 61.7702\n",
      "Epoch 871/1000, Tr Loss: 0.1107, Tr Acc: 96.8912, Val Loss: 0.9750, Val Acc: 66.5725\n",
      "Epoch 881/1000, Tr Loss: 0.1095, Tr Acc: 96.5144, Val Loss: 2.8977, Val Acc: 54.3315\n",
      "Epoch 891/1000, Tr Loss: 0.1066, Tr Acc: 96.8912, Val Loss: 1.6741, Val Acc: 56.9680\n",
      "Epoch 901/1000, Tr Loss: 0.1021, Tr Acc: 96.9383, Val Loss: 1.1284, Val Acc: 65.5367\n",
      "Epoch 911/1000, Tr Loss: 0.1074, Tr Acc: 96.8441, Val Loss: 1.0098, Val Acc: 67.2316\n",
      "Epoch 921/1000, Tr Loss: 0.0975, Tr Acc: 97.0796, Val Loss: 1.2480, Val Acc: 63.9360\n",
      "Epoch 931/1000, Tr Loss: 0.0940, Tr Acc: 97.2209, Val Loss: 2.4746, Val Acc: 56.7797\n",
      "Epoch 941/1000, Tr Loss: 0.1029, Tr Acc: 97.0796, Val Loss: 1.2406, Val Acc: 62.9944\n",
      "Epoch 951/1000, Tr Loss: 0.0954, Tr Acc: 97.0796, Val Loss: 1.3634, Val Acc: 60.9228\n",
      "Epoch 961/1000, Tr Loss: 0.0959, Tr Acc: 96.9383, Val Loss: 1.6562, Val Acc: 56.9680\n",
      "Epoch 971/1000, Tr Loss: 0.0936, Tr Acc: 97.3622, Val Loss: 2.5006, Val Acc: 57.0621\n",
      "Epoch 981/1000, Tr Loss: 0.0902, Tr Acc: 97.7390, Val Loss: 1.1131, Val Acc: 64.1243\n",
      "Epoch 991/1000, Tr Loss: 0.1020, Tr Acc: 96.9854, Val Loss: 1.4116, Val Acc: 61.5819\n",
      "Fold 2\n",
      "Epoch 1/1000, Tr Loss: 0.6379, Tr Acc: 81.9595, Val Loss: 0.7046, Val Acc: 67.0433\n",
      "Epoch 11/1000, Tr Loss: 0.4387, Tr Acc: 84.9270, Val Loss: 0.3396, Val Acc: 82.2034\n",
      "Epoch 21/1000, Tr Loss: 0.3797, Tr Acc: 85.2567, Val Loss: 0.3348, Val Acc: 83.2392\n",
      "Epoch 31/1000, Tr Loss: 0.3256, Tr Acc: 87.5648, Val Loss: 2.3664, Val Acc: 55.5556\n",
      "Epoch 41/1000, Tr Loss: 0.2945, Tr Acc: 87.1408, Val Loss: 0.6171, Val Acc: 69.8682\n",
      "Epoch 51/1000, Tr Loss: 0.2894, Tr Acc: 87.8945, Val Loss: 0.3823, Val Acc: 78.7194\n",
      "Epoch 61/1000, Tr Loss: 0.2701, Tr Acc: 87.7061, Val Loss: 0.5554, Val Acc: 72.3164\n",
      "Epoch 71/1000, Tr Loss: 0.2557, Tr Acc: 89.0721, Val Loss: 0.3972, Val Acc: 78.6252\n",
      "Epoch 81/1000, Tr Loss: 0.2401, Tr Acc: 89.4489, Val Loss: 0.2696, Val Acc: 87.5706\n",
      "Epoch 91/1000, Tr Loss: 0.2288, Tr Acc: 90.5794, Val Loss: 0.2962, Val Acc: 87.8531\n",
      "Epoch 101/1000, Tr Loss: 0.2125, Tr Acc: 91.3801, Val Loss: 0.9215, Val Acc: 70.0565\n",
      "Epoch 111/1000, Tr Loss: 0.2180, Tr Acc: 90.3910, Val Loss: 0.5888, Val Acc: 74.1055\n",
      "Epoch 121/1000, Tr Loss: 0.1969, Tr Acc: 91.8983, Val Loss: 0.2741, Val Acc: 88.7947\n",
      "Epoch 131/1000, Tr Loss: 0.1959, Tr Acc: 91.6627, Val Loss: 0.3099, Val Acc: 87.7589\n",
      "Epoch 141/1000, Tr Loss: 0.1929, Tr Acc: 92.2280, Val Loss: 0.4193, Val Acc: 78.8136\n",
      "Epoch 151/1000, Tr Loss: 0.1770, Tr Acc: 92.6048, Val Loss: 0.7177, Val Acc: 69.8682\n",
      "Epoch 161/1000, Tr Loss: 0.1729, Tr Acc: 93.4527, Val Loss: 0.5207, Val Acc: 76.5537\n",
      "Epoch 171/1000, Tr Loss: 0.1614, Tr Acc: 93.9708, Val Loss: 1.1234, Val Acc: 65.7250\n",
      "Epoch 181/1000, Tr Loss: 0.1696, Tr Acc: 92.5577, Val Loss: 0.4681, Val Acc: 79.4727\n",
      "Epoch 191/1000, Tr Loss: 0.1587, Tr Acc: 94.1121, Val Loss: 0.6732, Val Acc: 70.8098\n",
      "Epoch 201/1000, Tr Loss: 0.1695, Tr Acc: 93.1700, Val Loss: 0.7250, Val Acc: 73.7288\n",
      "Epoch 211/1000, Tr Loss: 0.1548, Tr Acc: 93.8766, Val Loss: 0.3273, Val Acc: 85.5932\n",
      "Epoch 221/1000, Tr Loss: 0.1436, Tr Acc: 94.5831, Val Loss: 0.7139, Val Acc: 72.0339\n",
      "Epoch 231/1000, Tr Loss: 0.1548, Tr Acc: 94.0179, Val Loss: 0.8485, Val Acc: 67.3258\n",
      "Epoch 241/1000, Tr Loss: 0.1467, Tr Acc: 94.2534, Val Loss: 1.5153, Val Acc: 65.1601\n",
      "Epoch 251/1000, Tr Loss: 0.1362, Tr Acc: 95.6665, Val Loss: 0.4842, Val Acc: 79.0019\n",
      "Epoch 261/1000, Tr Loss: 0.1340, Tr Acc: 95.1955, Val Loss: 0.8990, Val Acc: 69.7740\n",
      "Epoch 271/1000, Tr Loss: 0.1405, Tr Acc: 95.1955, Val Loss: 0.9047, Val Acc: 70.1507\n",
      "Epoch 281/1000, Tr Loss: 0.1320, Tr Acc: 95.0542, Val Loss: 1.3577, Val Acc: 62.2411\n",
      "Epoch 291/1000, Tr Loss: 0.1305, Tr Acc: 95.0071, Val Loss: 0.6989, Val Acc: 73.4463\n",
      "Epoch 301/1000, Tr Loss: 0.1222, Tr Acc: 95.9491, Val Loss: 0.4415, Val Acc: 81.4501\n",
      "Epoch 311/1000, Tr Loss: 0.1241, Tr Acc: 95.5252, Val Loss: 0.5378, Val Acc: 77.6836\n",
      "Epoch 321/1000, Tr Loss: 0.1189, Tr Acc: 95.8549, Val Loss: 0.7591, Val Acc: 71.8456\n",
      "Epoch 331/1000, Tr Loss: 0.1177, Tr Acc: 95.4781, Val Loss: 2.6251, Val Acc: 55.1789\n",
      "Epoch 341/1000, Tr Loss: 0.1107, Tr Acc: 96.4673, Val Loss: 0.8766, Val Acc: 69.3032\n",
      "Epoch 351/1000, Tr Loss: 0.1148, Tr Acc: 96.2788, Val Loss: 0.5029, Val Acc: 78.6252\n",
      "Epoch 361/1000, Tr Loss: 0.1109, Tr Acc: 96.1846, Val Loss: 1.1554, Val Acc: 67.4200\n",
      "Epoch 371/1000, Tr Loss: 0.1052, Tr Acc: 96.6086, Val Loss: 1.1489, Val Acc: 65.9134\n",
      "Epoch 381/1000, Tr Loss: 0.1059, Tr Acc: 96.3731, Val Loss: 0.6722, Val Acc: 73.4463\n",
      "Epoch 391/1000, Tr Loss: 0.1120, Tr Acc: 96.4202, Val Loss: 0.8706, Val Acc: 69.8682\n",
      "Epoch 401/1000, Tr Loss: 0.0982, Tr Acc: 96.9383, Val Loss: 1.2697, Val Acc: 63.5593\n",
      "Epoch 411/1000, Tr Loss: 0.1018, Tr Acc: 97.1267, Val Loss: 0.5108, Val Acc: 78.7194\n",
      "Epoch 421/1000, Tr Loss: 0.1000, Tr Acc: 97.1267, Val Loss: 1.3620, Val Acc: 64.8776\n",
      "Epoch 431/1000, Tr Loss: 0.0989, Tr Acc: 97.1267, Val Loss: 0.5489, Val Acc: 77.4011\n",
      "Epoch 441/1000, Tr Loss: 0.0993, Tr Acc: 96.7028, Val Loss: 0.9437, Val Acc: 68.6441\n",
      "Epoch 451/1000, Tr Loss: 0.0987, Tr Acc: 96.6557, Val Loss: 1.8860, Val Acc: 58.9454\n",
      "Epoch 461/1000, Tr Loss: 0.1010, Tr Acc: 96.8912, Val Loss: 0.5385, Val Acc: 79.0019\n",
      "Epoch 471/1000, Tr Loss: 0.0936, Tr Acc: 97.4093, Val Loss: 0.6627, Val Acc: 75.1412\n",
      "Epoch 481/1000, Tr Loss: 0.0907, Tr Acc: 96.9854, Val Loss: 3.0140, Val Acc: 54.8023\n",
      "Epoch 491/1000, Tr Loss: 0.0891, Tr Acc: 97.1738, Val Loss: 2.9336, Val Acc: 56.2147\n",
      "Epoch 501/1000, Tr Loss: 0.0857, Tr Acc: 97.5977, Val Loss: 0.5911, Val Acc: 78.5311\n",
      "Epoch 511/1000, Tr Loss: 0.0879, Tr Acc: 97.4564, Val Loss: 1.7526, Val Acc: 59.8870\n",
      "Epoch 521/1000, Tr Loss: 0.0929, Tr Acc: 97.4093, Val Loss: 1.0350, Val Acc: 67.2316\n",
      "Epoch 531/1000, Tr Loss: 0.0922, Tr Acc: 97.8804, Val Loss: 0.6945, Val Acc: 75.3296\n",
      "Epoch 541/1000, Tr Loss: 0.0800, Tr Acc: 97.9746, Val Loss: 0.5770, Val Acc: 77.4953\n",
      "Epoch 551/1000, Tr Loss: 0.0847, Tr Acc: 97.1738, Val Loss: 1.2259, Val Acc: 62.9002\n",
      "Epoch 561/1000, Tr Loss: 0.0837, Tr Acc: 97.5977, Val Loss: 1.8819, Val Acc: 62.9002\n",
      "Epoch 571/1000, Tr Loss: 0.0874, Tr Acc: 97.3151, Val Loss: 0.5716, Val Acc: 78.5311\n",
      "Epoch 581/1000, Tr Loss: 0.0792, Tr Acc: 97.6919, Val Loss: 0.7714, Val Acc: 72.5989\n",
      "Epoch 591/1000, Tr Loss: 0.0801, Tr Acc: 97.5035, Val Loss: 0.7647, Val Acc: 74.6704\n",
      "Epoch 601/1000, Tr Loss: 0.0776, Tr Acc: 97.8333, Val Loss: 1.6549, Val Acc: 59.3220\n",
      "Epoch 611/1000, Tr Loss: 0.0754, Tr Acc: 97.9275, Val Loss: 0.6149, Val Acc: 78.6252\n",
      "Epoch 621/1000, Tr Loss: 0.0795, Tr Acc: 97.8333, Val Loss: 1.8719, Val Acc: 63.0885\n",
      "Epoch 631/1000, Tr Loss: 0.0821, Tr Acc: 97.5506, Val Loss: 1.0872, Val Acc: 66.9492\n",
      "Epoch 641/1000, Tr Loss: 0.0714, Tr Acc: 98.2572, Val Loss: 0.8963, Val Acc: 69.3032\n",
      "Epoch 651/1000, Tr Loss: 0.0704, Tr Acc: 98.2101, Val Loss: 0.9595, Val Acc: 69.5857\n",
      "Epoch 661/1000, Tr Loss: 0.0782, Tr Acc: 97.7390, Val Loss: 0.6504, Val Acc: 75.3296\n",
      "Epoch 671/1000, Tr Loss: 0.0712, Tr Acc: 98.1159, Val Loss: 0.6262, Val Acc: 75.7062\n",
      "Epoch 681/1000, Tr Loss: 0.0658, Tr Acc: 98.2572, Val Loss: 1.7482, Val Acc: 60.9228\n",
      "Epoch 691/1000, Tr Loss: 0.0694, Tr Acc: 98.3043, Val Loss: 1.1987, Val Acc: 64.5951\n",
      "Epoch 701/1000, Tr Loss: 0.0632, Tr Acc: 98.5398, Val Loss: 1.1344, Val Acc: 65.9134\n",
      "Epoch 711/1000, Tr Loss: 0.0742, Tr Acc: 98.1630, Val Loss: 4.8703, Val Acc: 51.5066\n",
      "Epoch 721/1000, Tr Loss: 0.0653, Tr Acc: 98.4927, Val Loss: 1.4601, Val Acc: 62.7119\n",
      "Epoch 731/1000, Tr Loss: 0.0660, Tr Acc: 98.4456, Val Loss: 0.8069, Val Acc: 73.3522\n",
      "Epoch 741/1000, Tr Loss: 0.0717, Tr Acc: 98.1159, Val Loss: 0.7409, Val Acc: 75.5179\n",
      "Epoch 751/1000, Tr Loss: 0.0729, Tr Acc: 97.9746, Val Loss: 0.7380, Val Acc: 74.5763\n",
      "Epoch 761/1000, Tr Loss: 0.0619, Tr Acc: 98.2572, Val Loss: 0.7562, Val Acc: 77.2128\n",
      "Epoch 771/1000, Tr Loss: 0.0640, Tr Acc: 98.2572, Val Loss: 0.9139, Val Acc: 72.5989\n",
      "Epoch 781/1000, Tr Loss: 0.0624, Tr Acc: 98.6340, Val Loss: 0.6600, Val Acc: 76.6478\n",
      "Epoch 791/1000, Tr Loss: 0.0661, Tr Acc: 98.2572, Val Loss: 0.7643, Val Acc: 76.7420\n",
      "Epoch 801/1000, Tr Loss: 0.0643, Tr Acc: 97.6919, Val Loss: 0.9652, Val Acc: 71.5631\n",
      "Epoch 811/1000, Tr Loss: 0.0595, Tr Acc: 98.4456, Val Loss: 0.7980, Val Acc: 75.8945\n",
      "Epoch 821/1000, Tr Loss: 0.0578, Tr Acc: 98.8695, Val Loss: 4.1166, Val Acc: 53.0132\n",
      "Epoch 831/1000, Tr Loss: 0.0647, Tr Acc: 98.3043, Val Loss: 2.4677, Val Acc: 56.8738\n",
      "Epoch 841/1000, Tr Loss: 0.0585, Tr Acc: 98.6811, Val Loss: 3.2217, Val Acc: 55.3672\n",
      "Epoch 851/1000, Tr Loss: 0.0633, Tr Acc: 97.9746, Val Loss: 0.7936, Val Acc: 74.1996\n",
      "Epoch 861/1000, Tr Loss: 0.0594, Tr Acc: 98.8695, Val Loss: 1.7447, Val Acc: 65.6309\n",
      "Epoch 871/1000, Tr Loss: 0.0534, Tr Acc: 98.8695, Val Loss: 1.2414, Val Acc: 65.2542\n",
      "Epoch 881/1000, Tr Loss: 0.0606, Tr Acc: 98.3514, Val Loss: 0.8160, Val Acc: 76.3653\n",
      "Epoch 891/1000, Tr Loss: 0.0610, Tr Acc: 98.4456, Val Loss: 1.8272, Val Acc: 61.5819\n",
      "Epoch 901/1000, Tr Loss: 0.0540, Tr Acc: 98.8224, Val Loss: 1.0460, Val Acc: 70.7156\n",
      "Epoch 911/1000, Tr Loss: 0.0515, Tr Acc: 99.1050, Val Loss: 1.1868, Val Acc: 68.2674\n",
      "Epoch 921/1000, Tr Loss: 0.0497, Tr Acc: 99.0108, Val Loss: 0.8314, Val Acc: 75.0471\n",
      "Epoch 931/1000, Tr Loss: 0.0543, Tr Acc: 98.6340, Val Loss: 2.4663, Val Acc: 56.4972\n",
      "Epoch 941/1000, Tr Loss: 0.0553, Tr Acc: 98.5398, Val Loss: 0.9854, Val Acc: 73.3522\n",
      "Epoch 951/1000, Tr Loss: 0.0544, Tr Acc: 98.6340, Val Loss: 4.7745, Val Acc: 51.7891\n",
      "Epoch 961/1000, Tr Loss: 0.0512, Tr Acc: 98.7753, Val Loss: 0.8597, Val Acc: 73.5405\n",
      "Epoch 971/1000, Tr Loss: 0.0551, Tr Acc: 98.6811, Val Loss: 1.6523, Val Acc: 61.2053\n",
      "Epoch 981/1000, Tr Loss: 0.0554, Tr Acc: 98.5869, Val Loss: 2.1867, Val Acc: 61.9586\n",
      "Epoch 991/1000, Tr Loss: 0.0555, Tr Acc: 98.2101, Val Loss: 0.8232, Val Acc: 74.7646\n",
      "Fold 3\n",
      "Epoch 1/1000, Tr Loss: 0.5261, Tr Acc: 86.9586, Val Loss: 18.6857, Val Acc: 49.6701\n",
      "Epoch 11/1000, Tr Loss: 0.3281, Tr Acc: 89.0301, Val Loss: 3.5322, Val Acc: 56.6447\n",
      "Epoch 21/1000, Tr Loss: 0.2727, Tr Acc: 89.5951, Val Loss: 0.9836, Val Acc: 68.6145\n",
      "Epoch 31/1000, Tr Loss: 0.2531, Tr Acc: 89.8305, Val Loss: 3.8333, Val Acc: 55.8907\n",
      "Epoch 41/1000, Tr Loss: 0.2126, Tr Acc: 91.9021, Val Loss: 4.1843, Val Acc: 54.4769\n",
      "Epoch 51/1000, Tr Loss: 0.2083, Tr Acc: 91.9021, Val Loss: 0.2487, Val Acc: 88.2187\n",
      "Epoch 61/1000, Tr Loss: 0.1833, Tr Acc: 92.5612, Val Loss: 0.8153, Val Acc: 69.8398\n",
      "Epoch 71/1000, Tr Loss: 0.1717, Tr Acc: 92.8437, Val Loss: 1.0706, Val Acc: 68.8030\n",
      "Epoch 81/1000, Tr Loss: 0.1628, Tr Acc: 93.5970, Val Loss: 0.7267, Val Acc: 73.4213\n",
      "Epoch 91/1000, Tr Loss: 0.1568, Tr Acc: 93.9736, Val Loss: 1.5574, Val Acc: 62.6767\n",
      "Epoch 101/1000, Tr Loss: 0.1521, Tr Acc: 93.7853, Val Loss: 0.3082, Val Acc: 85.0141\n",
      "Epoch 111/1000, Tr Loss: 0.1547, Tr Acc: 93.6911, Val Loss: 0.7548, Val Acc: 72.8558\n",
      "Epoch 121/1000, Tr Loss: 0.1438, Tr Acc: 93.8795, Val Loss: 0.1879, Val Acc: 91.3289\n",
      "Epoch 131/1000, Tr Loss: 0.1298, Tr Acc: 94.8682, Val Loss: 1.0284, Val Acc: 68.4260\n",
      "Epoch 141/1000, Tr Loss: 0.1319, Tr Acc: 94.5386, Val Loss: 0.3208, Val Acc: 84.0716\n",
      "Epoch 151/1000, Tr Loss: 0.1262, Tr Acc: 95.1036, Val Loss: 0.8359, Val Acc: 70.2168\n",
      "Epoch 161/1000, Tr Loss: 0.1224, Tr Acc: 95.6215, Val Loss: 1.7981, Val Acc: 61.5457\n",
      "Epoch 171/1000, Tr Loss: 0.1274, Tr Acc: 95.1036, Val Loss: 1.0919, Val Acc: 69.6513\n",
      "Epoch 181/1000, Tr Loss: 0.1324, Tr Acc: 94.6328, Val Loss: 0.5388, Val Acc: 77.3798\n",
      "Epoch 191/1000, Tr Loss: 0.1185, Tr Acc: 95.5744, Val Loss: 0.2215, Val Acc: 89.2554\n",
      "Epoch 201/1000, Tr Loss: 0.1137, Tr Acc: 95.3390, Val Loss: 0.2946, Val Acc: 85.8624\n",
      "Epoch 211/1000, Tr Loss: 0.1208, Tr Acc: 95.2448, Val Loss: 1.0971, Val Acc: 67.2950\n",
      "Epoch 221/1000, Tr Loss: 0.1268, Tr Acc: 95.0565, Val Loss: 1.5408, Val Acc: 65.6927\n",
      "Epoch 231/1000, Tr Loss: 0.1176, Tr Acc: 96.0452, Val Loss: 0.3238, Val Acc: 83.7889\n",
      "Epoch 241/1000, Tr Loss: 0.1052, Tr Acc: 96.2335, Val Loss: 2.2599, Val Acc: 59.9434\n",
      "Epoch 251/1000, Tr Loss: 0.1090, Tr Acc: 96.0923, Val Loss: 0.3933, Val Acc: 82.3751\n",
      "Epoch 261/1000, Tr Loss: 0.0963, Tr Acc: 96.4689, Val Loss: 0.3782, Val Acc: 80.3016\n",
      "Epoch 271/1000, Tr Loss: 0.0967, Tr Acc: 96.9397, Val Loss: 0.2349, Val Acc: 89.9152\n",
      "Epoch 281/1000, Tr Loss: 0.1038, Tr Acc: 96.2806, Val Loss: 0.5331, Val Acc: 77.7568\n",
      "Epoch 291/1000, Tr Loss: 0.0887, Tr Acc: 96.8456, Val Loss: 0.6240, Val Acc: 75.5891\n",
      "Epoch 301/1000, Tr Loss: 0.1027, Tr Acc: 96.7514, Val Loss: 0.3618, Val Acc: 83.2234\n",
      "Epoch 311/1000, Tr Loss: 0.0981, Tr Acc: 96.9397, Val Loss: 0.7546, Val Acc: 72.6673\n",
      "Epoch 321/1000, Tr Loss: 0.0937, Tr Acc: 96.7514, Val Loss: 0.3996, Val Acc: 81.8096\n",
      "Epoch 331/1000, Tr Loss: 0.1014, Tr Acc: 96.4689, Val Loss: 0.3461, Val Acc: 84.4486\n",
      "Epoch 341/1000, Tr Loss: 0.0894, Tr Acc: 97.2222, Val Loss: 0.3169, Val Acc: 85.0141\n",
      "Epoch 351/1000, Tr Loss: 0.0773, Tr Acc: 97.6460, Val Loss: 0.5925, Val Acc: 75.8718\n",
      "Epoch 361/1000, Tr Loss: 0.0971, Tr Acc: 96.7985, Val Loss: 0.3991, Val Acc: 82.9406\n",
      "Epoch 371/1000, Tr Loss: 0.0909, Tr Acc: 97.0339, Val Loss: 0.5278, Val Acc: 77.2856\n",
      "Epoch 381/1000, Tr Loss: 0.0826, Tr Acc: 97.0810, Val Loss: 0.5657, Val Acc: 78.0396\n",
      "Epoch 391/1000, Tr Loss: 0.0789, Tr Acc: 97.6460, Val Loss: 0.5602, Val Acc: 77.5683\n",
      "Epoch 401/1000, Tr Loss: 0.0779, Tr Acc: 97.5989, Val Loss: 0.3592, Val Acc: 83.3176\n",
      "Epoch 411/1000, Tr Loss: 0.0835, Tr Acc: 97.3635, Val Loss: 1.0349, Val Acc: 69.0858\n",
      "Epoch 421/1000, Tr Loss: 0.0781, Tr Acc: 97.7872, Val Loss: 0.4541, Val Acc: 80.1131\n",
      "Epoch 431/1000, Tr Loss: 0.0767, Tr Acc: 97.7872, Val Loss: 0.3869, Val Acc: 82.5636\n",
      "Epoch 441/1000, Tr Loss: 0.0801, Tr Acc: 97.9284, Val Loss: 0.3453, Val Acc: 85.7681\n",
      "Epoch 451/1000, Tr Loss: 0.0813, Tr Acc: 97.5989, Val Loss: 1.7104, Val Acc: 63.1480\n",
      "Epoch 461/1000, Tr Loss: 0.0685, Tr Acc: 98.0697, Val Loss: 0.6218, Val Acc: 76.3431\n",
      "Epoch 471/1000, Tr Loss: 0.0813, Tr Acc: 97.3635, Val Loss: 1.5814, Val Acc: 64.8445\n",
      "Epoch 481/1000, Tr Loss: 0.0738, Tr Acc: 97.6930, Val Loss: 0.8996, Val Acc: 73.2328\n",
      "Epoch 491/1000, Tr Loss: 0.0758, Tr Acc: 97.8814, Val Loss: 0.4541, Val Acc: 80.1131\n",
      "Epoch 501/1000, Tr Loss: 0.0708, Tr Acc: 97.8814, Val Loss: 0.3322, Val Acc: 85.2026\n",
      "Epoch 511/1000, Tr Loss: 0.0684, Tr Acc: 97.9284, Val Loss: 0.3981, Val Acc: 82.6579\n",
      "Epoch 521/1000, Tr Loss: 0.0667, Tr Acc: 97.9284, Val Loss: 2.1009, Val Acc: 61.2630\n",
      "Epoch 531/1000, Tr Loss: 0.0780, Tr Acc: 97.1751, Val Loss: 0.3948, Val Acc: 82.7521\n",
      "Epoch 541/1000, Tr Loss: 0.0682, Tr Acc: 98.0697, Val Loss: 0.6848, Val Acc: 74.0811\n",
      "Epoch 551/1000, Tr Loss: 0.0667, Tr Acc: 98.2109, Val Loss: 0.4265, Val Acc: 82.0924\n",
      "Epoch 561/1000, Tr Loss: 0.0742, Tr Acc: 97.5518, Val Loss: 0.4900, Val Acc: 79.5476\n",
      "Epoch 571/1000, Tr Loss: 0.0672, Tr Acc: 97.9755, Val Loss: 0.4605, Val Acc: 78.9821\n",
      "Epoch 581/1000, Tr Loss: 0.0702, Tr Acc: 97.7401, Val Loss: 0.5084, Val Acc: 79.4533\n",
      "Epoch 591/1000, Tr Loss: 0.0633, Tr Acc: 97.9284, Val Loss: 3.5459, Val Acc: 57.9642\n",
      "Epoch 601/1000, Tr Loss: 0.0627, Tr Acc: 98.3051, Val Loss: 1.1760, Val Acc: 68.8030\n",
      "Epoch 611/1000, Tr Loss: 0.0658, Tr Acc: 98.3051, Val Loss: 0.5096, Val Acc: 79.1706\n",
      "Epoch 621/1000, Tr Loss: 0.0676, Tr Acc: 98.0697, Val Loss: 2.0737, Val Acc: 64.2790\n",
      "Epoch 631/1000, Tr Loss: 0.0655, Tr Acc: 97.7401, Val Loss: 0.6206, Val Acc: 77.2856\n",
      "Epoch 641/1000, Tr Loss: 0.0692, Tr Acc: 97.4576, Val Loss: 0.4442, Val Acc: 81.2441\n",
      "Epoch 651/1000, Tr Loss: 0.0625, Tr Acc: 98.0697, Val Loss: 0.8935, Val Acc: 72.5730\n",
      "Epoch 661/1000, Tr Loss: 0.0649, Tr Acc: 98.4463, Val Loss: 2.0302, Val Acc: 63.6192\n",
      "Epoch 671/1000, Tr Loss: 0.0571, Tr Acc: 98.7288, Val Loss: 1.3701, Val Acc: 64.8445\n",
      "Epoch 681/1000, Tr Loss: 0.0634, Tr Acc: 98.0697, Val Loss: 1.2561, Val Acc: 67.4835\n",
      "Epoch 691/1000, Tr Loss: 0.0630, Tr Acc: 98.2109, Val Loss: 0.4393, Val Acc: 81.4326\n",
      "Epoch 701/1000, Tr Loss: 0.0525, Tr Acc: 98.6347, Val Loss: 0.9349, Val Acc: 71.4420\n",
      "Epoch 711/1000, Tr Loss: 0.0612, Tr Acc: 98.3051, Val Loss: 0.7818, Val Acc: 71.7248\n",
      "Epoch 721/1000, Tr Loss: 0.0638, Tr Acc: 98.3522, Val Loss: 0.4240, Val Acc: 83.3176\n",
      "Epoch 731/1000, Tr Loss: 0.0558, Tr Acc: 98.5405, Val Loss: 0.4022, Val Acc: 83.7889\n",
      "Epoch 741/1000, Tr Loss: 0.0567, Tr Acc: 98.4934, Val Loss: 0.9632, Val Acc: 71.6305\n",
      "Epoch 751/1000, Tr Loss: 0.0546, Tr Acc: 98.3992, Val Loss: 0.5657, Val Acc: 78.3223\n",
      "Epoch 761/1000, Tr Loss: 0.0558, Tr Acc: 98.3522, Val Loss: 0.6886, Val Acc: 72.6673\n",
      "Epoch 771/1000, Tr Loss: 0.0581, Tr Acc: 98.3051, Val Loss: 0.7783, Val Acc: 73.0443\n",
      "Epoch 781/1000, Tr Loss: 0.0569, Tr Acc: 98.7759, Val Loss: 2.2668, Val Acc: 62.2055\n",
      "Epoch 791/1000, Tr Loss: 0.0568, Tr Acc: 98.6347, Val Loss: 1.6945, Val Acc: 65.4100\n",
      "Epoch 801/1000, Tr Loss: 0.0543, Tr Acc: 98.8230, Val Loss: 0.5467, Val Acc: 78.2281\n",
      "Epoch 811/1000, Tr Loss: 0.0529, Tr Acc: 98.3522, Val Loss: 0.6512, Val Acc: 75.7776\n",
      "Epoch 821/1000, Tr Loss: 0.0566, Tr Acc: 98.2109, Val Loss: 0.4945, Val Acc: 81.4326\n",
      "Epoch 831/1000, Tr Loss: 0.0576, Tr Acc: 98.1638, Val Loss: 0.7064, Val Acc: 74.7408\n",
      "Epoch 841/1000, Tr Loss: 0.0566, Tr Acc: 98.3051, Val Loss: 2.2442, Val Acc: 61.4515\n",
      "Epoch 851/1000, Tr Loss: 0.0491, Tr Acc: 98.9642, Val Loss: 1.2518, Val Acc: 67.4835\n",
      "Epoch 861/1000, Tr Loss: 0.0498, Tr Acc: 98.6817, Val Loss: 3.1398, Val Acc: 58.4354\n",
      "Epoch 871/1000, Tr Loss: 0.0556, Tr Acc: 98.6347, Val Loss: 0.5066, Val Acc: 79.5476\n",
      "Epoch 881/1000, Tr Loss: 0.0509, Tr Acc: 98.5876, Val Loss: 0.7053, Val Acc: 74.5523\n",
      "Epoch 891/1000, Tr Loss: 0.0462, Tr Acc: 98.9642, Val Loss: 0.5356, Val Acc: 78.6051\n",
      "Epoch 901/1000, Tr Loss: 0.0572, Tr Acc: 98.4934, Val Loss: 1.4805, Val Acc: 64.4675\n",
      "Epoch 911/1000, Tr Loss: 0.0528, Tr Acc: 98.6347, Val Loss: 3.7195, Val Acc: 55.7964\n",
      "Epoch 921/1000, Tr Loss: 0.0524, Tr Acc: 98.4934, Val Loss: 0.7459, Val Acc: 73.9868\n",
      "Epoch 931/1000, Tr Loss: 0.0527, Tr Acc: 98.7759, Val Loss: 0.6523, Val Acc: 76.1546\n",
      "Epoch 941/1000, Tr Loss: 0.0489, Tr Acc: 98.5876, Val Loss: 0.8290, Val Acc: 73.1385\n",
      "Epoch 951/1000, Tr Loss: 0.0474, Tr Acc: 98.6817, Val Loss: 0.5907, Val Acc: 76.9086\n",
      "Epoch 961/1000, Tr Loss: 0.0425, Tr Acc: 98.7288, Val Loss: 1.4428, Val Acc: 65.7870\n",
      "Epoch 971/1000, Tr Loss: 0.0518, Tr Acc: 98.6347, Val Loss: 0.8802, Val Acc: 72.4788\n",
      "Epoch 981/1000, Tr Loss: 0.0529, Tr Acc: 98.5876, Val Loss: 0.6596, Val Acc: 76.5316\n",
      "Epoch 991/1000, Tr Loss: 0.0494, Tr Acc: 98.8701, Val Loss: 1.1101, Val Acc: 70.6880\n",
      "Fold 1\n",
      "Epoch 1/1000, Tr Loss: 0.3389, Tr Acc: 90.5794, Val Loss: 1.6155, Val Acc: 64.3126\n",
      "Epoch 11/1000, Tr Loss: 0.2090, Tr Acc: 92.4635, Val Loss: 1.9275, Val Acc: 64.7834\n",
      "Epoch 21/1000, Tr Loss: 0.1912, Tr Acc: 92.7461, Val Loss: 1.4380, Val Acc: 66.6667\n",
      "Epoch 31/1000, Tr Loss: 0.1555, Tr Acc: 93.9708, Val Loss: 2.1058, Val Acc: 64.0301\n",
      "Epoch 41/1000, Tr Loss: 0.1462, Tr Acc: 94.1592, Val Loss: 2.6755, Val Acc: 60.9228\n",
      "Epoch 51/1000, Tr Loss: 0.1339, Tr Acc: 94.8658, Val Loss: 0.3652, Val Acc: 83.6158\n",
      "Epoch 61/1000, Tr Loss: 0.1261, Tr Acc: 95.5252, Val Loss: 0.2069, Val Acc: 90.3955\n",
      "Epoch 71/1000, Tr Loss: 0.1202, Tr Acc: 95.2897, Val Loss: 1.4573, Val Acc: 66.0075\n",
      "Epoch 81/1000, Tr Loss: 0.1049, Tr Acc: 96.0904, Val Loss: 0.7851, Val Acc: 74.9529\n",
      "Epoch 91/1000, Tr Loss: 0.0897, Tr Acc: 96.6086, Val Loss: 0.4986, Val Acc: 79.4727\n",
      "Epoch 101/1000, Tr Loss: 0.1120, Tr Acc: 95.9020, Val Loss: 0.6132, Val Acc: 76.9303\n",
      "Epoch 111/1000, Tr Loss: 0.0993, Tr Acc: 96.4202, Val Loss: 0.4408, Val Acc: 79.7552\n",
      "Epoch 121/1000, Tr Loss: 0.0843, Tr Acc: 96.8912, Val Loss: 0.2567, Val Acc: 88.5122\n",
      "Epoch 131/1000, Tr Loss: 0.0871, Tr Acc: 96.9383, Val Loss: 0.2650, Val Acc: 87.2881\n",
      "Epoch 141/1000, Tr Loss: 0.0913, Tr Acc: 96.7028, Val Loss: 0.2312, Val Acc: 89.3597\n",
      "Epoch 151/1000, Tr Loss: 0.0995, Tr Acc: 95.9962, Val Loss: 0.2101, Val Acc: 92.0904\n",
      "Epoch 161/1000, Tr Loss: 0.0915, Tr Acc: 96.7028, Val Loss: 0.3820, Val Acc: 84.1808\n",
      "Epoch 171/1000, Tr Loss: 0.0800, Tr Acc: 97.3622, Val Loss: 1.0366, Val Acc: 72.1281\n",
      "Epoch 181/1000, Tr Loss: 0.0937, Tr Acc: 96.5615, Val Loss: 0.7971, Val Acc: 72.6930\n",
      "Epoch 191/1000, Tr Loss: 0.0742, Tr Acc: 97.7390, Val Loss: 0.7607, Val Acc: 75.0471\n",
      "Epoch 201/1000, Tr Loss: 0.0776, Tr Acc: 97.2680, Val Loss: 0.3136, Val Acc: 85.4991\n",
      "Epoch 211/1000, Tr Loss: 0.0792, Tr Acc: 97.1267, Val Loss: 0.2102, Val Acc: 91.2429\n",
      "Epoch 221/1000, Tr Loss: 0.0689, Tr Acc: 98.0217, Val Loss: 0.2174, Val Acc: 91.0546\n",
      "Epoch 231/1000, Tr Loss: 0.0719, Tr Acc: 97.5977, Val Loss: 0.3678, Val Acc: 82.1092\n",
      "Epoch 241/1000, Tr Loss: 0.0723, Tr Acc: 97.4564, Val Loss: 1.5059, Val Acc: 65.3484\n",
      "Epoch 251/1000, Tr Loss: 0.0726, Tr Acc: 97.6919, Val Loss: 0.2266, Val Acc: 90.3955\n",
      "Epoch 261/1000, Tr Loss: 0.0770, Tr Acc: 98.0217, Val Loss: 2.1207, Val Acc: 63.5593\n",
      "Epoch 271/1000, Tr Loss: 0.0715, Tr Acc: 97.5977, Val Loss: 0.3017, Val Acc: 86.5348\n",
      "Epoch 281/1000, Tr Loss: 0.0705, Tr Acc: 97.8333, Val Loss: 0.3615, Val Acc: 85.0282\n",
      "Epoch 291/1000, Tr Loss: 0.0685, Tr Acc: 97.9746, Val Loss: 0.7143, Val Acc: 74.5763\n",
      "Epoch 301/1000, Tr Loss: 0.0709, Tr Acc: 97.6919, Val Loss: 0.6662, Val Acc: 76.1770\n",
      "Epoch 311/1000, Tr Loss: 0.0638, Tr Acc: 98.3043, Val Loss: 6.7098, Val Acc: 50.8475\n",
      "Epoch 321/1000, Tr Loss: 0.0623, Tr Acc: 98.2572, Val Loss: 0.2378, Val Acc: 88.6064\n",
      "Epoch 331/1000, Tr Loss: 0.0601, Tr Acc: 97.9275, Val Loss: 1.4503, Val Acc: 64.6893\n",
      "Epoch 341/1000, Tr Loss: 0.0634, Tr Acc: 98.2572, Val Loss: 1.9636, Val Acc: 64.3126\n",
      "Epoch 351/1000, Tr Loss: 0.0641, Tr Acc: 98.4456, Val Loss: 1.2084, Val Acc: 71.7514\n",
      "Epoch 361/1000, Tr Loss: 0.0568, Tr Acc: 98.3985, Val Loss: 2.6683, Val Acc: 59.8870\n",
      "Epoch 371/1000, Tr Loss: 0.0593, Tr Acc: 98.4927, Val Loss: 1.4795, Val Acc: 65.4426\n",
      "Epoch 381/1000, Tr Loss: 0.0551, Tr Acc: 98.4927, Val Loss: 0.4159, Val Acc: 80.8851\n",
      "Epoch 391/1000, Tr Loss: 0.0594, Tr Acc: 98.4456, Val Loss: 0.5547, Val Acc: 78.6252\n",
      "Epoch 401/1000, Tr Loss: 0.0556, Tr Acc: 98.3043, Val Loss: 1.1449, Val Acc: 68.1733\n",
      "Epoch 411/1000, Tr Loss: 0.0537, Tr Acc: 98.8224, Val Loss: 0.8718, Val Acc: 74.9529\n",
      "Epoch 421/1000, Tr Loss: 0.0565, Tr Acc: 98.0688, Val Loss: 2.3798, Val Acc: 63.0885\n",
      "Epoch 431/1000, Tr Loss: 0.0680, Tr Acc: 97.9746, Val Loss: 0.3269, Val Acc: 86.8173\n",
      "Epoch 441/1000, Tr Loss: 0.0576, Tr Acc: 98.3514, Val Loss: 0.2989, Val Acc: 87.1940\n",
      "Epoch 451/1000, Tr Loss: 0.0649, Tr Acc: 97.9746, Val Loss: 1.1763, Val Acc: 69.1149\n",
      "Epoch 461/1000, Tr Loss: 0.0542, Tr Acc: 98.3514, Val Loss: 0.3624, Val Acc: 82.3917\n",
      "Epoch 471/1000, Tr Loss: 0.0620, Tr Acc: 97.8804, Val Loss: 0.6559, Val Acc: 76.7420\n",
      "Epoch 481/1000, Tr Loss: 0.0479, Tr Acc: 98.7282, Val Loss: 0.4158, Val Acc: 84.0866\n",
      "Epoch 491/1000, Tr Loss: 0.0633, Tr Acc: 97.8804, Val Loss: 2.4230, Val Acc: 64.7834\n",
      "Epoch 501/1000, Tr Loss: 0.0483, Tr Acc: 99.0579, Val Loss: 2.0865, Val Acc: 64.5951\n",
      "Epoch 511/1000, Tr Loss: 0.0521, Tr Acc: 98.3514, Val Loss: 0.4474, Val Acc: 81.3559\n",
      "Epoch 521/1000, Tr Loss: 0.0510, Tr Acc: 98.4456, Val Loss: 0.3033, Val Acc: 87.1940\n",
      "Epoch 531/1000, Tr Loss: 0.0507, Tr Acc: 98.5869, Val Loss: 2.2998, Val Acc: 61.2994\n",
      "Epoch 541/1000, Tr Loss: 0.0458, Tr Acc: 99.0579, Val Loss: 0.4341, Val Acc: 82.2976\n",
      "Epoch 551/1000, Tr Loss: 0.0517, Tr Acc: 98.5869, Val Loss: 3.9647, Val Acc: 58.7571\n",
      "Epoch 561/1000, Tr Loss: 0.0480, Tr Acc: 98.6811, Val Loss: 0.7026, Val Acc: 74.7646\n",
      "Epoch 571/1000, Tr Loss: 0.0550, Tr Acc: 98.4456, Val Loss: 0.3788, Val Acc: 84.5574\n",
      "Epoch 581/1000, Tr Loss: 0.0517, Tr Acc: 98.3514, Val Loss: 0.7495, Val Acc: 77.4011\n",
      "Epoch 591/1000, Tr Loss: 0.0425, Tr Acc: 98.8224, Val Loss: 1.0366, Val Acc: 71.1864\n",
      "Epoch 601/1000, Tr Loss: 0.0483, Tr Acc: 98.5869, Val Loss: 0.4248, Val Acc: 83.7100\n",
      "Epoch 611/1000, Tr Loss: 0.0476, Tr Acc: 98.9637, Val Loss: 0.5827, Val Acc: 77.8719\n",
      "Epoch 621/1000, Tr Loss: 0.0468, Tr Acc: 98.6340, Val Loss: 0.5038, Val Acc: 81.4501\n",
      "Epoch 631/1000, Tr Loss: 0.0451, Tr Acc: 98.8695, Val Loss: 0.4254, Val Acc: 83.2392\n",
      "Epoch 641/1000, Tr Loss: 0.0438, Tr Acc: 98.7282, Val Loss: 0.4202, Val Acc: 82.5800\n",
      "Epoch 651/1000, Tr Loss: 0.0481, Tr Acc: 98.4927, Val Loss: 0.5995, Val Acc: 77.7778\n",
      "Epoch 661/1000, Tr Loss: 0.0393, Tr Acc: 99.1050, Val Loss: 1.7878, Val Acc: 65.3484\n",
      "Epoch 671/1000, Tr Loss: 0.0426, Tr Acc: 99.0579, Val Loss: 1.6826, Val Acc: 66.1017\n",
      "Epoch 681/1000, Tr Loss: 0.0429, Tr Acc: 99.1992, Val Loss: 0.4878, Val Acc: 81.9209\n",
      "Epoch 691/1000, Tr Loss: 0.0418, Tr Acc: 98.9637, Val Loss: 0.9754, Val Acc: 72.4105\n",
      "Epoch 701/1000, Tr Loss: 0.0476, Tr Acc: 98.5869, Val Loss: 0.5576, Val Acc: 80.8851\n",
      "Epoch 711/1000, Tr Loss: 0.0367, Tr Acc: 99.1992, Val Loss: 1.1785, Val Acc: 67.6083\n",
      "Epoch 721/1000, Tr Loss: 0.0396, Tr Acc: 99.1521, Val Loss: 1.1201, Val Acc: 73.5405\n",
      "Epoch 731/1000, Tr Loss: 0.0386, Tr Acc: 99.0579, Val Loss: 1.7114, Val Acc: 64.0301\n",
      "Epoch 741/1000, Tr Loss: 0.0487, Tr Acc: 98.7282, Val Loss: 0.8289, Val Acc: 74.4821\n",
      "Epoch 751/1000, Tr Loss: 0.0424, Tr Acc: 99.1521, Val Loss: 1.5043, Val Acc: 69.2090\n",
      "Epoch 761/1000, Tr Loss: 0.0325, Tr Acc: 99.1050, Val Loss: 2.2283, Val Acc: 64.5951\n",
      "Epoch 771/1000, Tr Loss: 0.0402, Tr Acc: 98.7282, Val Loss: 0.4295, Val Acc: 85.3107\n",
      "Epoch 781/1000, Tr Loss: 0.0470, Tr Acc: 98.9637, Val Loss: 4.1204, Val Acc: 59.2279\n",
      "Epoch 791/1000, Tr Loss: 0.0448, Tr Acc: 98.9637, Val Loss: 0.5882, Val Acc: 78.9077\n",
      "Epoch 801/1000, Tr Loss: 0.0336, Tr Acc: 99.2934, Val Loss: 1.2807, Val Acc: 69.8682\n",
      "Epoch 811/1000, Tr Loss: 0.0393, Tr Acc: 98.6811, Val Loss: 1.5490, Val Acc: 65.0659\n",
      "Epoch 821/1000, Tr Loss: 0.0360, Tr Acc: 99.2463, Val Loss: 0.8223, Val Acc: 74.6704\n",
      "Epoch 831/1000, Tr Loss: 0.0403, Tr Acc: 98.5869, Val Loss: 1.6285, Val Acc: 66.2900\n",
      "Epoch 841/1000, Tr Loss: 0.0362, Tr Acc: 98.8695, Val Loss: 0.5100, Val Acc: 81.4501\n",
      "Epoch 851/1000, Tr Loss: 0.0384, Tr Acc: 98.7282, Val Loss: 3.7260, Val Acc: 60.6403\n",
      "Epoch 861/1000, Tr Loss: 0.0457, Tr Acc: 98.8695, Val Loss: 1.0079, Val Acc: 71.9397\n",
      "Epoch 871/1000, Tr Loss: 0.0417, Tr Acc: 98.9637, Val Loss: 1.4308, Val Acc: 66.1959\n",
      "Epoch 881/1000, Tr Loss: 0.0338, Tr Acc: 98.9637, Val Loss: 5.8386, Val Acc: 55.7439\n",
      "Epoch 891/1000, Tr Loss: 0.0353, Tr Acc: 99.1992, Val Loss: 0.6226, Val Acc: 79.0019\n",
      "Epoch 901/1000, Tr Loss: 0.0376, Tr Acc: 99.1521, Val Loss: 0.6347, Val Acc: 77.8719\n",
      "Epoch 911/1000, Tr Loss: 0.0415, Tr Acc: 98.7282, Val Loss: 1.5070, Val Acc: 63.5593\n",
      "Epoch 921/1000, Tr Loss: 0.0379, Tr Acc: 98.9166, Val Loss: 0.6080, Val Acc: 77.8719\n",
      "Epoch 931/1000, Tr Loss: 0.0325, Tr Acc: 99.1521, Val Loss: 0.7240, Val Acc: 75.3296\n",
      "Epoch 941/1000, Tr Loss: 0.0356, Tr Acc: 99.2934, Val Loss: 0.8013, Val Acc: 73.5405\n",
      "Epoch 951/1000, Tr Loss: 0.0412, Tr Acc: 98.7282, Val Loss: 1.4781, Val Acc: 66.9492\n",
      "Epoch 961/1000, Tr Loss: 0.0346, Tr Acc: 99.1992, Val Loss: 0.5809, Val Acc: 80.3202\n",
      "Epoch 971/1000, Tr Loss: 0.0360, Tr Acc: 99.2934, Val Loss: 1.0528, Val Acc: 71.9397\n",
      "Epoch 981/1000, Tr Loss: 0.0318, Tr Acc: 99.4348, Val Loss: 0.6668, Val Acc: 78.3427\n",
      "Epoch 991/1000, Tr Loss: 0.0371, Tr Acc: 99.0108, Val Loss: 0.5968, Val Acc: 79.5669\n",
      "Fold 2\n",
      "Epoch 1/1000, Tr Loss: 0.2603, Tr Acc: 92.5106, Val Loss: 13.0027, Val Acc: 52.5424\n",
      "Epoch 11/1000, Tr Loss: 0.1658, Tr Acc: 93.5469, Val Loss: 0.4496, Val Acc: 81.2618\n",
      "Epoch 21/1000, Tr Loss: 0.1406, Tr Acc: 94.8186, Val Loss: 0.5115, Val Acc: 81.9209\n",
      "Epoch 31/1000, Tr Loss: 0.1229, Tr Acc: 95.5723, Val Loss: 0.2022, Val Acc: 90.1130\n",
      "Epoch 41/1000, Tr Loss: 0.1074, Tr Acc: 96.2318, Val Loss: 0.1178, Val Acc: 94.9153\n",
      "Epoch 51/1000, Tr Loss: 0.1047, Tr Acc: 95.9020, Val Loss: 0.5732, Val Acc: 82.3917\n",
      "Epoch 61/1000, Tr Loss: 0.1130, Tr Acc: 95.7136, Val Loss: 0.4727, Val Acc: 78.0603\n",
      "Epoch 71/1000, Tr Loss: 0.1060, Tr Acc: 96.3731, Val Loss: 0.3771, Val Acc: 84.1808\n",
      "Epoch 81/1000, Tr Loss: 0.0854, Tr Acc: 96.5615, Val Loss: 0.1633, Val Acc: 92.9379\n",
      "Epoch 91/1000, Tr Loss: 0.0776, Tr Acc: 97.2209, Val Loss: 0.2171, Val Acc: 89.0772\n",
      "Epoch 101/1000, Tr Loss: 0.0810, Tr Acc: 96.8441, Val Loss: 1.8588, Val Acc: 63.3710\n",
      "Epoch 111/1000, Tr Loss: 0.0769, Tr Acc: 96.9383, Val Loss: 0.2046, Val Acc: 89.3597\n",
      "Epoch 121/1000, Tr Loss: 0.0788, Tr Acc: 96.9854, Val Loss: 0.1584, Val Acc: 93.1262\n",
      "Epoch 131/1000, Tr Loss: 0.0694, Tr Acc: 97.8333, Val Loss: 2.2633, Val Acc: 60.2637\n",
      "Epoch 141/1000, Tr Loss: 0.0746, Tr Acc: 97.8804, Val Loss: 2.9591, Val Acc: 59.3220\n",
      "Epoch 151/1000, Tr Loss: 0.0730, Tr Acc: 97.3151, Val Loss: 0.4002, Val Acc: 83.3333\n",
      "Epoch 161/1000, Tr Loss: 0.0615, Tr Acc: 97.9746, Val Loss: 0.1473, Val Acc: 93.4087\n",
      "Epoch 171/1000, Tr Loss: 0.0660, Tr Acc: 97.8804, Val Loss: 1.3949, Val Acc: 67.0433\n",
      "Epoch 181/1000, Tr Loss: 0.0695, Tr Acc: 97.4093, Val Loss: 0.8701, Val Acc: 75.0471\n",
      "Epoch 191/1000, Tr Loss: 0.0705, Tr Acc: 97.6919, Val Loss: 0.4708, Val Acc: 82.1092\n",
      "Epoch 201/1000, Tr Loss: 0.0603, Tr Acc: 98.1630, Val Loss: 1.7416, Val Acc: 67.0433\n",
      "Epoch 211/1000, Tr Loss: 0.0606, Tr Acc: 98.0688, Val Loss: 0.3195, Val Acc: 84.9341\n",
      "Epoch 221/1000, Tr Loss: 0.0670, Tr Acc: 97.7861, Val Loss: 0.1778, Val Acc: 93.0320\n",
      "Epoch 231/1000, Tr Loss: 0.0586, Tr Acc: 98.1630, Val Loss: 2.6884, Val Acc: 61.4878\n",
      "Epoch 241/1000, Tr Loss: 0.0655, Tr Acc: 97.5506, Val Loss: 0.2717, Val Acc: 87.2881\n",
      "Epoch 251/1000, Tr Loss: 0.0586, Tr Acc: 98.1159, Val Loss: 1.7666, Val Acc: 61.8644\n",
      "Epoch 261/1000, Tr Loss: 0.0577, Tr Acc: 98.0688, Val Loss: 2.8609, Val Acc: 63.4652\n",
      "Epoch 271/1000, Tr Loss: 0.0605, Tr Acc: 97.9746, Val Loss: 2.1737, Val Acc: 67.3258\n",
      "Epoch 281/1000, Tr Loss: 0.0593, Tr Acc: 98.2572, Val Loss: 0.2144, Val Acc: 89.3597\n",
      "Epoch 291/1000, Tr Loss: 0.0591, Tr Acc: 97.7861, Val Loss: 0.1546, Val Acc: 92.8437\n",
      "Epoch 301/1000, Tr Loss: 0.0504, Tr Acc: 98.2572, Val Loss: 0.2508, Val Acc: 88.2298\n",
      "Epoch 311/1000, Tr Loss: 0.0567, Tr Acc: 98.2101, Val Loss: 0.9494, Val Acc: 69.3974\n",
      "Epoch 321/1000, Tr Loss: 0.0583, Tr Acc: 98.1630, Val Loss: 0.2421, Val Acc: 90.2072\n",
      "Epoch 331/1000, Tr Loss: 0.0521, Tr Acc: 98.3043, Val Loss: 7.6720, Val Acc: 55.1789\n",
      "Epoch 341/1000, Tr Loss: 0.0557, Tr Acc: 98.4927, Val Loss: 0.2921, Val Acc: 86.6290\n",
      "Epoch 351/1000, Tr Loss: 0.0478, Tr Acc: 98.5398, Val Loss: 0.1325, Val Acc: 93.8795\n",
      "Epoch 361/1000, Tr Loss: 0.0553, Tr Acc: 98.1159, Val Loss: 0.5110, Val Acc: 80.6968\n",
      "Epoch 371/1000, Tr Loss: 0.0431, Tr Acc: 98.4927, Val Loss: 2.9326, Val Acc: 58.6629\n",
      "Epoch 381/1000, Tr Loss: 0.0540, Tr Acc: 98.2101, Val Loss: 2.3037, Val Acc: 63.9360\n",
      "Epoch 391/1000, Tr Loss: 0.0519, Tr Acc: 98.6811, Val Loss: 0.4166, Val Acc: 82.1092\n",
      "Epoch 401/1000, Tr Loss: 0.0409, Tr Acc: 98.8224, Val Loss: 3.6709, Val Acc: 61.3936\n",
      "Epoch 411/1000, Tr Loss: 0.0521, Tr Acc: 98.8224, Val Loss: 3.2701, Val Acc: 56.6855\n",
      "Epoch 421/1000, Tr Loss: 0.0493, Tr Acc: 98.5869, Val Loss: 0.5377, Val Acc: 81.3559\n",
      "Epoch 431/1000, Tr Loss: 0.0427, Tr Acc: 98.7282, Val Loss: 0.3512, Val Acc: 84.3691\n",
      "Epoch 441/1000, Tr Loss: 0.0465, Tr Acc: 98.6340, Val Loss: 0.3849, Val Acc: 85.4991\n",
      "Epoch 451/1000, Tr Loss: 0.0424, Tr Acc: 98.8224, Val Loss: 2.9854, Val Acc: 57.2505\n",
      "Epoch 461/1000, Tr Loss: 0.0464, Tr Acc: 98.9637, Val Loss: 0.3070, Val Acc: 86.4407\n",
      "Epoch 471/1000, Tr Loss: 0.0422, Tr Acc: 99.0579, Val Loss: 0.2779, Val Acc: 87.3823\n",
      "Epoch 481/1000, Tr Loss: 0.0372, Tr Acc: 99.0579, Val Loss: 1.1160, Val Acc: 70.0565\n",
      "Epoch 491/1000, Tr Loss: 0.0444, Tr Acc: 98.6340, Val Loss: 0.4070, Val Acc: 85.0282\n",
      "Epoch 501/1000, Tr Loss: 0.0433, Tr Acc: 99.2934, Val Loss: 0.2310, Val Acc: 89.6422\n",
      "Epoch 511/1000, Tr Loss: 0.0447, Tr Acc: 98.6340, Val Loss: 0.5432, Val Acc: 81.9209\n",
      "Epoch 521/1000, Tr Loss: 0.0379, Tr Acc: 99.2934, Val Loss: 2.7990, Val Acc: 61.2053\n",
      "Epoch 531/1000, Tr Loss: 0.0411, Tr Acc: 99.1521, Val Loss: 4.2969, Val Acc: 58.0979\n",
      "Epoch 541/1000, Tr Loss: 0.0444, Tr Acc: 98.4456, Val Loss: 1.2520, Val Acc: 68.7382\n",
      "Epoch 551/1000, Tr Loss: 0.0402, Tr Acc: 98.9637, Val Loss: 4.0296, Val Acc: 61.0169\n",
      "Epoch 561/1000, Tr Loss: 0.0490, Tr Acc: 98.5869, Val Loss: 1.1682, Val Acc: 74.0113\n",
      "Epoch 571/1000, Tr Loss: 0.0400, Tr Acc: 98.6811, Val Loss: 0.6561, Val Acc: 78.8136\n",
      "Epoch 581/1000, Tr Loss: 0.0364, Tr Acc: 99.0108, Val Loss: 0.3933, Val Acc: 83.5217\n",
      "Epoch 591/1000, Tr Loss: 0.0358, Tr Acc: 99.2934, Val Loss: 0.3788, Val Acc: 85.3107\n",
      "Epoch 601/1000, Tr Loss: 0.0432, Tr Acc: 98.8224, Val Loss: 0.3370, Val Acc: 84.1808\n",
      "Epoch 611/1000, Tr Loss: 0.0370, Tr Acc: 99.0108, Val Loss: 1.2191, Val Acc: 70.4331\n",
      "Epoch 621/1000, Tr Loss: 0.0491, Tr Acc: 98.7282, Val Loss: 1.5392, Val Acc: 70.5273\n",
      "Epoch 631/1000, Tr Loss: 0.0367, Tr Acc: 98.9637, Val Loss: 1.4402, Val Acc: 67.6083\n",
      "Epoch 641/1000, Tr Loss: 0.0376, Tr Acc: 99.1050, Val Loss: 2.5626, Val Acc: 59.0395\n",
      "Epoch 651/1000, Tr Loss: 0.0436, Tr Acc: 98.8695, Val Loss: 1.5537, Val Acc: 67.9849\n",
      "Epoch 661/1000, Tr Loss: 0.0321, Tr Acc: 99.1992, Val Loss: 5.3080, Val Acc: 54.5198\n",
      "Epoch 671/1000, Tr Loss: 0.0342, Tr Acc: 98.9166, Val Loss: 1.8945, Val Acc: 66.9492\n",
      "Epoch 681/1000, Tr Loss: 0.0381, Tr Acc: 99.0108, Val Loss: 0.3954, Val Acc: 83.8983\n",
      "Epoch 691/1000, Tr Loss: 0.0332, Tr Acc: 99.0108, Val Loss: 0.3550, Val Acc: 86.8173\n",
      "Epoch 701/1000, Tr Loss: 0.0310, Tr Acc: 99.1050, Val Loss: 0.5214, Val Acc: 81.4501\n",
      "Epoch 711/1000, Tr Loss: 0.0422, Tr Acc: 98.5398, Val Loss: 0.2877, Val Acc: 88.7006\n",
      "Epoch 721/1000, Tr Loss: 0.0341, Tr Acc: 99.0108, Val Loss: 2.5057, Val Acc: 58.0038\n",
      "Epoch 731/1000, Tr Loss: 0.0341, Tr Acc: 99.0108, Val Loss: 0.9231, Val Acc: 72.9755\n",
      "Epoch 741/1000, Tr Loss: 0.0347, Tr Acc: 98.9166, Val Loss: 0.9759, Val Acc: 70.9040\n",
      "Epoch 751/1000, Tr Loss: 0.0339, Tr Acc: 99.0108, Val Loss: 0.3186, Val Acc: 86.8173\n",
      "Epoch 761/1000, Tr Loss: 0.0314, Tr Acc: 99.1992, Val Loss: 4.4969, Val Acc: 53.6723\n",
      "Epoch 771/1000, Tr Loss: 0.0345, Tr Acc: 99.0579, Val Loss: 0.5318, Val Acc: 80.4143\n",
      "Epoch 781/1000, Tr Loss: 0.0391, Tr Acc: 98.8224, Val Loss: 4.8272, Val Acc: 54.8023\n",
      "Epoch 791/1000, Tr Loss: 0.0283, Tr Acc: 99.4348, Val Loss: 0.4964, Val Acc: 82.4859\n",
      "Epoch 801/1000, Tr Loss: 0.0293, Tr Acc: 99.3877, Val Loss: 1.3401, Val Acc: 67.5141\n",
      "Epoch 811/1000, Tr Loss: 0.0304, Tr Acc: 99.2934, Val Loss: 1.9028, Val Acc: 66.9492\n",
      "Epoch 821/1000, Tr Loss: 0.0341, Tr Acc: 98.8695, Val Loss: 1.3852, Val Acc: 72.1281\n",
      "Epoch 831/1000, Tr Loss: 0.0339, Tr Acc: 98.9166, Val Loss: 2.0246, Val Acc: 65.3484\n",
      "Epoch 841/1000, Tr Loss: 0.0310, Tr Acc: 99.3877, Val Loss: 0.7348, Val Acc: 78.1544\n",
      "Epoch 851/1000, Tr Loss: 0.0394, Tr Acc: 98.8224, Val Loss: 3.0704, Val Acc: 56.8738\n",
      "Epoch 861/1000, Tr Loss: 0.0390, Tr Acc: 99.0108, Val Loss: 0.5480, Val Acc: 82.0151\n",
      "Epoch 871/1000, Tr Loss: 0.0315, Tr Acc: 99.3406, Val Loss: 1.6768, Val Acc: 69.3974\n",
      "Epoch 881/1000, Tr Loss: 0.0364, Tr Acc: 99.2934, Val Loss: 3.1775, Val Acc: 58.0979\n",
      "Epoch 891/1000, Tr Loss: 0.0332, Tr Acc: 99.2934, Val Loss: 1.0883, Val Acc: 71.4689\n",
      "Epoch 901/1000, Tr Loss: 0.0281, Tr Acc: 99.1992, Val Loss: 1.5763, Val Acc: 66.2900\n",
      "Epoch 911/1000, Tr Loss: 0.0293, Tr Acc: 99.2463, Val Loss: 1.1108, Val Acc: 73.7288\n",
      "Epoch 921/1000, Tr Loss: 0.0257, Tr Acc: 99.3406, Val Loss: 0.5192, Val Acc: 82.8625\n",
      "Epoch 931/1000, Tr Loss: 0.0300, Tr Acc: 99.0579, Val Loss: 2.2475, Val Acc: 59.7928\n",
      "Epoch 941/1000, Tr Loss: 0.0297, Tr Acc: 99.2934, Val Loss: 1.0693, Val Acc: 72.8814\n",
      "Epoch 951/1000, Tr Loss: 0.0320, Tr Acc: 99.1521, Val Loss: 0.9556, Val Acc: 76.6478\n",
      "Epoch 961/1000, Tr Loss: 0.0281, Tr Acc: 99.1992, Val Loss: 1.0593, Val Acc: 74.4821\n",
      "Epoch 971/1000, Tr Loss: 0.0364, Tr Acc: 99.0579, Val Loss: 1.3503, Val Acc: 70.9981\n",
      "Epoch 981/1000, Tr Loss: 0.0300, Tr Acc: 99.0108, Val Loss: 0.7736, Val Acc: 79.2844\n",
      "Epoch 991/1000, Tr Loss: 0.0264, Tr Acc: 99.2934, Val Loss: 1.5843, Val Acc: 66.7608\n",
      "Fold 3\n",
      "Epoch 1/1000, Tr Loss: 0.2767, Tr Acc: 92.6083, Val Loss: 22.7163, Val Acc: 49.6701\n",
      "Epoch 11/1000, Tr Loss: 0.1715, Tr Acc: 94.5386, Val Loss: 2.6351, Val Acc: 59.5664\n",
      "Epoch 21/1000, Tr Loss: 0.1265, Tr Acc: 95.2448, Val Loss: 0.5918, Val Acc: 80.6786\n",
      "Epoch 31/1000, Tr Loss: 0.1087, Tr Acc: 95.9040, Val Loss: 0.1763, Val Acc: 91.9887\n",
      "Epoch 41/1000, Tr Loss: 0.0893, Tr Acc: 97.0339, Val Loss: 0.7200, Val Acc: 75.2121\n",
      "Epoch 51/1000, Tr Loss: 0.1021, Tr Acc: 96.6102, Val Loss: 1.5827, Val Acc: 70.2168\n",
      "Epoch 61/1000, Tr Loss: 0.0932, Tr Acc: 96.5631, Val Loss: 0.1746, Val Acc: 91.4232\n",
      "Epoch 71/1000, Tr Loss: 0.0766, Tr Acc: 97.3635, Val Loss: 0.5904, Val Acc: 77.7568\n",
      "Epoch 81/1000, Tr Loss: 0.0896, Tr Acc: 96.8456, Val Loss: 3.2762, Val Acc: 58.3412\n",
      "Epoch 91/1000, Tr Loss: 0.0737, Tr Acc: 97.3635, Val Loss: 0.2574, Val Acc: 87.8417\n",
      "Epoch 101/1000, Tr Loss: 0.0816, Tr Acc: 97.2693, Val Loss: 0.7444, Val Acc: 76.8143\n",
      "Epoch 111/1000, Tr Loss: 0.0739, Tr Acc: 97.3635, Val Loss: 0.1683, Val Acc: 92.4599\n",
      "Epoch 121/1000, Tr Loss: 0.0733, Tr Acc: 97.5989, Val Loss: 0.8018, Val Acc: 73.9868\n",
      "Epoch 131/1000, Tr Loss: 0.0678, Tr Acc: 97.4576, Val Loss: 0.2400, Val Acc: 89.7267\n",
      "Epoch 141/1000, Tr Loss: 0.0629, Tr Acc: 98.1638, Val Loss: 0.0836, Val Acc: 97.3610\n",
      "Epoch 151/1000, Tr Loss: 0.0639, Tr Acc: 97.5518, Val Loss: 0.2348, Val Acc: 89.7267\n",
      "Epoch 161/1000, Tr Loss: 0.0546, Tr Acc: 97.7872, Val Loss: 1.1586, Val Acc: 69.7455\n",
      "Epoch 171/1000, Tr Loss: 0.0648, Tr Acc: 97.5518, Val Loss: 1.1268, Val Acc: 72.1018\n",
      "Epoch 181/1000, Tr Loss: 0.0600, Tr Acc: 97.9284, Val Loss: 0.6132, Val Acc: 77.4741\n",
      "Epoch 191/1000, Tr Loss: 0.0652, Tr Acc: 97.7872, Val Loss: 0.3487, Val Acc: 84.5429\n",
      "Epoch 201/1000, Tr Loss: 0.0591, Tr Acc: 98.0697, Val Loss: 0.2017, Val Acc: 91.4232\n",
      "Epoch 211/1000, Tr Loss: 0.0498, Tr Acc: 98.7288, Val Loss: 0.0948, Val Acc: 96.6070\n",
      "Epoch 221/1000, Tr Loss: 0.0589, Tr Acc: 97.9755, Val Loss: 0.3101, Val Acc: 85.8624\n",
      "Epoch 231/1000, Tr Loss: 0.0511, Tr Acc: 98.3992, Val Loss: 0.2312, Val Acc: 89.2554\n",
      "Epoch 241/1000, Tr Loss: 0.0576, Tr Acc: 98.0226, Val Loss: 2.5716, Val Acc: 59.5664\n",
      "Epoch 251/1000, Tr Loss: 0.0511, Tr Acc: 98.6347, Val Loss: 0.5576, Val Acc: 79.7361\n",
      "Epoch 261/1000, Tr Loss: 0.0527, Tr Acc: 98.3522, Val Loss: 0.2159, Val Acc: 89.9152\n",
      "Epoch 271/1000, Tr Loss: 0.0585, Tr Acc: 98.1168, Val Loss: 0.3963, Val Acc: 83.9774\n",
      "Epoch 281/1000, Tr Loss: 0.0541, Tr Acc: 98.3522, Val Loss: 0.4313, Val Acc: 83.6946\n",
      "Epoch 291/1000, Tr Loss: 0.0518, Tr Acc: 98.2109, Val Loss: 0.7405, Val Acc: 75.8718\n",
      "Epoch 301/1000, Tr Loss: 0.0472, Tr Acc: 98.5405, Val Loss: 0.3306, Val Acc: 85.2969\n",
      "Epoch 311/1000, Tr Loss: 0.0443, Tr Acc: 98.5405, Val Loss: 1.3204, Val Acc: 71.1593\n",
      "Epoch 321/1000, Tr Loss: 0.0528, Tr Acc: 98.0226, Val Loss: 0.2429, Val Acc: 87.7474\n",
      "Epoch 331/1000, Tr Loss: 0.0479, Tr Acc: 98.4463, Val Loss: 0.2562, Val Acc: 88.0302\n",
      "Epoch 341/1000, Tr Loss: 0.0447, Tr Acc: 98.6817, Val Loss: 1.7378, Val Acc: 65.5042\n",
      "Epoch 351/1000, Tr Loss: 0.0418, Tr Acc: 98.4463, Val Loss: 3.5275, Val Acc: 59.5664\n",
      "Epoch 361/1000, Tr Loss: 0.0468, Tr Acc: 98.2580, Val Loss: 0.4434, Val Acc: 84.3544\n",
      "Epoch 371/1000, Tr Loss: 0.0474, Tr Acc: 98.2109, Val Loss: 1.4801, Val Acc: 71.1593\n",
      "Epoch 381/1000, Tr Loss: 0.0456, Tr Acc: 98.6817, Val Loss: 2.0854, Val Acc: 64.5617\n",
      "Epoch 391/1000, Tr Loss: 0.0453, Tr Acc: 98.6347, Val Loss: 1.7172, Val Acc: 69.4628\n",
      "Epoch 401/1000, Tr Loss: 0.0478, Tr Acc: 98.6347, Val Loss: 2.1676, Val Acc: 64.2790\n",
      "Epoch 411/1000, Tr Loss: 0.0407, Tr Acc: 98.6347, Val Loss: 0.1836, Val Acc: 92.3657\n",
      "Epoch 421/1000, Tr Loss: 0.0475, Tr Acc: 98.4463, Val Loss: 2.1117, Val Acc: 65.0330\n",
      "Epoch 431/1000, Tr Loss: 0.0428, Tr Acc: 98.8701, Val Loss: 1.3480, Val Acc: 69.1800\n",
      "Epoch 441/1000, Tr Loss: 0.0495, Tr Acc: 98.6347, Val Loss: 0.2638, Val Acc: 87.4647\n",
      "Epoch 451/1000, Tr Loss: 0.0452, Tr Acc: 98.3522, Val Loss: 0.6720, Val Acc: 77.6626\n",
      "Epoch 461/1000, Tr Loss: 0.0424, Tr Acc: 98.6347, Val Loss: 2.1743, Val Acc: 67.1065\n",
      "Epoch 471/1000, Tr Loss: 0.0368, Tr Acc: 99.1996, Val Loss: 1.4740, Val Acc: 68.5203\n",
      "Epoch 481/1000, Tr Loss: 0.0492, Tr Acc: 98.6817, Val Loss: 1.1545, Val Acc: 67.9548\n",
      "Epoch 491/1000, Tr Loss: 0.0433, Tr Acc: 98.8701, Val Loss: 2.8038, Val Acc: 60.1320\n",
      "Epoch 501/1000, Tr Loss: 0.0397, Tr Acc: 98.8701, Val Loss: 0.4112, Val Acc: 84.2601\n",
      "Epoch 511/1000, Tr Loss: 0.0448, Tr Acc: 98.7288, Val Loss: 0.4875, Val Acc: 80.3959\n",
      "Epoch 521/1000, Tr Loss: 0.0419, Tr Acc: 98.9171, Val Loss: 0.2214, Val Acc: 90.6692\n",
      "Epoch 531/1000, Tr Loss: 0.0435, Tr Acc: 98.7288, Val Loss: 0.3391, Val Acc: 85.4854\n",
      "Epoch 541/1000, Tr Loss: 0.0406, Tr Acc: 98.8230, Val Loss: 0.7742, Val Acc: 76.0603\n",
      "Epoch 551/1000, Tr Loss: 0.0411, Tr Acc: 98.9171, Val Loss: 0.3391, Val Acc: 84.7314\n",
      "Epoch 561/1000, Tr Loss: 0.0319, Tr Acc: 99.2938, Val Loss: 1.0188, Val Acc: 75.7776\n",
      "Epoch 571/1000, Tr Loss: 0.0508, Tr Acc: 98.5876, Val Loss: 1.1467, Val Acc: 74.7408\n",
      "Epoch 581/1000, Tr Loss: 0.0335, Tr Acc: 99.0113, Val Loss: 0.6379, Val Acc: 78.4166\n",
      "Epoch 591/1000, Tr Loss: 0.0397, Tr Acc: 98.8701, Val Loss: 0.4782, Val Acc: 80.8671\n",
      "Epoch 601/1000, Tr Loss: 0.0368, Tr Acc: 99.1055, Val Loss: 1.2789, Val Acc: 70.1225\n",
      "Epoch 611/1000, Tr Loss: 0.0348, Tr Acc: 98.9642, Val Loss: 1.8708, Val Acc: 65.6927\n",
      "Epoch 621/1000, Tr Loss: 0.0382, Tr Acc: 98.9171, Val Loss: 3.1804, Val Acc: 60.1320\n",
      "Epoch 631/1000, Tr Loss: 0.0343, Tr Acc: 99.0113, Val Loss: 0.2630, Val Acc: 87.3704\n",
      "Epoch 641/1000, Tr Loss: 0.0392, Tr Acc: 98.9642, Val Loss: 0.2187, Val Acc: 89.8209\n",
      "Epoch 651/1000, Tr Loss: 0.0377, Tr Acc: 99.0113, Val Loss: 0.7851, Val Acc: 75.8718\n",
      "Epoch 661/1000, Tr Loss: 0.0342, Tr Acc: 99.1525, Val Loss: 1.3015, Val Acc: 69.6513\n",
      "Epoch 671/1000, Tr Loss: 0.0426, Tr Acc: 98.4934, Val Loss: 0.5643, Val Acc: 80.9614\n",
      "Epoch 681/1000, Tr Loss: 0.0358, Tr Acc: 99.0113, Val Loss: 0.3754, Val Acc: 83.3176\n",
      "Epoch 691/1000, Tr Loss: 0.0352, Tr Acc: 99.3879, Val Loss: 4.4315, Val Acc: 55.3252\n",
      "Epoch 701/1000, Tr Loss: 0.0406, Tr Acc: 99.0113, Val Loss: 0.4318, Val Acc: 82.8464\n",
      "Epoch 711/1000, Tr Loss: 0.0386, Tr Acc: 99.0584, Val Loss: 0.4633, Val Acc: 81.4326\n",
      "Epoch 721/1000, Tr Loss: 0.0342, Tr Acc: 99.1055, Val Loss: 0.3824, Val Acc: 83.9774\n",
      "Epoch 731/1000, Tr Loss: 0.0361, Tr Acc: 98.9642, Val Loss: 0.2848, Val Acc: 88.3129\n",
      "Epoch 741/1000, Tr Loss: 0.0388, Tr Acc: 98.6817, Val Loss: 0.3294, Val Acc: 84.8256\n",
      "Epoch 751/1000, Tr Loss: 0.0315, Tr Acc: 99.0584, Val Loss: 0.7661, Val Acc: 75.5891\n",
      "Epoch 761/1000, Tr Loss: 0.0342, Tr Acc: 98.9642, Val Loss: 0.8254, Val Acc: 75.3063\n",
      "Epoch 771/1000, Tr Loss: 0.0320, Tr Acc: 98.9642, Val Loss: 0.9106, Val Acc: 75.9661\n",
      "Epoch 781/1000, Tr Loss: 0.0331, Tr Acc: 99.0584, Val Loss: 0.4798, Val Acc: 81.3384\n",
      "Epoch 791/1000, Tr Loss: 0.0311, Tr Acc: 99.0584, Val Loss: 0.3893, Val Acc: 83.8831\n",
      "Epoch 801/1000, Tr Loss: 0.0355, Tr Acc: 99.0584, Val Loss: 0.4305, Val Acc: 82.6579\n",
      "Epoch 811/1000, Tr Loss: 0.0373, Tr Acc: 98.8230, Val Loss: 1.7322, Val Acc: 68.2375\n",
      "Epoch 821/1000, Tr Loss: 0.0247, Tr Acc: 99.3409, Val Loss: 1.5066, Val Acc: 67.0123\n",
      "Epoch 831/1000, Tr Loss: 0.0362, Tr Acc: 98.8230, Val Loss: 0.9474, Val Acc: 71.3478\n",
      "Epoch 841/1000, Tr Loss: 0.0247, Tr Acc: 99.4821, Val Loss: 2.3937, Val Acc: 65.5985\n",
      "Epoch 851/1000, Tr Loss: 0.0282, Tr Acc: 99.1525, Val Loss: 0.8527, Val Acc: 74.6466\n",
      "Epoch 861/1000, Tr Loss: 0.0292, Tr Acc: 99.2938, Val Loss: 0.8711, Val Acc: 73.1385\n",
      "Epoch 871/1000, Tr Loss: 0.0259, Tr Acc: 99.4350, Val Loss: 0.5215, Val Acc: 79.8303\n",
      "Epoch 881/1000, Tr Loss: 0.0386, Tr Acc: 98.9171, Val Loss: 1.4949, Val Acc: 69.1800\n",
      "Epoch 891/1000, Tr Loss: 0.0306, Tr Acc: 99.3409, Val Loss: 0.6540, Val Acc: 77.8511\n",
      "Epoch 901/1000, Tr Loss: 0.0277, Tr Acc: 99.3879, Val Loss: 3.8658, Val Acc: 58.5297\n",
      "Epoch 911/1000, Tr Loss: 0.0332, Tr Acc: 99.1055, Val Loss: 0.7353, Val Acc: 76.0603\n",
      "Epoch 921/1000, Tr Loss: 0.0283, Tr Acc: 99.3879, Val Loss: 0.5203, Val Acc: 82.7521\n",
      "Epoch 931/1000, Tr Loss: 0.0270, Tr Acc: 99.3409, Val Loss: 2.5360, Val Acc: 62.6767\n",
      "Epoch 941/1000, Tr Loss: 0.0307, Tr Acc: 99.0113, Val Loss: 0.7624, Val Acc: 73.7983\n",
      "Epoch 951/1000, Tr Loss: 0.0287, Tr Acc: 99.3879, Val Loss: 1.3989, Val Acc: 72.1018\n",
      "Epoch 961/1000, Tr Loss: 0.0295, Tr Acc: 99.1996, Val Loss: 0.5590, Val Acc: 79.7361\n",
      "Epoch 971/1000, Tr Loss: 0.0318, Tr Acc: 99.1525, Val Loss: 0.4426, Val Acc: 82.0924\n",
      "Epoch 981/1000, Tr Loss: 0.0340, Tr Acc: 99.1525, Val Loss: 0.3826, Val Acc: 84.8256\n",
      "Epoch 991/1000, Tr Loss: 0.0238, Tr Acc: 99.5292, Val Loss: 0.2772, Val Acc: 87.0877\n",
      "Fold 1\n",
      "Epoch 1/1000, Tr Loss: 0.1994, Tr Acc: 94.6773, Val Loss: 5.7663, Val Acc: 56.2147\n",
      "Epoch 11/1000, Tr Loss: 0.1235, Tr Acc: 95.3839, Val Loss: 2.1279, Val Acc: 65.8192\n",
      "Epoch 21/1000, Tr Loss: 0.0953, Tr Acc: 96.9383, Val Loss: 1.3915, Val Acc: 71.7514\n",
      "Epoch 31/1000, Tr Loss: 0.0907, Tr Acc: 96.7028, Val Loss: 0.3865, Val Acc: 84.2750\n",
      "Epoch 41/1000, Tr Loss: 0.0760, Tr Acc: 97.6448, Val Loss: 1.3427, Val Acc: 68.9266\n",
      "Epoch 51/1000, Tr Loss: 0.0748, Tr Acc: 97.0325, Val Loss: 0.1995, Val Acc: 90.5838\n",
      "Epoch 61/1000, Tr Loss: 0.0622, Tr Acc: 97.7390, Val Loss: 0.4815, Val Acc: 81.9209\n",
      "Epoch 71/1000, Tr Loss: 0.0665, Tr Acc: 97.7861, Val Loss: 0.3978, Val Acc: 83.8983\n",
      "Epoch 81/1000, Tr Loss: 0.0676, Tr Acc: 97.5506, Val Loss: 0.9745, Val Acc: 76.5537\n",
      "Epoch 91/1000, Tr Loss: 0.0647, Tr Acc: 97.8804, Val Loss: 1.2133, Val Acc: 72.8814\n",
      "Epoch 101/1000, Tr Loss: 0.0506, Tr Acc: 97.9746, Val Loss: 0.5453, Val Acc: 81.1676\n",
      "Epoch 111/1000, Tr Loss: 0.0575, Tr Acc: 98.0217, Val Loss: 1.1876, Val Acc: 71.3748\n",
      "Epoch 121/1000, Tr Loss: 0.0572, Tr Acc: 98.3043, Val Loss: 0.6328, Val Acc: 78.1544\n",
      "Epoch 131/1000, Tr Loss: 0.0558, Tr Acc: 98.2101, Val Loss: 1.2391, Val Acc: 70.9981\n",
      "Epoch 141/1000, Tr Loss: 0.0511, Tr Acc: 98.3043, Val Loss: 0.9190, Val Acc: 72.5989\n",
      "Epoch 151/1000, Tr Loss: 0.0550, Tr Acc: 98.2101, Val Loss: 0.5388, Val Acc: 79.7552\n",
      "Epoch 161/1000, Tr Loss: 0.0561, Tr Acc: 98.5869, Val Loss: 0.1161, Val Acc: 95.4802\n",
      "Epoch 171/1000, Tr Loss: 0.0454, Tr Acc: 98.7282, Val Loss: 0.2355, Val Acc: 90.0188\n",
      "Epoch 181/1000, Tr Loss: 0.0445, Tr Acc: 99.0579, Val Loss: 0.1716, Val Acc: 91.9962\n",
      "Epoch 191/1000, Tr Loss: 0.0507, Tr Acc: 98.4456, Val Loss: 2.0880, Val Acc: 68.1733\n",
      "Epoch 201/1000, Tr Loss: 0.0494, Tr Acc: 98.3043, Val Loss: 1.6717, Val Acc: 68.4557\n",
      "Epoch 211/1000, Tr Loss: 0.0495, Tr Acc: 98.3985, Val Loss: 1.9524, Val Acc: 66.1017\n",
      "Epoch 221/1000, Tr Loss: 0.0528, Tr Acc: 98.3514, Val Loss: 0.3806, Val Acc: 82.8625\n",
      "Epoch 231/1000, Tr Loss: 0.0381, Tr Acc: 99.0579, Val Loss: 0.2686, Val Acc: 88.1356\n",
      "Epoch 241/1000, Tr Loss: 0.0503, Tr Acc: 98.7753, Val Loss: 0.6091, Val Acc: 78.2486\n",
      "Epoch 251/1000, Tr Loss: 0.0476, Tr Acc: 98.5398, Val Loss: 0.7068, Val Acc: 77.8719\n",
      "Epoch 261/1000, Tr Loss: 0.0501, Tr Acc: 98.3985, Val Loss: 0.6023, Val Acc: 80.6968\n",
      "Epoch 271/1000, Tr Loss: 0.0438, Tr Acc: 98.4927, Val Loss: 1.2582, Val Acc: 70.9981\n",
      "Epoch 281/1000, Tr Loss: 0.0391, Tr Acc: 98.6340, Val Loss: 0.2373, Val Acc: 89.8305\n",
      "Epoch 291/1000, Tr Loss: 0.0414, Tr Acc: 98.6340, Val Loss: 2.5317, Val Acc: 62.0527\n",
      "Epoch 301/1000, Tr Loss: 0.0436, Tr Acc: 98.7753, Val Loss: 0.2110, Val Acc: 89.6422\n",
      "Epoch 311/1000, Tr Loss: 0.0426, Tr Acc: 98.6811, Val Loss: 0.2258, Val Acc: 89.9247\n",
      "Epoch 321/1000, Tr Loss: 0.0458, Tr Acc: 98.6811, Val Loss: 1.0999, Val Acc: 72.5047\n",
      "Epoch 331/1000, Tr Loss: 0.0386, Tr Acc: 98.9166, Val Loss: 0.3275, Val Acc: 87.3823\n",
      "Epoch 341/1000, Tr Loss: 0.0526, Tr Acc: 98.4927, Val Loss: 1.0909, Val Acc: 75.2354\n",
      "Epoch 351/1000, Tr Loss: 0.0512, Tr Acc: 98.5398, Val Loss: 0.3097, Val Acc: 88.1356\n",
      "Epoch 361/1000, Tr Loss: 0.0379, Tr Acc: 98.7753, Val Loss: 0.8708, Val Acc: 77.2128\n",
      "Epoch 371/1000, Tr Loss: 0.0357, Tr Acc: 99.0108, Val Loss: 5.7439, Val Acc: 56.4030\n",
      "Epoch 381/1000, Tr Loss: 0.0442, Tr Acc: 98.3043, Val Loss: 2.8519, Val Acc: 64.5009\n",
      "Epoch 391/1000, Tr Loss: 0.0404, Tr Acc: 98.9166, Val Loss: 0.2041, Val Acc: 89.8305\n",
      "Epoch 401/1000, Tr Loss: 0.0371, Tr Acc: 98.6811, Val Loss: 1.6626, Val Acc: 63.4652\n",
      "Epoch 411/1000, Tr Loss: 0.0479, Tr Acc: 98.5869, Val Loss: 0.2851, Val Acc: 89.0772\n",
      "Epoch 421/1000, Tr Loss: 0.0364, Tr Acc: 98.9637, Val Loss: 0.2186, Val Acc: 90.1130\n",
      "Epoch 431/1000, Tr Loss: 0.0311, Tr Acc: 99.0108, Val Loss: 0.2405, Val Acc: 89.0772\n",
      "Epoch 441/1000, Tr Loss: 0.0358, Tr Acc: 99.2463, Val Loss: 1.1652, Val Acc: 69.3032\n",
      "Epoch 451/1000, Tr Loss: 0.0354, Tr Acc: 99.3877, Val Loss: 1.8955, Val Acc: 66.7608\n",
      "Epoch 461/1000, Tr Loss: 0.0409, Tr Acc: 98.5869, Val Loss: 0.2727, Val Acc: 89.6422\n",
      "Epoch 471/1000, Tr Loss: 0.0397, Tr Acc: 98.5869, Val Loss: 0.8262, Val Acc: 75.5179\n",
      "Epoch 481/1000, Tr Loss: 0.0323, Tr Acc: 98.8224, Val Loss: 0.7308, Val Acc: 75.3296\n",
      "Epoch 491/1000, Tr Loss: 0.0357, Tr Acc: 99.0579, Val Loss: 1.3638, Val Acc: 67.1375\n",
      "Epoch 501/1000, Tr Loss: 0.0408, Tr Acc: 98.8224, Val Loss: 1.7081, Val Acc: 66.3842\n",
      "Epoch 511/1000, Tr Loss: 0.0356, Tr Acc: 98.8695, Val Loss: 0.2664, Val Acc: 88.7006\n",
      "Epoch 521/1000, Tr Loss: 0.0398, Tr Acc: 99.1521, Val Loss: 1.4616, Val Acc: 71.8456\n",
      "Epoch 531/1000, Tr Loss: 0.0296, Tr Acc: 99.1050, Val Loss: 1.5311, Val Acc: 69.3032\n",
      "Epoch 541/1000, Tr Loss: 0.0319, Tr Acc: 99.1521, Val Loss: 0.2343, Val Acc: 89.1714\n",
      "Epoch 551/1000, Tr Loss: 0.0275, Tr Acc: 99.3877, Val Loss: 0.7438, Val Acc: 77.8719\n",
      "Epoch 561/1000, Tr Loss: 0.0312, Tr Acc: 99.1050, Val Loss: 0.4538, Val Acc: 83.0508\n",
      "Epoch 571/1000, Tr Loss: 0.0278, Tr Acc: 99.2934, Val Loss: 0.2456, Val Acc: 89.0772\n",
      "Epoch 581/1000, Tr Loss: 0.0279, Tr Acc: 99.2934, Val Loss: 0.2883, Val Acc: 88.5122\n",
      "Epoch 591/1000, Tr Loss: 0.0372, Tr Acc: 99.1521, Val Loss: 0.5932, Val Acc: 80.3202\n",
      "Epoch 601/1000, Tr Loss: 0.0359, Tr Acc: 99.0108, Val Loss: 0.4468, Val Acc: 82.7684\n",
      "Epoch 611/1000, Tr Loss: 0.0356, Tr Acc: 99.1521, Val Loss: 0.3095, Val Acc: 87.7589\n",
      "Epoch 621/1000, Tr Loss: 0.0328, Tr Acc: 98.8224, Val Loss: 0.7758, Val Acc: 77.4011\n",
      "Epoch 631/1000, Tr Loss: 0.0302, Tr Acc: 99.1992, Val Loss: 1.5360, Val Acc: 71.1864\n",
      "Epoch 641/1000, Tr Loss: 0.0265, Tr Acc: 99.3406, Val Loss: 0.3663, Val Acc: 85.7815\n",
      "Epoch 651/1000, Tr Loss: 0.0270, Tr Acc: 99.4819, Val Loss: 0.8199, Val Acc: 77.4953\n",
      "Epoch 661/1000, Tr Loss: 0.0320, Tr Acc: 98.8224, Val Loss: 2.5260, Val Acc: 61.8644\n",
      "Epoch 671/1000, Tr Loss: 0.0324, Tr Acc: 98.9166, Val Loss: 1.1907, Val Acc: 71.6573\n",
      "Epoch 681/1000, Tr Loss: 0.0226, Tr Acc: 99.5290, Val Loss: 0.2954, Val Acc: 87.6648\n",
      "Epoch 691/1000, Tr Loss: 0.0285, Tr Acc: 99.4819, Val Loss: 1.5841, Val Acc: 69.3032\n",
      "Epoch 701/1000, Tr Loss: 0.0330, Tr Acc: 98.9637, Val Loss: 0.2932, Val Acc: 87.9473\n",
      "Epoch 711/1000, Tr Loss: 0.0287, Tr Acc: 99.2463, Val Loss: 1.7009, Val Acc: 67.1375\n",
      "Epoch 721/1000, Tr Loss: 0.0345, Tr Acc: 99.1050, Val Loss: 0.2265, Val Acc: 89.6422\n",
      "Epoch 731/1000, Tr Loss: 0.0289, Tr Acc: 99.1521, Val Loss: 1.7760, Val Acc: 67.3258\n",
      "Epoch 741/1000, Tr Loss: 0.0277, Tr Acc: 99.2934, Val Loss: 2.1621, Val Acc: 65.5367\n",
      "Epoch 751/1000, Tr Loss: 0.0261, Tr Acc: 99.2463, Val Loss: 0.8820, Val Acc: 73.2580\n",
      "Epoch 761/1000, Tr Loss: 0.0325, Tr Acc: 99.1521, Val Loss: 0.5178, Val Acc: 80.7910\n",
      "Epoch 771/1000, Tr Loss: 0.0287, Tr Acc: 99.1521, Val Loss: 2.1064, Val Acc: 66.1017\n",
      "Epoch 781/1000, Tr Loss: 0.0244, Tr Acc: 99.3406, Val Loss: 0.3109, Val Acc: 87.1940\n",
      "Epoch 791/1000, Tr Loss: 0.0326, Tr Acc: 98.8224, Val Loss: 6.2592, Val Acc: 53.0132\n",
      "Epoch 801/1000, Tr Loss: 0.0275, Tr Acc: 99.1050, Val Loss: 0.3461, Val Acc: 86.4407\n",
      "Epoch 811/1000, Tr Loss: 0.0322, Tr Acc: 99.1992, Val Loss: 1.9015, Val Acc: 68.3616\n",
      "Epoch 821/1000, Tr Loss: 0.0209, Tr Acc: 99.6703, Val Loss: 0.3257, Val Acc: 86.4407\n",
      "Epoch 831/1000, Tr Loss: 0.0267, Tr Acc: 99.4348, Val Loss: 0.3537, Val Acc: 85.2166\n",
      "Epoch 841/1000, Tr Loss: 0.0280, Tr Acc: 99.3877, Val Loss: 0.2488, Val Acc: 89.7363\n",
      "Epoch 851/1000, Tr Loss: 0.0269, Tr Acc: 99.2463, Val Loss: 0.3940, Val Acc: 84.3691\n",
      "Epoch 861/1000, Tr Loss: 0.0246, Tr Acc: 99.2463, Val Loss: 0.8754, Val Acc: 75.6121\n",
      "Epoch 871/1000, Tr Loss: 0.0237, Tr Acc: 99.4819, Val Loss: 2.0883, Val Acc: 68.7382\n",
      "Epoch 881/1000, Tr Loss: 0.0219, Tr Acc: 99.4348, Val Loss: 0.3565, Val Acc: 87.0056\n",
      "Epoch 891/1000, Tr Loss: 0.0301, Tr Acc: 99.1050, Val Loss: 0.9520, Val Acc: 73.4463\n",
      "Epoch 901/1000, Tr Loss: 0.0215, Tr Acc: 99.5761, Val Loss: 1.6656, Val Acc: 66.9492\n",
      "Epoch 911/1000, Tr Loss: 0.0218, Tr Acc: 99.2934, Val Loss: 1.3973, Val Acc: 71.0923\n",
      "Epoch 921/1000, Tr Loss: 0.0222, Tr Acc: 99.4819, Val Loss: 2.0352, Val Acc: 67.8908\n",
      "Epoch 931/1000, Tr Loss: 0.0264, Tr Acc: 99.5290, Val Loss: 0.4120, Val Acc: 85.7815\n",
      "Epoch 941/1000, Tr Loss: 0.0287, Tr Acc: 99.2934, Val Loss: 0.9480, Val Acc: 74.2938\n",
      "Epoch 951/1000, Tr Loss: 0.0203, Tr Acc: 99.3877, Val Loss: 2.3673, Val Acc: 63.9360\n",
      "Epoch 961/1000, Tr Loss: 0.0252, Tr Acc: 99.3877, Val Loss: 0.4789, Val Acc: 81.9209\n",
      "Epoch 971/1000, Tr Loss: 0.0230, Tr Acc: 99.5290, Val Loss: 0.3712, Val Acc: 85.6874\n",
      "Epoch 981/1000, Tr Loss: 0.0253, Tr Acc: 99.3406, Val Loss: 0.6722, Val Acc: 77.5895\n",
      "Epoch 991/1000, Tr Loss: 0.0231, Tr Acc: 99.4348, Val Loss: 1.9932, Val Acc: 67.7966\n",
      "Fold 2\n",
      "Epoch 1/1000, Tr Loss: 0.1313, Tr Acc: 96.0433, Val Loss: 30.2450, Val Acc: 47.8343\n",
      "Epoch 11/1000, Tr Loss: 0.1049, Tr Acc: 96.6557, Val Loss: 1.9796, Val Acc: 66.1959\n",
      "Epoch 21/1000, Tr Loss: 0.0918, Tr Acc: 96.7970, Val Loss: 2.4974, Val Acc: 64.7834\n",
      "Epoch 31/1000, Tr Loss: 0.0682, Tr Acc: 97.4564, Val Loss: 2.1573, Val Acc: 67.4200\n",
      "Epoch 41/1000, Tr Loss: 0.0694, Tr Acc: 97.8804, Val Loss: 0.9887, Val Acc: 76.7420\n",
      "Epoch 51/1000, Tr Loss: 0.0754, Tr Acc: 96.7028, Val Loss: 1.4004, Val Acc: 74.9529\n",
      "Epoch 61/1000, Tr Loss: 0.0602, Tr Acc: 97.8804, Val Loss: 0.1971, Val Acc: 91.0546\n",
      "Epoch 71/1000, Tr Loss: 0.0614, Tr Acc: 98.1630, Val Loss: 2.8500, Val Acc: 63.1827\n",
      "Epoch 81/1000, Tr Loss: 0.0518, Tr Acc: 98.1159, Val Loss: 0.5972, Val Acc: 79.9435\n",
      "Epoch 91/1000, Tr Loss: 0.0519, Tr Acc: 98.3514, Val Loss: 0.3296, Val Acc: 85.8757\n",
      "Epoch 101/1000, Tr Loss: 0.0550, Tr Acc: 98.2101, Val Loss: 0.1206, Val Acc: 94.8211\n",
      "Epoch 111/1000, Tr Loss: 0.0496, Tr Acc: 98.3985, Val Loss: 0.4352, Val Acc: 84.2750\n",
      "Epoch 121/1000, Tr Loss: 0.0553, Tr Acc: 98.2572, Val Loss: 0.1927, Val Acc: 91.4313\n",
      "Epoch 131/1000, Tr Loss: 0.0402, Tr Acc: 98.6811, Val Loss: 0.0913, Val Acc: 96.3277\n",
      "Epoch 141/1000, Tr Loss: 0.0460, Tr Acc: 98.7282, Val Loss: 1.2126, Val Acc: 71.1864\n",
      "Epoch 151/1000, Tr Loss: 0.0453, Tr Acc: 98.2572, Val Loss: 1.1135, Val Acc: 75.2354\n",
      "Epoch 161/1000, Tr Loss: 0.0444, Tr Acc: 98.7282, Val Loss: 0.1797, Val Acc: 91.5254\n",
      "Epoch 171/1000, Tr Loss: 0.0444, Tr Acc: 98.8695, Val Loss: 0.6701, Val Acc: 81.2618\n",
      "Epoch 181/1000, Tr Loss: 0.0390, Tr Acc: 99.0579, Val Loss: 0.2286, Val Acc: 89.8305\n",
      "Epoch 191/1000, Tr Loss: 0.0440, Tr Acc: 98.2101, Val Loss: 0.4838, Val Acc: 84.3691\n",
      "Epoch 201/1000, Tr Loss: 0.0370, Tr Acc: 98.6811, Val Loss: 0.5815, Val Acc: 81.9209\n",
      "Epoch 211/1000, Tr Loss: 0.0424, Tr Acc: 98.7282, Val Loss: 1.4611, Val Acc: 68.6441\n",
      "Epoch 221/1000, Tr Loss: 0.0396, Tr Acc: 98.6340, Val Loss: 0.1780, Val Acc: 91.3371\n",
      "Epoch 231/1000, Tr Loss: 0.0403, Tr Acc: 98.8695, Val Loss: 0.1460, Val Acc: 93.2203\n",
      "Epoch 241/1000, Tr Loss: 0.0340, Tr Acc: 98.8224, Val Loss: 1.2023, Val Acc: 69.3974\n",
      "Epoch 251/1000, Tr Loss: 0.0404, Tr Acc: 98.8224, Val Loss: 1.5505, Val Acc: 69.4915\n",
      "Epoch 261/1000, Tr Loss: 0.0401, Tr Acc: 98.6340, Val Loss: 0.7726, Val Acc: 80.6968\n",
      "Epoch 271/1000, Tr Loss: 0.0483, Tr Acc: 98.5869, Val Loss: 3.3412, Val Acc: 65.0659\n",
      "Epoch 281/1000, Tr Loss: 0.0419, Tr Acc: 98.8695, Val Loss: 0.1954, Val Acc: 90.9605\n",
      "Epoch 291/1000, Tr Loss: 0.0358, Tr Acc: 98.9166, Val Loss: 0.5587, Val Acc: 79.7552\n",
      "Epoch 301/1000, Tr Loss: 0.0369, Tr Acc: 98.8224, Val Loss: 1.1958, Val Acc: 76.1770\n",
      "Epoch 311/1000, Tr Loss: 0.0405, Tr Acc: 98.7753, Val Loss: 0.6498, Val Acc: 79.9435\n",
      "Epoch 321/1000, Tr Loss: 0.0321, Tr Acc: 99.1521, Val Loss: 0.1377, Val Acc: 93.7853\n",
      "Epoch 331/1000, Tr Loss: 0.0343, Tr Acc: 99.0108, Val Loss: 0.5022, Val Acc: 83.8983\n",
      "Epoch 341/1000, Tr Loss: 0.0326, Tr Acc: 99.0579, Val Loss: 0.2721, Val Acc: 87.0998\n",
      "Epoch 351/1000, Tr Loss: 0.0362, Tr Acc: 98.8695, Val Loss: 1.3664, Val Acc: 72.5047\n",
      "Epoch 361/1000, Tr Loss: 0.0341, Tr Acc: 99.2934, Val Loss: 0.4149, Val Acc: 85.2166\n",
      "Epoch 371/1000, Tr Loss: 0.0270, Tr Acc: 99.2463, Val Loss: 0.2986, Val Acc: 88.7947\n",
      "Epoch 381/1000, Tr Loss: 0.0323, Tr Acc: 99.0579, Val Loss: 0.9768, Val Acc: 76.3653\n",
      "Epoch 391/1000, Tr Loss: 0.0309, Tr Acc: 99.1992, Val Loss: 0.7919, Val Acc: 77.5895\n",
      "Epoch 401/1000, Tr Loss: 0.0259, Tr Acc: 99.1992, Val Loss: 0.2809, Val Acc: 89.0772\n",
      "Epoch 411/1000, Tr Loss: 0.0379, Tr Acc: 98.9637, Val Loss: 0.2025, Val Acc: 89.7363\n",
      "Epoch 421/1000, Tr Loss: 0.0296, Tr Acc: 99.3406, Val Loss: 0.7809, Val Acc: 79.0019\n",
      "Epoch 431/1000, Tr Loss: 0.0330, Tr Acc: 98.9166, Val Loss: 2.5074, Val Acc: 61.7702\n",
      "Epoch 441/1000, Tr Loss: 0.0276, Tr Acc: 99.3406, Val Loss: 0.9003, Val Acc: 75.1412\n",
      "Epoch 451/1000, Tr Loss: 0.0241, Tr Acc: 99.4819, Val Loss: 0.4648, Val Acc: 83.8983\n",
      "Epoch 461/1000, Tr Loss: 0.0291, Tr Acc: 99.3406, Val Loss: 2.0679, Val Acc: 64.3126\n",
      "Epoch 471/1000, Tr Loss: 0.0283, Tr Acc: 99.4348, Val Loss: 3.2047, Val Acc: 62.6177\n",
      "Epoch 481/1000, Tr Loss: 0.0368, Tr Acc: 98.8695, Val Loss: 0.5386, Val Acc: 81.7326\n",
      "Epoch 491/1000, Tr Loss: 0.0295, Tr Acc: 99.1050, Val Loss: 5.9516, Val Acc: 53.2957\n",
      "Epoch 501/1000, Tr Loss: 0.0301, Tr Acc: 99.0579, Val Loss: 4.5879, Val Acc: 61.3936\n",
      "Epoch 511/1000, Tr Loss: 0.0266, Tr Acc: 99.4348, Val Loss: 0.2254, Val Acc: 89.9247\n",
      "Epoch 521/1000, Tr Loss: 0.0222, Tr Acc: 99.6232, Val Loss: 0.2612, Val Acc: 89.3597\n",
      "Epoch 531/1000, Tr Loss: 0.0278, Tr Acc: 99.1992, Val Loss: 2.1185, Val Acc: 66.1017\n",
      "Epoch 541/1000, Tr Loss: 0.0292, Tr Acc: 99.3406, Val Loss: 0.5874, Val Acc: 81.9209\n",
      "Epoch 551/1000, Tr Loss: 0.0389, Tr Acc: 98.9166, Val Loss: 0.3850, Val Acc: 84.4633\n",
      "Epoch 561/1000, Tr Loss: 0.0235, Tr Acc: 99.5290, Val Loss: 0.6770, Val Acc: 76.9303\n",
      "Epoch 571/1000, Tr Loss: 0.0261, Tr Acc: 99.2934, Val Loss: 1.7762, Val Acc: 65.1601\n",
      "Epoch 581/1000, Tr Loss: 0.0280, Tr Acc: 99.1992, Val Loss: 0.4681, Val Acc: 80.7910\n",
      "Epoch 591/1000, Tr Loss: 0.0227, Tr Acc: 99.4348, Val Loss: 0.8893, Val Acc: 77.4953\n",
      "Epoch 601/1000, Tr Loss: 0.0224, Tr Acc: 99.4348, Val Loss: 0.2184, Val Acc: 90.4896\n",
      "Epoch 611/1000, Tr Loss: 0.0265, Tr Acc: 99.4348, Val Loss: 2.0353, Val Acc: 68.3616\n",
      "Epoch 621/1000, Tr Loss: 0.0243, Tr Acc: 99.3406, Val Loss: 0.3706, Val Acc: 85.2166\n",
      "Epoch 631/1000, Tr Loss: 0.0225, Tr Acc: 99.4819, Val Loss: 1.5675, Val Acc: 69.6798\n",
      "Epoch 641/1000, Tr Loss: 0.0201, Tr Acc: 99.5761, Val Loss: 0.4418, Val Acc: 83.2392\n",
      "Epoch 651/1000, Tr Loss: 0.0251, Tr Acc: 99.4348, Val Loss: 1.0884, Val Acc: 74.6704\n",
      "Epoch 661/1000, Tr Loss: 0.0287, Tr Acc: 99.1521, Val Loss: 2.2694, Val Acc: 67.7024\n",
      "Epoch 671/1000, Tr Loss: 0.0272, Tr Acc: 99.4819, Val Loss: 2.1255, Val Acc: 68.5499\n",
      "Epoch 681/1000, Tr Loss: 0.0253, Tr Acc: 99.3406, Val Loss: 1.1174, Val Acc: 75.5179\n",
      "Epoch 691/1000, Tr Loss: 0.0294, Tr Acc: 98.9637, Val Loss: 1.7551, Val Acc: 69.1149\n",
      "Epoch 701/1000, Tr Loss: 0.0199, Tr Acc: 99.6232, Val Loss: 0.3153, Val Acc: 86.1582\n",
      "Epoch 711/1000, Tr Loss: 0.0200, Tr Acc: 99.4819, Val Loss: 2.9902, Val Acc: 62.6177\n",
      "Epoch 721/1000, Tr Loss: 0.0199, Tr Acc: 99.6232, Val Loss: 1.7350, Val Acc: 63.0885\n",
      "Epoch 731/1000, Tr Loss: 0.0248, Tr Acc: 99.1992, Val Loss: 0.5225, Val Acc: 81.1676\n",
      "Epoch 741/1000, Tr Loss: 0.0244, Tr Acc: 99.3877, Val Loss: 2.4213, Val Acc: 64.7834\n",
      "Epoch 751/1000, Tr Loss: 0.0267, Tr Acc: 99.3877, Val Loss: 0.8384, Val Acc: 73.6347\n",
      "Epoch 761/1000, Tr Loss: 0.0264, Tr Acc: 99.1992, Val Loss: 5.2864, Val Acc: 55.6497\n",
      "Epoch 771/1000, Tr Loss: 0.0247, Tr Acc: 99.5290, Val Loss: 0.2758, Val Acc: 87.6648\n",
      "Epoch 781/1000, Tr Loss: 0.0279, Tr Acc: 99.1521, Val Loss: 0.4604, Val Acc: 83.1450\n",
      "Epoch 791/1000, Tr Loss: 0.0355, Tr Acc: 99.2463, Val Loss: 1.3852, Val Acc: 67.6083\n",
      "Epoch 801/1000, Tr Loss: 0.0305, Tr Acc: 99.1050, Val Loss: 0.1738, Val Acc: 92.4670\n",
      "Epoch 811/1000, Tr Loss: 0.0220, Tr Acc: 99.3406, Val Loss: 0.6293, Val Acc: 79.0019\n",
      "Epoch 821/1000, Tr Loss: 0.0241, Tr Acc: 99.6232, Val Loss: 0.7567, Val Acc: 79.0019\n",
      "Epoch 831/1000, Tr Loss: 0.0274, Tr Acc: 99.1050, Val Loss: 1.3184, Val Acc: 72.5047\n",
      "Epoch 841/1000, Tr Loss: 0.0224, Tr Acc: 99.2463, Val Loss: 0.5919, Val Acc: 80.7910\n",
      "Epoch 851/1000, Tr Loss: 0.0214, Tr Acc: 99.4348, Val Loss: 0.2643, Val Acc: 89.5480\n",
      "Epoch 861/1000, Tr Loss: 0.0194, Tr Acc: 99.6703, Val Loss: 2.4806, Val Acc: 65.6309\n",
      "Epoch 871/1000, Tr Loss: 0.0208, Tr Acc: 99.5290, Val Loss: 0.3927, Val Acc: 85.4991\n",
      "Epoch 881/1000, Tr Loss: 0.0209, Tr Acc: 99.3406, Val Loss: 2.2677, Val Acc: 62.8060\n",
      "Epoch 891/1000, Tr Loss: 0.0236, Tr Acc: 99.5761, Val Loss: 1.1543, Val Acc: 72.5989\n",
      "Epoch 901/1000, Tr Loss: 0.0172, Tr Acc: 99.6703, Val Loss: 1.4750, Val Acc: 69.6798\n",
      "Epoch 911/1000, Tr Loss: 0.0177, Tr Acc: 99.7174, Val Loss: 0.2714, Val Acc: 88.9831\n",
      "Epoch 921/1000, Tr Loss: 0.0227, Tr Acc: 99.5290, Val Loss: 2.2534, Val Acc: 66.3842\n",
      "Epoch 931/1000, Tr Loss: 0.0178, Tr Acc: 99.5290, Val Loss: 3.4106, Val Acc: 62.0527\n",
      "Epoch 941/1000, Tr Loss: 0.0232, Tr Acc: 99.2463, Val Loss: 2.4415, Val Acc: 66.4783\n",
      "Epoch 951/1000, Tr Loss: 0.0225, Tr Acc: 99.3877, Val Loss: 1.2883, Val Acc: 67.6083\n",
      "Epoch 961/1000, Tr Loss: 0.0219, Tr Acc: 99.4348, Val Loss: 0.3931, Val Acc: 84.7458\n",
      "Epoch 971/1000, Tr Loss: 0.0234, Tr Acc: 99.2463, Val Loss: 3.3069, Val Acc: 64.7834\n",
      "Epoch 981/1000, Tr Loss: 0.0168, Tr Acc: 99.8116, Val Loss: 1.2777, Val Acc: 70.9040\n",
      "Epoch 991/1000, Tr Loss: 0.0236, Tr Acc: 99.3406, Val Loss: 1.2815, Val Acc: 73.9171\n",
      "Fold 3\n",
      "Epoch 1/1000, Tr Loss: 0.1306, Tr Acc: 96.0452, Val Loss: 1.4554, Val Acc: 74.1753\n",
      "Epoch 11/1000, Tr Loss: 0.0978, Tr Acc: 96.4218, Val Loss: 1.1254, Val Acc: 72.7615\n",
      "Epoch 21/1000, Tr Loss: 0.0695, Tr Acc: 97.4576, Val Loss: 3.0945, Val Acc: 62.9595\n",
      "Epoch 31/1000, Tr Loss: 0.0567, Tr Acc: 97.8814, Val Loss: 0.8536, Val Acc: 79.1706\n",
      "Epoch 41/1000, Tr Loss: 0.0736, Tr Acc: 97.8814, Val Loss: 4.0931, Val Acc: 59.7549\n",
      "Epoch 51/1000, Tr Loss: 0.0616, Tr Acc: 97.9284, Val Loss: 0.5142, Val Acc: 79.3591\n",
      "Epoch 61/1000, Tr Loss: 0.0571, Tr Acc: 98.0226, Val Loss: 2.1320, Val Acc: 69.7455\n",
      "Epoch 71/1000, Tr Loss: 0.0611, Tr Acc: 97.7401, Val Loss: 0.2647, Val Acc: 89.4439\n",
      "Epoch 81/1000, Tr Loss: 0.0412, Tr Acc: 98.6347, Val Loss: 0.7248, Val Acc: 76.6258\n",
      "Epoch 91/1000, Tr Loss: 0.0391, Tr Acc: 98.9642, Val Loss: 1.6974, Val Acc: 65.2215\n",
      "Epoch 101/1000, Tr Loss: 0.0478, Tr Acc: 98.3992, Val Loss: 0.1668, Val Acc: 92.1772\n",
      "Epoch 111/1000, Tr Loss: 0.0548, Tr Acc: 98.3051, Val Loss: 0.2426, Val Acc: 89.4439\n",
      "Epoch 121/1000, Tr Loss: 0.0514, Tr Acc: 98.1168, Val Loss: 0.4322, Val Acc: 83.8831\n",
      "Epoch 131/1000, Tr Loss: 0.0465, Tr Acc: 98.3051, Val Loss: 0.6340, Val Acc: 78.7936\n",
      "Epoch 141/1000, Tr Loss: 0.0409, Tr Acc: 98.3992, Val Loss: 0.2049, Val Acc: 90.6692\n",
      "Epoch 151/1000, Tr Loss: 0.0303, Tr Acc: 99.1525, Val Loss: 0.1302, Val Acc: 94.4392\n",
      "Epoch 161/1000, Tr Loss: 0.0424, Tr Acc: 98.9171, Val Loss: 1.1855, Val Acc: 71.1593\n",
      "Epoch 171/1000, Tr Loss: 0.0497, Tr Acc: 98.6817, Val Loss: 3.3915, Val Acc: 63.0537\n",
      "Epoch 181/1000, Tr Loss: 0.0466, Tr Acc: 98.6817, Val Loss: 0.2847, Val Acc: 87.8417\n",
      "Epoch 191/1000, Tr Loss: 0.0358, Tr Acc: 98.8230, Val Loss: 0.2034, Val Acc: 90.8577\n",
      "Epoch 201/1000, Tr Loss: 0.0401, Tr Acc: 98.9171, Val Loss: 0.2241, Val Acc: 90.2922\n",
      "Epoch 211/1000, Tr Loss: 0.0478, Tr Acc: 98.5876, Val Loss: 2.5258, Val Acc: 65.4100\n",
      "Epoch 221/1000, Tr Loss: 0.0403, Tr Acc: 98.7288, Val Loss: 0.3463, Val Acc: 86.5221\n",
      "Epoch 231/1000, Tr Loss: 0.0399, Tr Acc: 98.6347, Val Loss: 0.1399, Val Acc: 92.9312\n",
      "Epoch 241/1000, Tr Loss: 0.0346, Tr Acc: 99.0584, Val Loss: 0.1891, Val Acc: 91.7059\n",
      "Epoch 251/1000, Tr Loss: 0.0355, Tr Acc: 98.8701, Val Loss: 4.1296, Val Acc: 56.3619\n",
      "Epoch 261/1000, Tr Loss: 0.0318, Tr Acc: 99.1525, Val Loss: 1.7163, Val Acc: 68.8030\n",
      "Epoch 271/1000, Tr Loss: 0.0364, Tr Acc: 98.9171, Val Loss: 0.2224, Val Acc: 89.7267\n",
      "Epoch 281/1000, Tr Loss: 0.0392, Tr Acc: 98.7759, Val Loss: 0.3306, Val Acc: 86.5221\n",
      "Epoch 291/1000, Tr Loss: 0.0412, Tr Acc: 98.7759, Val Loss: 0.6153, Val Acc: 77.2856\n",
      "Epoch 301/1000, Tr Loss: 0.0314, Tr Acc: 99.1055, Val Loss: 0.0973, Val Acc: 95.6645\n",
      "Epoch 311/1000, Tr Loss: 0.0377, Tr Acc: 98.7288, Val Loss: 3.3924, Val Acc: 60.8860\n",
      "Epoch 321/1000, Tr Loss: 0.0327, Tr Acc: 98.7288, Val Loss: 0.3114, Val Acc: 86.3336\n",
      "Epoch 331/1000, Tr Loss: 0.0303, Tr Acc: 99.1996, Val Loss: 3.4010, Val Acc: 56.4562\n",
      "Epoch 341/1000, Tr Loss: 0.0398, Tr Acc: 98.8701, Val Loss: 0.7600, Val Acc: 78.4166\n",
      "Epoch 351/1000, Tr Loss: 0.0305, Tr Acc: 99.1996, Val Loss: 0.7456, Val Acc: 78.5108\n",
      "Epoch 361/1000, Tr Loss: 0.0390, Tr Acc: 99.0113, Val Loss: 1.4345, Val Acc: 68.9915\n",
      "Epoch 371/1000, Tr Loss: 0.0336, Tr Acc: 99.0584, Val Loss: 1.0129, Val Acc: 71.4420\n",
      "Epoch 381/1000, Tr Loss: 0.0379, Tr Acc: 98.8230, Val Loss: 0.6656, Val Acc: 78.0396\n",
      "Epoch 391/1000, Tr Loss: 0.0305, Tr Acc: 99.4821, Val Loss: 0.2713, Val Acc: 87.1819\n",
      "Epoch 401/1000, Tr Loss: 0.0303, Tr Acc: 99.2938, Val Loss: 0.2461, Val Acc: 88.9727\n",
      "Epoch 411/1000, Tr Loss: 0.0365, Tr Acc: 99.1055, Val Loss: 0.3203, Val Acc: 85.6739\n",
      "Epoch 421/1000, Tr Loss: 0.0304, Tr Acc: 98.9171, Val Loss: 1.3353, Val Acc: 69.8398\n",
      "Epoch 431/1000, Tr Loss: 0.0323, Tr Acc: 98.9171, Val Loss: 1.7986, Val Acc: 66.3525\n",
      "Epoch 441/1000, Tr Loss: 0.0376, Tr Acc: 99.0584, Val Loss: 0.2045, Val Acc: 91.1404\n",
      "Epoch 451/1000, Tr Loss: 0.0363, Tr Acc: 98.9642, Val Loss: 0.3985, Val Acc: 84.2601\n",
      "Epoch 461/1000, Tr Loss: 0.0324, Tr Acc: 98.8230, Val Loss: 1.9484, Val Acc: 68.0490\n",
      "Epoch 471/1000, Tr Loss: 0.0304, Tr Acc: 99.1996, Val Loss: 2.2329, Val Acc: 69.2743\n",
      "Epoch 481/1000, Tr Loss: 0.0381, Tr Acc: 98.7288, Val Loss: 0.6779, Val Acc: 80.3016\n",
      "Epoch 491/1000, Tr Loss: 0.0322, Tr Acc: 99.2938, Val Loss: 0.4155, Val Acc: 83.6004\n",
      "Epoch 501/1000, Tr Loss: 0.0284, Tr Acc: 99.3879, Val Loss: 0.1322, Val Acc: 94.1565\n",
      "Epoch 511/1000, Tr Loss: 0.0337, Tr Acc: 98.9642, Val Loss: 0.4288, Val Acc: 84.6371\n",
      "Epoch 521/1000, Tr Loss: 0.0302, Tr Acc: 99.1055, Val Loss: 1.2993, Val Acc: 68.0490\n",
      "Epoch 531/1000, Tr Loss: 0.0238, Tr Acc: 99.4350, Val Loss: 0.2075, Val Acc: 90.4807\n",
      "Epoch 541/1000, Tr Loss: 0.0323, Tr Acc: 98.9642, Val Loss: 4.0220, Val Acc: 59.8492\n",
      "Epoch 551/1000, Tr Loss: 0.0275, Tr Acc: 99.3409, Val Loss: 0.5543, Val Acc: 80.9614\n",
      "Epoch 561/1000, Tr Loss: 0.0247, Tr Acc: 99.2938, Val Loss: 0.2177, Val Acc: 89.0669\n",
      "Epoch 571/1000, Tr Loss: 0.0293, Tr Acc: 99.0113, Val Loss: 2.0305, Val Acc: 66.0697\n",
      "Epoch 581/1000, Tr Loss: 0.0227, Tr Acc: 99.4821, Val Loss: 0.1790, Val Acc: 91.8944\n",
      "Epoch 591/1000, Tr Loss: 0.0316, Tr Acc: 98.8701, Val Loss: 0.1684, Val Acc: 92.1772\n",
      "Epoch 601/1000, Tr Loss: 0.0308, Tr Acc: 99.0113, Val Loss: 4.5631, Val Acc: 55.2309\n",
      "Epoch 611/1000, Tr Loss: 0.0311, Tr Acc: 99.1055, Val Loss: 1.7272, Val Acc: 66.5410\n",
      "Epoch 621/1000, Tr Loss: 0.0255, Tr Acc: 99.2938, Val Loss: 0.6725, Val Acc: 79.7361\n",
      "Epoch 631/1000, Tr Loss: 0.0263, Tr Acc: 99.2938, Val Loss: 1.6662, Val Acc: 64.7502\n",
      "Epoch 641/1000, Tr Loss: 0.0333, Tr Acc: 99.0584, Val Loss: 0.2109, Val Acc: 90.2922\n",
      "Epoch 651/1000, Tr Loss: 0.0326, Tr Acc: 99.0584, Val Loss: 3.8017, Val Acc: 62.1112\n",
      "Epoch 661/1000, Tr Loss: 0.0256, Tr Acc: 99.2467, Val Loss: 0.2107, Val Acc: 90.1037\n",
      "Epoch 671/1000, Tr Loss: 0.0249, Tr Acc: 99.1525, Val Loss: 0.3599, Val Acc: 85.9566\n",
      "Epoch 681/1000, Tr Loss: 0.0339, Tr Acc: 99.3409, Val Loss: 1.0515, Val Acc: 75.5891\n",
      "Epoch 691/1000, Tr Loss: 0.0273, Tr Acc: 99.3409, Val Loss: 0.1951, Val Acc: 90.7634\n",
      "Epoch 701/1000, Tr Loss: 0.0316, Tr Acc: 99.1055, Val Loss: 2.2385, Val Acc: 63.4307\n",
      "Epoch 711/1000, Tr Loss: 0.0292, Tr Acc: 99.2467, Val Loss: 2.7062, Val Acc: 62.0170\n",
      "Epoch 721/1000, Tr Loss: 0.0288, Tr Acc: 99.2938, Val Loss: 0.5453, Val Acc: 81.1499\n",
      "Epoch 731/1000, Tr Loss: 0.0201, Tr Acc: 99.5763, Val Loss: 0.3892, Val Acc: 84.6371\n",
      "Epoch 741/1000, Tr Loss: 0.0278, Tr Acc: 99.2938, Val Loss: 0.3061, Val Acc: 87.8417\n",
      "Epoch 751/1000, Tr Loss: 0.0328, Tr Acc: 99.1996, Val Loss: 2.3514, Val Acc: 64.3732\n",
      "Epoch 761/1000, Tr Loss: 0.0259, Tr Acc: 99.1996, Val Loss: 0.3179, Val Acc: 87.1819\n",
      "Epoch 771/1000, Tr Loss: 0.0238, Tr Acc: 99.4350, Val Loss: 0.7576, Val Acc: 76.4373\n",
      "Epoch 781/1000, Tr Loss: 0.0251, Tr Acc: 99.2938, Val Loss: 0.4287, Val Acc: 83.2234\n",
      "Epoch 791/1000, Tr Loss: 0.0227, Tr Acc: 99.4350, Val Loss: 2.6464, Val Acc: 63.9962\n",
      "Epoch 801/1000, Tr Loss: 0.0270, Tr Acc: 99.3879, Val Loss: 2.6816, Val Acc: 62.3940\n",
      "Epoch 811/1000, Tr Loss: 0.0277, Tr Acc: 99.1055, Val Loss: 0.4210, Val Acc: 82.6579\n",
      "Epoch 821/1000, Tr Loss: 0.0311, Tr Acc: 99.0113, Val Loss: 1.8851, Val Acc: 68.1433\n",
      "Epoch 831/1000, Tr Loss: 0.0242, Tr Acc: 99.2467, Val Loss: 0.4124, Val Acc: 84.6371\n",
      "Epoch 841/1000, Tr Loss: 0.0202, Tr Acc: 99.5763, Val Loss: 0.5806, Val Acc: 78.6051\n",
      "Epoch 851/1000, Tr Loss: 0.0233, Tr Acc: 99.2467, Val Loss: 1.8984, Val Acc: 68.8973\n",
      "Epoch 861/1000, Tr Loss: 0.0198, Tr Acc: 99.6234, Val Loss: 2.5956, Val Acc: 66.1640\n",
      "Epoch 871/1000, Tr Loss: 0.0226, Tr Acc: 99.4821, Val Loss: 2.0761, Val Acc: 63.7135\n",
      "Epoch 881/1000, Tr Loss: 0.0231, Tr Acc: 99.5292, Val Loss: 1.1864, Val Acc: 69.7455\n",
      "Epoch 891/1000, Tr Loss: 0.0176, Tr Acc: 99.7175, Val Loss: 0.2134, Val Acc: 89.5382\n",
      "Epoch 901/1000, Tr Loss: 0.0269, Tr Acc: 99.2938, Val Loss: 0.2677, Val Acc: 88.1244\n",
      "Epoch 911/1000, Tr Loss: 0.0261, Tr Acc: 99.2938, Val Loss: 2.8820, Val Acc: 60.5090\n",
      "Epoch 921/1000, Tr Loss: 0.0251, Tr Acc: 99.4350, Val Loss: 0.2612, Val Acc: 87.1819\n",
      "Epoch 931/1000, Tr Loss: 0.0239, Tr Acc: 99.3409, Val Loss: 0.3286, Val Acc: 86.0509\n",
      "Epoch 941/1000, Tr Loss: 0.0158, Tr Acc: 99.6704, Val Loss: 1.8090, Val Acc: 68.8973\n",
      "Epoch 951/1000, Tr Loss: 0.0206, Tr Acc: 99.7175, Val Loss: 1.3127, Val Acc: 72.4788\n",
      "Epoch 961/1000, Tr Loss: 0.0307, Tr Acc: 99.1996, Val Loss: 0.3824, Val Acc: 83.6946\n",
      "Epoch 971/1000, Tr Loss: 0.0221, Tr Acc: 99.5763, Val Loss: 0.6306, Val Acc: 80.9614\n",
      "Epoch 981/1000, Tr Loss: 0.0279, Tr Acc: 99.2467, Val Loss: 1.7749, Val Acc: 69.6513\n",
      "Epoch 991/1000, Tr Loss: 0.0220, Tr Acc: 99.6234, Val Loss: 7.2908, Val Acc: 54.8539\n",
      "Fold 1\n",
      "Epoch 1/1000, Tr Loss: 0.1136, Tr Acc: 95.9020, Val Loss: 7.3926, Val Acc: 55.4614\n",
      "Epoch 11/1000, Tr Loss: 0.0768, Tr Acc: 97.5977, Val Loss: 0.3839, Val Acc: 86.4407\n",
      "Epoch 21/1000, Tr Loss: 0.0474, Tr Acc: 98.4456, Val Loss: 0.2608, Val Acc: 88.0414\n",
      "Epoch 31/1000, Tr Loss: 0.0544, Tr Acc: 97.9746, Val Loss: 0.6122, Val Acc: 81.5443\n",
      "Epoch 41/1000, Tr Loss: 0.0476, Tr Acc: 98.5398, Val Loss: 1.1034, Val Acc: 72.9755\n",
      "Epoch 51/1000, Tr Loss: 0.0522, Tr Acc: 98.5869, Val Loss: 0.1543, Val Acc: 93.3145\n",
      "Epoch 61/1000, Tr Loss: 0.0440, Tr Acc: 98.6811, Val Loss: 0.2661, Val Acc: 86.6290\n",
      "Epoch 71/1000, Tr Loss: 0.0552, Tr Acc: 98.3043, Val Loss: 0.1839, Val Acc: 91.8079\n",
      "Epoch 81/1000, Tr Loss: 0.0551, Tr Acc: 97.9275, Val Loss: 0.9550, Val Acc: 77.4953\n",
      "Epoch 91/1000, Tr Loss: 0.0389, Tr Acc: 98.8224, Val Loss: 0.5880, Val Acc: 81.7326\n",
      "Epoch 101/1000, Tr Loss: 0.0362, Tr Acc: 98.9166, Val Loss: 0.8227, Val Acc: 76.6478\n",
      "Epoch 111/1000, Tr Loss: 0.0407, Tr Acc: 98.9637, Val Loss: 0.5083, Val Acc: 83.5217\n",
      "Epoch 121/1000, Tr Loss: 0.0374, Tr Acc: 98.8224, Val Loss: 0.1149, Val Acc: 94.6328\n",
      "Epoch 131/1000, Tr Loss: 0.0405, Tr Acc: 98.5869, Val Loss: 1.2765, Val Acc: 75.8945\n",
      "Epoch 141/1000, Tr Loss: 0.0391, Tr Acc: 98.6811, Val Loss: 0.6416, Val Acc: 80.9793\n",
      "Epoch 151/1000, Tr Loss: 0.0377, Tr Acc: 98.7753, Val Loss: 4.0971, Val Acc: 60.3578\n",
      "Epoch 161/1000, Tr Loss: 0.0386, Tr Acc: 98.4456, Val Loss: 0.6559, Val Acc: 80.2260\n",
      "Epoch 171/1000, Tr Loss: 0.0354, Tr Acc: 99.1992, Val Loss: 3.5380, Val Acc: 65.3484\n",
      "Epoch 181/1000, Tr Loss: 0.0342, Tr Acc: 99.1521, Val Loss: 1.2404, Val Acc: 74.1055\n",
      "Epoch 191/1000, Tr Loss: 0.0372, Tr Acc: 99.0108, Val Loss: 0.2302, Val Acc: 90.7721\n",
      "Epoch 201/1000, Tr Loss: 0.0327, Tr Acc: 98.6811, Val Loss: 1.7574, Val Acc: 69.4915\n",
      "Epoch 211/1000, Tr Loss: 0.0360, Tr Acc: 98.9637, Val Loss: 1.8260, Val Acc: 71.6573\n",
      "Epoch 221/1000, Tr Loss: 0.0313, Tr Acc: 99.1992, Val Loss: 1.9967, Val Acc: 68.8324\n",
      "Epoch 231/1000, Tr Loss: 0.0271, Tr Acc: 99.1992, Val Loss: 0.6863, Val Acc: 79.4727\n",
      "Epoch 241/1000, Tr Loss: 0.0309, Tr Acc: 99.1521, Val Loss: 0.3077, Val Acc: 87.0056\n",
      "Epoch 251/1000, Tr Loss: 0.0314, Tr Acc: 99.2463, Val Loss: 4.8696, Val Acc: 56.7797\n",
      "Epoch 261/1000, Tr Loss: 0.0362, Tr Acc: 98.9166, Val Loss: 0.2933, Val Acc: 88.3239\n",
      "Epoch 271/1000, Tr Loss: 0.0301, Tr Acc: 98.9637, Val Loss: 0.2203, Val Acc: 91.0546\n",
      "Epoch 281/1000, Tr Loss: 0.0347, Tr Acc: 99.0579, Val Loss: 0.2224, Val Acc: 90.3013\n",
      "Epoch 291/1000, Tr Loss: 0.0296, Tr Acc: 99.1050, Val Loss: 0.2508, Val Acc: 89.6422\n",
      "Epoch 301/1000, Tr Loss: 0.0357, Tr Acc: 98.8224, Val Loss: 0.4028, Val Acc: 85.3107\n",
      "Epoch 311/1000, Tr Loss: 0.0320, Tr Acc: 99.1521, Val Loss: 0.0849, Val Acc: 96.8927\n",
      "Epoch 321/1000, Tr Loss: 0.0269, Tr Acc: 99.2463, Val Loss: 1.2667, Val Acc: 70.6215\n",
      "Epoch 331/1000, Tr Loss: 0.0258, Tr Acc: 99.3877, Val Loss: 0.1790, Val Acc: 91.8079\n",
      "Epoch 341/1000, Tr Loss: 0.0290, Tr Acc: 99.3877, Val Loss: 0.2857, Val Acc: 87.7589\n",
      "Epoch 351/1000, Tr Loss: 0.0275, Tr Acc: 99.2934, Val Loss: 0.3151, Val Acc: 87.6648\n",
      "Epoch 361/1000, Tr Loss: 0.0305, Tr Acc: 99.1992, Val Loss: 0.3794, Val Acc: 84.8399\n",
      "Epoch 371/1000, Tr Loss: 0.0336, Tr Acc: 99.1521, Val Loss: 0.1511, Val Acc: 93.9736\n",
      "Epoch 381/1000, Tr Loss: 0.0261, Tr Acc: 99.4348, Val Loss: 0.8356, Val Acc: 75.0471\n",
      "Epoch 391/1000, Tr Loss: 0.0289, Tr Acc: 99.1992, Val Loss: 1.0709, Val Acc: 73.9171\n",
      "Epoch 401/1000, Tr Loss: 0.0265, Tr Acc: 99.3406, Val Loss: 0.5230, Val Acc: 83.8041\n",
      "Epoch 411/1000, Tr Loss: 0.0385, Tr Acc: 98.8695, Val Loss: 0.6821, Val Acc: 80.6026\n",
      "Epoch 421/1000, Tr Loss: 0.0281, Tr Acc: 99.2934, Val Loss: 2.4040, Val Acc: 65.9134\n",
      "Epoch 431/1000, Tr Loss: 0.0296, Tr Acc: 99.1992, Val Loss: 1.0046, Val Acc: 74.0113\n",
      "Epoch 441/1000, Tr Loss: 0.0243, Tr Acc: 99.3877, Val Loss: 4.8340, Val Acc: 54.7081\n",
      "Epoch 451/1000, Tr Loss: 0.0273, Tr Acc: 99.2934, Val Loss: 3.5358, Val Acc: 64.7834\n",
      "Epoch 461/1000, Tr Loss: 0.0264, Tr Acc: 99.4348, Val Loss: 1.3731, Val Acc: 73.0697\n",
      "Epoch 471/1000, Tr Loss: 0.0223, Tr Acc: 99.4348, Val Loss: 0.2010, Val Acc: 90.3013\n",
      "Epoch 481/1000, Tr Loss: 0.0234, Tr Acc: 99.3877, Val Loss: 0.9808, Val Acc: 78.0603\n",
      "Epoch 491/1000, Tr Loss: 0.0227, Tr Acc: 99.2934, Val Loss: 0.3681, Val Acc: 86.7232\n",
      "Epoch 501/1000, Tr Loss: 0.0271, Tr Acc: 99.1521, Val Loss: 1.0362, Val Acc: 75.6121\n",
      "Epoch 511/1000, Tr Loss: 0.0281, Tr Acc: 99.0579, Val Loss: 0.2673, Val Acc: 88.7947\n",
      "Epoch 521/1000, Tr Loss: 0.0276, Tr Acc: 99.3877, Val Loss: 0.3482, Val Acc: 86.0640\n",
      "Epoch 531/1000, Tr Loss: 0.0313, Tr Acc: 99.0579, Val Loss: 0.9096, Val Acc: 74.9529\n",
      "Epoch 541/1000, Tr Loss: 0.0256, Tr Acc: 99.1050, Val Loss: 1.5525, Val Acc: 70.1507\n",
      "Epoch 551/1000, Tr Loss: 0.0211, Tr Acc: 99.7174, Val Loss: 1.8508, Val Acc: 66.1017\n",
      "Epoch 561/1000, Tr Loss: 0.0220, Tr Acc: 99.5290, Val Loss: 1.9461, Val Acc: 64.8776\n",
      "Epoch 571/1000, Tr Loss: 0.0266, Tr Acc: 99.0579, Val Loss: 0.2121, Val Acc: 90.9605\n",
      "Epoch 581/1000, Tr Loss: 0.0254, Tr Acc: 99.2463, Val Loss: 5.2537, Val Acc: 56.2147\n",
      "Epoch 591/1000, Tr Loss: 0.0227, Tr Acc: 99.4348, Val Loss: 2.1739, Val Acc: 67.4200\n",
      "Epoch 601/1000, Tr Loss: 0.0268, Tr Acc: 99.2463, Val Loss: 0.4012, Val Acc: 85.8757\n",
      "Epoch 611/1000, Tr Loss: 0.0302, Tr Acc: 99.1521, Val Loss: 0.6367, Val Acc: 77.8719\n",
      "Epoch 621/1000, Tr Loss: 0.0239, Tr Acc: 99.3406, Val Loss: 0.5382, Val Acc: 81.2618\n",
      "Epoch 631/1000, Tr Loss: 0.0202, Tr Acc: 99.5290, Val Loss: 0.7980, Val Acc: 75.8945\n",
      "Epoch 641/1000, Tr Loss: 0.0175, Tr Acc: 99.6703, Val Loss: 0.2260, Val Acc: 91.3371\n",
      "Epoch 651/1000, Tr Loss: 0.0205, Tr Acc: 99.5290, Val Loss: 0.5705, Val Acc: 80.9793\n",
      "Epoch 661/1000, Tr Loss: 0.0255, Tr Acc: 99.1992, Val Loss: 0.5460, Val Acc: 81.9209\n",
      "Epoch 671/1000, Tr Loss: 0.0212, Tr Acc: 99.4819, Val Loss: 0.2072, Val Acc: 92.0904\n",
      "Epoch 681/1000, Tr Loss: 0.0212, Tr Acc: 99.3877, Val Loss: 0.4463, Val Acc: 85.9699\n",
      "Epoch 691/1000, Tr Loss: 0.0229, Tr Acc: 99.4348, Val Loss: 1.8538, Val Acc: 72.8814\n",
      "Epoch 701/1000, Tr Loss: 0.0182, Tr Acc: 99.6232, Val Loss: 0.2869, Val Acc: 89.0772\n",
      "Epoch 711/1000, Tr Loss: 0.0255, Tr Acc: 99.3877, Val Loss: 0.7004, Val Acc: 79.4727\n",
      "Epoch 721/1000, Tr Loss: 0.0242, Tr Acc: 99.2934, Val Loss: 1.4555, Val Acc: 72.3164\n",
      "Epoch 731/1000, Tr Loss: 0.0233, Tr Acc: 99.3406, Val Loss: 0.5626, Val Acc: 81.7326\n",
      "Epoch 741/1000, Tr Loss: 0.0251, Tr Acc: 99.4819, Val Loss: 0.2521, Val Acc: 90.5838\n",
      "Epoch 751/1000, Tr Loss: 0.0254, Tr Acc: 99.4348, Val Loss: 0.3160, Val Acc: 87.3823\n",
      "Epoch 761/1000, Tr Loss: 0.0246, Tr Acc: 99.2934, Val Loss: 3.1901, Val Acc: 62.9944\n",
      "Epoch 771/1000, Tr Loss: 0.0256, Tr Acc: 99.2934, Val Loss: 1.9923, Val Acc: 68.4557\n",
      "Epoch 781/1000, Tr Loss: 0.0189, Tr Acc: 99.5290, Val Loss: 0.1404, Val Acc: 94.0678\n",
      "Epoch 791/1000, Tr Loss: 0.0252, Tr Acc: 99.3877, Val Loss: 1.1072, Val Acc: 74.7646\n",
      "Epoch 801/1000, Tr Loss: 0.0204, Tr Acc: 99.4819, Val Loss: 0.7440, Val Acc: 79.0019\n",
      "Epoch 811/1000, Tr Loss: 0.0210, Tr Acc: 99.4348, Val Loss: 0.1793, Val Acc: 91.8079\n",
      "Epoch 821/1000, Tr Loss: 0.0210, Tr Acc: 99.3877, Val Loss: 0.1914, Val Acc: 91.7137\n",
      "Epoch 831/1000, Tr Loss: 0.0169, Tr Acc: 99.4819, Val Loss: 0.6002, Val Acc: 81.4501\n",
      "Epoch 841/1000, Tr Loss: 0.0217, Tr Acc: 99.4348, Val Loss: 0.8138, Val Acc: 79.1902\n",
      "Epoch 851/1000, Tr Loss: 0.0198, Tr Acc: 99.5761, Val Loss: 0.7166, Val Acc: 78.4369\n",
      "Epoch 861/1000, Tr Loss: 0.0157, Tr Acc: 99.6232, Val Loss: 0.3678, Val Acc: 85.8757\n",
      "Epoch 871/1000, Tr Loss: 0.0213, Tr Acc: 99.2463, Val Loss: 0.7767, Val Acc: 77.7778\n",
      "Epoch 881/1000, Tr Loss: 0.0193, Tr Acc: 99.5290, Val Loss: 0.3736, Val Acc: 86.4407\n",
      "Epoch 891/1000, Tr Loss: 0.0291, Tr Acc: 98.9637, Val Loss: 5.0931, Val Acc: 55.6497\n",
      "Epoch 901/1000, Tr Loss: 0.0221, Tr Acc: 99.3877, Val Loss: 0.2172, Val Acc: 90.3013\n",
      "Epoch 911/1000, Tr Loss: 0.0189, Tr Acc: 99.4819, Val Loss: 0.3672, Val Acc: 87.1940\n",
      "Epoch 921/1000, Tr Loss: 0.0178, Tr Acc: 99.6232, Val Loss: 0.4298, Val Acc: 83.1450\n",
      "Epoch 931/1000, Tr Loss: 0.0183, Tr Acc: 99.5290, Val Loss: 2.1249, Val Acc: 66.3842\n",
      "Epoch 941/1000, Tr Loss: 0.0169, Tr Acc: 99.6703, Val Loss: 0.4475, Val Acc: 86.0640\n",
      "Epoch 951/1000, Tr Loss: 0.0188, Tr Acc: 99.4819, Val Loss: 2.0179, Val Acc: 68.9266\n",
      "Epoch 961/1000, Tr Loss: 0.0174, Tr Acc: 99.5761, Val Loss: 3.0515, Val Acc: 67.3258\n",
      "Epoch 971/1000, Tr Loss: 0.0137, Tr Acc: 99.7645, Val Loss: 0.7804, Val Acc: 78.2486\n",
      "Epoch 981/1000, Tr Loss: 0.0172, Tr Acc: 99.5290, Val Loss: 5.7712, Val Acc: 59.4162\n",
      "Epoch 991/1000, Tr Loss: 0.0157, Tr Acc: 99.5761, Val Loss: 0.4029, Val Acc: 85.5932\n",
      "Fold 2\n",
      "Epoch 1/1000, Tr Loss: 0.1006, Tr Acc: 96.8441, Val Loss: 14.0966, Val Acc: 48.7759\n",
      "Epoch 11/1000, Tr Loss: 0.0674, Tr Acc: 97.3151, Val Loss: 5.8162, Val Acc: 56.9680\n",
      "Epoch 21/1000, Tr Loss: 0.0563, Tr Acc: 97.9275, Val Loss: 0.4683, Val Acc: 83.9925\n",
      "Epoch 31/1000, Tr Loss: 0.0454, Tr Acc: 98.6340, Val Loss: 0.3110, Val Acc: 87.0056\n",
      "Epoch 41/1000, Tr Loss: 0.0439, Tr Acc: 98.5869, Val Loss: 2.6973, Val Acc: 64.3126\n",
      "Epoch 51/1000, Tr Loss: 0.0430, Tr Acc: 99.0108, Val Loss: 0.8922, Val Acc: 74.4821\n",
      "Epoch 61/1000, Tr Loss: 0.0500, Tr Acc: 98.3985, Val Loss: 0.2856, Val Acc: 87.0998\n",
      "Epoch 71/1000, Tr Loss: 0.0425, Tr Acc: 98.4456, Val Loss: 0.7099, Val Acc: 77.0245\n",
      "Epoch 81/1000, Tr Loss: 0.0428, Tr Acc: 98.5398, Val Loss: 0.3992, Val Acc: 84.8399\n",
      "Epoch 91/1000, Tr Loss: 0.0343, Tr Acc: 98.9637, Val Loss: 0.3770, Val Acc: 87.4765\n",
      "Epoch 101/1000, Tr Loss: 0.0457, Tr Acc: 98.7753, Val Loss: 4.3136, Val Acc: 58.0979\n",
      "Epoch 111/1000, Tr Loss: 0.0435, Tr Acc: 98.4456, Val Loss: 3.0007, Val Acc: 64.5009\n",
      "Epoch 121/1000, Tr Loss: 0.0408, Tr Acc: 98.6811, Val Loss: 0.3649, Val Acc: 86.9115\n",
      "Epoch 131/1000, Tr Loss: 0.0359, Tr Acc: 98.9166, Val Loss: 4.2535, Val Acc: 61.3936\n",
      "Epoch 141/1000, Tr Loss: 0.0382, Tr Acc: 98.8695, Val Loss: 0.6827, Val Acc: 80.7910\n",
      "Epoch 151/1000, Tr Loss: 0.0315, Tr Acc: 99.1992, Val Loss: 0.3144, Val Acc: 86.9115\n",
      "Epoch 161/1000, Tr Loss: 0.0401, Tr Acc: 98.6340, Val Loss: 0.1840, Val Acc: 90.5838\n",
      "Epoch 171/1000, Tr Loss: 0.0345, Tr Acc: 98.7282, Val Loss: 0.8611, Val Acc: 77.7778\n",
      "Epoch 181/1000, Tr Loss: 0.0389, Tr Acc: 98.8224, Val Loss: 7.5935, Val Acc: 52.5424\n",
      "Epoch 191/1000, Tr Loss: 0.0275, Tr Acc: 99.2463, Val Loss: 0.3527, Val Acc: 84.7458\n",
      "Epoch 201/1000, Tr Loss: 0.0255, Tr Acc: 99.3877, Val Loss: 0.6199, Val Acc: 80.8851\n",
      "Epoch 211/1000, Tr Loss: 0.0360, Tr Acc: 98.9637, Val Loss: 0.1516, Val Acc: 92.7495\n",
      "Epoch 221/1000, Tr Loss: 0.0325, Tr Acc: 99.2463, Val Loss: 0.4322, Val Acc: 85.7815\n",
      "Epoch 231/1000, Tr Loss: 0.0380, Tr Acc: 98.9637, Val Loss: 0.3428, Val Acc: 88.6064\n",
      "Epoch 241/1000, Tr Loss: 0.0253, Tr Acc: 99.3406, Val Loss: 0.4911, Val Acc: 80.7910\n",
      "Epoch 251/1000, Tr Loss: 0.0443, Tr Acc: 98.7282, Val Loss: 0.8693, Val Acc: 78.0603\n",
      "Epoch 261/1000, Tr Loss: 0.0320, Tr Acc: 99.0108, Val Loss: 0.5351, Val Acc: 79.2844\n",
      "Epoch 271/1000, Tr Loss: 0.0358, Tr Acc: 98.9166, Val Loss: 1.5422, Val Acc: 71.5631\n",
      "Epoch 281/1000, Tr Loss: 0.0365, Tr Acc: 98.7753, Val Loss: 0.3006, Val Acc: 87.7589\n",
      "Epoch 291/1000, Tr Loss: 0.0388, Tr Acc: 99.0579, Val Loss: 1.0796, Val Acc: 73.8230\n",
      "Epoch 301/1000, Tr Loss: 0.0226, Tr Acc: 99.3406, Val Loss: 0.1087, Val Acc: 95.7627\n",
      "Epoch 311/1000, Tr Loss: 0.0286, Tr Acc: 99.2463, Val Loss: 1.1575, Val Acc: 77.2128\n",
      "Epoch 321/1000, Tr Loss: 0.0366, Tr Acc: 98.7753, Val Loss: 1.9569, Val Acc: 70.7156\n",
      "Epoch 331/1000, Tr Loss: 0.0225, Tr Acc: 99.6232, Val Loss: 0.0658, Val Acc: 97.8343\n",
      "Epoch 341/1000, Tr Loss: 0.0228, Tr Acc: 99.2934, Val Loss: 0.5623, Val Acc: 82.7684\n",
      "Epoch 351/1000, Tr Loss: 0.0229, Tr Acc: 99.3406, Val Loss: 0.1815, Val Acc: 91.6196\n",
      "Epoch 361/1000, Tr Loss: 0.0288, Tr Acc: 99.2463, Val Loss: 1.2271, Val Acc: 70.6215\n",
      "Epoch 371/1000, Tr Loss: 0.0282, Tr Acc: 99.1521, Val Loss: 3.0672, Val Acc: 60.3578\n",
      "Epoch 381/1000, Tr Loss: 0.0239, Tr Acc: 99.3877, Val Loss: 0.3593, Val Acc: 85.4991\n",
      "Epoch 391/1000, Tr Loss: 0.0234, Tr Acc: 99.3406, Val Loss: 1.1051, Val Acc: 75.6121\n",
      "Epoch 401/1000, Tr Loss: 0.0237, Tr Acc: 99.1521, Val Loss: 1.2031, Val Acc: 74.7646\n",
      "Epoch 411/1000, Tr Loss: 0.0267, Tr Acc: 99.2463, Val Loss: 2.9906, Val Acc: 63.9360\n",
      "Epoch 421/1000, Tr Loss: 0.0255, Tr Acc: 99.4348, Val Loss: 1.8416, Val Acc: 68.3616\n",
      "Epoch 431/1000, Tr Loss: 0.0259, Tr Acc: 99.4819, Val Loss: 0.3538, Val Acc: 87.6648\n",
      "Epoch 441/1000, Tr Loss: 0.0236, Tr Acc: 99.4348, Val Loss: 0.4766, Val Acc: 80.8851\n",
      "Epoch 451/1000, Tr Loss: 0.0190, Tr Acc: 99.6232, Val Loss: 0.9374, Val Acc: 75.8004\n",
      "Epoch 461/1000, Tr Loss: 0.0256, Tr Acc: 99.2463, Val Loss: 0.2302, Val Acc: 91.0546\n",
      "Epoch 471/1000, Tr Loss: 0.0241, Tr Acc: 99.2463, Val Loss: 0.2504, Val Acc: 89.1714\n",
      "Epoch 481/1000, Tr Loss: 0.0283, Tr Acc: 99.2463, Val Loss: 0.2705, Val Acc: 90.0188\n",
      "Epoch 491/1000, Tr Loss: 0.0185, Tr Acc: 99.4819, Val Loss: 3.0179, Val Acc: 62.9002\n",
      "Epoch 501/1000, Tr Loss: 0.0215, Tr Acc: 99.3877, Val Loss: 0.3681, Val Acc: 86.2524\n",
      "Epoch 511/1000, Tr Loss: 0.0220, Tr Acc: 99.3877, Val Loss: 0.5164, Val Acc: 81.2618\n",
      "Epoch 521/1000, Tr Loss: 0.0203, Tr Acc: 99.5761, Val Loss: 0.2074, Val Acc: 90.2072\n",
      "Epoch 531/1000, Tr Loss: 0.0211, Tr Acc: 99.3406, Val Loss: 3.0582, Val Acc: 60.3578\n",
      "Epoch 541/1000, Tr Loss: 0.0176, Tr Acc: 99.5761, Val Loss: 0.7700, Val Acc: 77.3070\n",
      "Epoch 551/1000, Tr Loss: 0.0195, Tr Acc: 99.4348, Val Loss: 0.3890, Val Acc: 85.9699\n",
      "Epoch 561/1000, Tr Loss: 0.0317, Tr Acc: 98.9166, Val Loss: 0.3555, Val Acc: 85.9699\n",
      "Epoch 571/1000, Tr Loss: 0.0241, Tr Acc: 99.3877, Val Loss: 0.5416, Val Acc: 82.5800\n",
      "Epoch 581/1000, Tr Loss: 0.0226, Tr Acc: 99.4819, Val Loss: 0.6982, Val Acc: 79.4727\n",
      "Epoch 591/1000, Tr Loss: 0.0218, Tr Acc: 99.4348, Val Loss: 0.1775, Val Acc: 92.3729\n",
      "Epoch 601/1000, Tr Loss: 0.0214, Tr Acc: 99.5761, Val Loss: 2.4144, Val Acc: 64.2185\n",
      "Epoch 611/1000, Tr Loss: 0.0225, Tr Acc: 99.2934, Val Loss: 3.4891, Val Acc: 61.0169\n",
      "Epoch 621/1000, Tr Loss: 0.0216, Tr Acc: 99.4348, Val Loss: 0.4889, Val Acc: 85.8757\n",
      "Epoch 631/1000, Tr Loss: 0.0229, Tr Acc: 99.3406, Val Loss: 7.1781, Val Acc: 51.1299\n",
      "Epoch 641/1000, Tr Loss: 0.0256, Tr Acc: 99.4348, Val Loss: 0.5412, Val Acc: 82.0151\n",
      "Epoch 651/1000, Tr Loss: 0.0176, Tr Acc: 99.5761, Val Loss: 1.6630, Val Acc: 68.7382\n",
      "Epoch 661/1000, Tr Loss: 0.0150, Tr Acc: 99.8116, Val Loss: 0.5717, Val Acc: 81.5443\n",
      "Epoch 671/1000, Tr Loss: 0.0219, Tr Acc: 99.4819, Val Loss: 0.4422, Val Acc: 83.8983\n",
      "Epoch 681/1000, Tr Loss: 0.0222, Tr Acc: 99.3406, Val Loss: 0.8271, Val Acc: 79.3785\n",
      "Epoch 691/1000, Tr Loss: 0.0202, Tr Acc: 99.2463, Val Loss: 0.1802, Val Acc: 91.9962\n",
      "Epoch 701/1000, Tr Loss: 0.0158, Tr Acc: 99.7174, Val Loss: 0.4267, Val Acc: 85.0282\n",
      "Epoch 711/1000, Tr Loss: 0.0179, Tr Acc: 99.4348, Val Loss: 0.1760, Val Acc: 92.6554\n",
      "Epoch 721/1000, Tr Loss: 0.0168, Tr Acc: 99.5761, Val Loss: 0.2385, Val Acc: 88.2298\n",
      "Epoch 731/1000, Tr Loss: 0.0162, Tr Acc: 99.6232, Val Loss: 0.5702, Val Acc: 80.4143\n",
      "Epoch 741/1000, Tr Loss: 0.0189, Tr Acc: 99.5761, Val Loss: 0.2146, Val Acc: 90.3013\n",
      "Epoch 751/1000, Tr Loss: 0.0292, Tr Acc: 99.1050, Val Loss: 5.3077, Val Acc: 53.7665\n",
      "Epoch 761/1000, Tr Loss: 0.0157, Tr Acc: 99.5290, Val Loss: 0.2221, Val Acc: 90.3013\n",
      "Epoch 771/1000, Tr Loss: 0.0231, Tr Acc: 99.4819, Val Loss: 0.4972, Val Acc: 83.3333\n",
      "Epoch 781/1000, Tr Loss: 0.0172, Tr Acc: 99.4348, Val Loss: 1.0641, Val Acc: 77.6836\n",
      "Epoch 791/1000, Tr Loss: 0.0218, Tr Acc: 99.4819, Val Loss: 7.1591, Val Acc: 53.3898\n",
      "Epoch 801/1000, Tr Loss: 0.0229, Tr Acc: 99.4819, Val Loss: 0.3889, Val Acc: 85.0282\n",
      "Epoch 811/1000, Tr Loss: 0.0261, Tr Acc: 99.1992, Val Loss: 6.6394, Val Acc: 57.9096\n",
      "Epoch 821/1000, Tr Loss: 0.0230, Tr Acc: 99.5290, Val Loss: 0.6037, Val Acc: 82.9567\n",
      "Epoch 831/1000, Tr Loss: 0.0215, Tr Acc: 99.2463, Val Loss: 0.2995, Val Acc: 86.2524\n",
      "Epoch 841/1000, Tr Loss: 0.0180, Tr Acc: 99.3406, Val Loss: 1.5307, Val Acc: 68.6441\n",
      "Epoch 851/1000, Tr Loss: 0.0186, Tr Acc: 99.5761, Val Loss: 0.3388, Val Acc: 87.7589\n",
      "Epoch 861/1000, Tr Loss: 0.0152, Tr Acc: 99.6232, Val Loss: 0.3379, Val Acc: 88.3239\n",
      "Epoch 871/1000, Tr Loss: 0.0236, Tr Acc: 99.3877, Val Loss: 3.4748, Val Acc: 62.8060\n",
      "Epoch 881/1000, Tr Loss: 0.0189, Tr Acc: 99.4819, Val Loss: 0.2290, Val Acc: 89.8305\n",
      "Epoch 891/1000, Tr Loss: 0.0210, Tr Acc: 99.5290, Val Loss: 0.7687, Val Acc: 78.0603\n",
      "Epoch 901/1000, Tr Loss: 0.0130, Tr Acc: 99.8116, Val Loss: 1.9811, Val Acc: 65.4426\n",
      "Epoch 911/1000, Tr Loss: 0.0139, Tr Acc: 99.6232, Val Loss: 1.8131, Val Acc: 70.2448\n",
      "Epoch 921/1000, Tr Loss: 0.0188, Tr Acc: 99.4819, Val Loss: 0.6806, Val Acc: 79.6610\n",
      "Epoch 941/1000, Tr Loss: 0.0237, Tr Acc: 99.3877, Val Loss: 2.2953, Val Acc: 65.2542\n",
      "Epoch 951/1000, Tr Loss: 0.0225, Tr Acc: 99.3406, Val Loss: 0.4087, Val Acc: 87.0998\n",
      "Epoch 961/1000, Tr Loss: 0.0167, Tr Acc: 99.6232, Val Loss: 1.1551, Val Acc: 73.1638\n",
      "Epoch 971/1000, Tr Loss: 0.0172, Tr Acc: 99.5290, Val Loss: 0.6685, Val Acc: 79.5669\n",
      "Epoch 981/1000, Tr Loss: 0.0165, Tr Acc: 99.6232, Val Loss: 0.7605, Val Acc: 79.3785\n",
      "Epoch 991/1000, Tr Loss: 0.0121, Tr Acc: 99.7645, Val Loss: 1.0136, Val Acc: 78.7194\n",
      "Fold 3\n",
      "Epoch 1/1000, Tr Loss: 0.1144, Tr Acc: 96.9397, Val Loss: 11.5932, Val Acc: 51.8379\n",
      "Epoch 11/1000, Tr Loss: 0.0667, Tr Acc: 97.8343, Val Loss: 3.5967, Val Acc: 63.5250\n",
      "Epoch 21/1000, Tr Loss: 0.0551, Tr Acc: 98.3992, Val Loss: 0.7046, Val Acc: 75.1178\n",
      "Epoch 31/1000, Tr Loss: 0.0456, Tr Acc: 98.5876, Val Loss: 0.4749, Val Acc: 82.7521\n",
      "Epoch 401/1000, Tr Loss: 0.0312, Tr Acc: 99.1521, Val Loss: 0.1539, Val Acc: 93.1262\n",
      "Epoch 411/1000, Tr Loss: 0.0186, Tr Acc: 99.5290, Val Loss: 0.2313, Val Acc: 91.9021\n",
      "Epoch 421/1000, Tr Loss: 0.0238, Tr Acc: 99.5290, Val Loss: 0.2819, Val Acc: 88.5122\n",
      "Epoch 431/1000, Tr Loss: 0.0199, Tr Acc: 99.5761, Val Loss: 0.2496, Val Acc: 90.0188\n",
      "Epoch 441/1000, Tr Loss: 0.0275, Tr Acc: 99.1521, Val Loss: 1.2413, Val Acc: 72.4105\n",
      "Epoch 451/1000, Tr Loss: 0.0211, Tr Acc: 99.4819, Val Loss: 0.1759, Val Acc: 92.1846\n",
      "Epoch 461/1000, Tr Loss: 0.0220, Tr Acc: 99.4348, Val Loss: 0.1980, Val Acc: 91.2429\n",
      "Epoch 471/1000, Tr Loss: 0.0187, Tr Acc: 99.4819, Val Loss: 3.5761, Val Acc: 62.7119\n",
      "Epoch 481/1000, Tr Loss: 0.0205, Tr Acc: 99.4348, Val Loss: 0.4186, Val Acc: 84.8399\n",
      "Epoch 491/1000, Tr Loss: 0.0272, Tr Acc: 99.3406, Val Loss: 1.1230, Val Acc: 75.5179\n",
      "Epoch 501/1000, Tr Loss: 0.0147, Tr Acc: 99.5761, Val Loss: 0.6219, Val Acc: 80.7910\n",
      "Epoch 511/1000, Tr Loss: 0.0224, Tr Acc: 99.2934, Val Loss: 0.8393, Val Acc: 74.2938\n",
      "Epoch 521/1000, Tr Loss: 0.0200, Tr Acc: 99.3877, Val Loss: 1.2001, Val Acc: 76.4595\n",
      "Epoch 531/1000, Tr Loss: 0.0224, Tr Acc: 99.4819, Val Loss: 1.2594, Val Acc: 72.2222\n",
      "Epoch 541/1000, Tr Loss: 0.0204, Tr Acc: 99.4348, Val Loss: 5.6497, Val Acc: 60.9228\n",
      "Epoch 551/1000, Tr Loss: 0.0186, Tr Acc: 99.5290, Val Loss: 1.4031, Val Acc: 72.5047\n",
      "Epoch 561/1000, Tr Loss: 0.0168, Tr Acc: 99.6232, Val Loss: 0.5375, Val Acc: 82.6742\n",
      "Epoch 571/1000, Tr Loss: 0.0172, Tr Acc: 99.5290, Val Loss: 1.7283, Val Acc: 70.4331\n",
      "Epoch 581/1000, Tr Loss: 0.0187, Tr Acc: 99.6232, Val Loss: 1.1340, Val Acc: 73.8230\n",
      "Epoch 591/1000, Tr Loss: 0.0214, Tr Acc: 99.4348, Val Loss: 1.5140, Val Acc: 70.5273\n",
      "Epoch 601/1000, Tr Loss: 0.0180, Tr Acc: 99.6232, Val Loss: 0.5379, Val Acc: 81.4501\n",
      "Epoch 611/1000, Tr Loss: 0.0183, Tr Acc: 99.3877, Val Loss: 2.1110, Val Acc: 68.5499\n",
      "Epoch 621/1000, Tr Loss: 0.0195, Tr Acc: 99.3406, Val Loss: 1.0500, Val Acc: 75.6121\n",
      "Epoch 631/1000, Tr Loss: 0.0222, Tr Acc: 99.2463, Val Loss: 0.4942, Val Acc: 83.8983\n",
      "Epoch 641/1000, Tr Loss: 0.0210, Tr Acc: 99.3877, Val Loss: 0.7472, Val Acc: 78.9077\n",
      "Epoch 651/1000, Tr Loss: 0.0178, Tr Acc: 99.6232, Val Loss: 0.5763, Val Acc: 82.2976\n",
      "Epoch 661/1000, Tr Loss: 0.0167, Tr Acc: 99.7174, Val Loss: 0.2483, Val Acc: 88.4181\n",
      "Epoch 671/1000, Tr Loss: 0.0191, Tr Acc: 99.5761, Val Loss: 0.1403, Val Acc: 93.9736\n",
      "Epoch 681/1000, Tr Loss: 0.0147, Tr Acc: 99.7645, Val Loss: 1.8335, Val Acc: 67.9849\n",
      "Epoch 691/1000, Tr Loss: 0.0222, Tr Acc: 99.5290, Val Loss: 3.6093, Val Acc: 61.9586\n",
      "Epoch 701/1000, Tr Loss: 0.0222, Tr Acc: 99.6232, Val Loss: 4.2186, Val Acc: 61.4878\n",
      "Epoch 711/1000, Tr Loss: 0.0202, Tr Acc: 99.5290, Val Loss: 0.8391, Val Acc: 76.0829\n",
      "Epoch 721/1000, Tr Loss: 0.0167, Tr Acc: 99.7174, Val Loss: 2.3203, Val Acc: 67.4200\n",
      "Epoch 731/1000, Tr Loss: 0.0156, Tr Acc: 99.6703, Val Loss: 0.3882, Val Acc: 86.4407\n",
      "Epoch 741/1000, Tr Loss: 0.0184, Tr Acc: 99.3406, Val Loss: 1.1909, Val Acc: 75.1412\n",
      "Epoch 751/1000, Tr Loss: 0.0135, Tr Acc: 99.6232, Val Loss: 0.3351, Val Acc: 86.1582\n",
      "Epoch 761/1000, Tr Loss: 0.0203, Tr Acc: 99.6703, Val Loss: 0.8613, Val Acc: 78.5311\n",
      "Epoch 771/1000, Tr Loss: 0.0196, Tr Acc: 99.4819, Val Loss: 0.4365, Val Acc: 85.4991\n",
      "Epoch 781/1000, Tr Loss: 0.0251, Tr Acc: 99.3877, Val Loss: 3.6015, Val Acc: 61.5819\n",
      "Epoch 791/1000, Tr Loss: 0.0177, Tr Acc: 99.6232, Val Loss: 1.3900, Val Acc: 68.0791\n",
      "Epoch 801/1000, Tr Loss: 0.0180, Tr Acc: 99.3406, Val Loss: 0.9966, Val Acc: 78.6252\n",
      "Epoch 811/1000, Tr Loss: 0.0184, Tr Acc: 99.4819, Val Loss: 0.7092, Val Acc: 80.8851\n",
      "Epoch 821/1000, Tr Loss: 0.0163, Tr Acc: 99.4819, Val Loss: 0.6728, Val Acc: 80.9793\n",
      "Epoch 831/1000, Tr Loss: 0.0157, Tr Acc: 99.6703, Val Loss: 0.8161, Val Acc: 79.6610\n",
      "Epoch 841/1000, Tr Loss: 0.0148, Tr Acc: 99.5761, Val Loss: 1.6786, Val Acc: 72.9755\n",
      "Epoch 851/1000, Tr Loss: 0.0125, Tr Acc: 99.6232, Val Loss: 1.3400, Val Acc: 70.0565\n",
      "Epoch 861/1000, Tr Loss: 0.0178, Tr Acc: 99.4819, Val Loss: 1.6539, Val Acc: 70.1507\n",
      "Epoch 871/1000, Tr Loss: 0.0133, Tr Acc: 99.8587, Val Loss: 0.9344, Val Acc: 77.7778\n",
      "Epoch 881/1000, Tr Loss: 0.0158, Tr Acc: 99.5290, Val Loss: 0.2607, Val Acc: 88.5122\n",
      "Epoch 891/1000, Tr Loss: 0.0190, Tr Acc: 99.6232, Val Loss: 2.2341, Val Acc: 69.3032\n",
      "Epoch 901/1000, Tr Loss: 0.0152, Tr Acc: 99.5290, Val Loss: 0.3104, Val Acc: 86.1582\n",
      "Epoch 911/1000, Tr Loss: 0.0125, Tr Acc: 99.7645, Val Loss: 0.2737, Val Acc: 89.3597\n",
      "Epoch 921/1000, Tr Loss: 0.0190, Tr Acc: 99.3406, Val Loss: 2.3054, Val Acc: 67.1375\n",
      "Epoch 931/1000, Tr Loss: 0.0140, Tr Acc: 99.5290, Val Loss: 0.3024, Val Acc: 89.3597\n",
      "Epoch 941/1000, Tr Loss: 0.0177, Tr Acc: 99.4348, Val Loss: 0.3141, Val Acc: 88.2298\n",
      "Epoch 951/1000, Tr Loss: 0.0183, Tr Acc: 99.5761, Val Loss: 2.4662, Val Acc: 67.3258\n",
      "Epoch 961/1000, Tr Loss: 0.0138, Tr Acc: 99.6703, Val Loss: 1.6798, Val Acc: 73.6347\n",
      "Epoch 971/1000, Tr Loss: 0.0174, Tr Acc: 99.4819, Val Loss: 1.9531, Val Acc: 65.5367\n",
      "Epoch 981/1000, Tr Loss: 0.0148, Tr Acc: 99.7174, Val Loss: 3.1386, Val Acc: 61.7702\n",
      "Epoch 991/1000, Tr Loss: 0.0172, Tr Acc: 99.5290, Val Loss: 1.3739, Val Acc: 74.9529\n",
      "Fold 2\n",
      "Epoch 1/1000, Tr Loss: 0.0869, Tr Acc: 97.8804, Val Loss: 20.3936, Val Acc: 47.9284\n",
      "Epoch 11/1000, Tr Loss: 0.0451, Tr Acc: 98.2101, Val Loss: 5.7705, Val Acc: 57.8154\n",
      "Epoch 21/1000, Tr Loss: 0.0428, Tr Acc: 98.4927, Val Loss: 1.8235, Val Acc: 65.9134\n",
      "Epoch 31/1000, Tr Loss: 0.0484, Tr Acc: 98.4927, Val Loss: 0.2539, Val Acc: 89.1714\n",
      "Epoch 41/1000, Tr Loss: 0.0358, Tr Acc: 99.0579, Val Loss: 0.1976, Val Acc: 92.6554\n",
      "Epoch 51/1000, Tr Loss: 0.0350, Tr Acc: 98.7753, Val Loss: 1.0705, Val Acc: 76.8362\n",
      "Epoch 61/1000, Tr Loss: 0.0382, Tr Acc: 99.0579, Val Loss: 0.2446, Val Acc: 90.0188\n",
      "Epoch 71/1000, Tr Loss: 0.0299, Tr Acc: 99.1521, Val Loss: 1.7854, Val Acc: 69.7740\n",
      "Epoch 81/1000, Tr Loss: 0.0280, Tr Acc: 99.2463, Val Loss: 0.1666, Val Acc: 92.2787\n",
      "Epoch 91/1000, Tr Loss: 0.0335, Tr Acc: 98.8224, Val Loss: 0.1914, Val Acc: 92.1846\n",
      "Epoch 101/1000, Tr Loss: 0.0278, Tr Acc: 99.0108, Val Loss: 1.7135, Val Acc: 74.1055\n",
      "Epoch 111/1000, Tr Loss: 0.0258, Tr Acc: 99.2463, Val Loss: 0.1523, Val Acc: 93.0320\n",
      "Epoch 121/1000, Tr Loss: 0.0279, Tr Acc: 99.2463, Val Loss: 0.5338, Val Acc: 81.6384\n",
      "Epoch 131/1000, Tr Loss: 0.0279, Tr Acc: 98.8695, Val Loss: 0.2293, Val Acc: 89.2655\n",
      "Epoch 141/1000, Tr Loss: 0.0212, Tr Acc: 99.4348, Val Loss: 0.2125, Val Acc: 90.3013\n",
      "Epoch 151/1000, Tr Loss: 0.0311, Tr Acc: 99.0579, Val Loss: 0.6890, Val Acc: 80.1318\n",
      "Epoch 161/1000, Tr Loss: 0.0286, Tr Acc: 99.1050, Val Loss: 0.8156, Val Acc: 79.0019\n",
      "Epoch 171/1000, Tr Loss: 0.0220, Tr Acc: 99.4819, Val Loss: 0.2313, Val Acc: 90.3955\n",
      "Epoch 181/1000, Tr Loss: 0.0262, Tr Acc: 99.1050, Val Loss: 0.8741, Val Acc: 79.1902\n",
      "Epoch 41/1000, Tr Loss: 0.0395, Tr Acc: 99.0113, Val Loss: 0.2722, Val Acc: 89.0669\n",
      "Epoch 51/1000, Tr Loss: 0.0422, Tr Acc: 98.7288, Val Loss: 0.1496, Val Acc: 92.2714\n",
      "Epoch 61/1000, Tr Loss: 0.0390, Tr Acc: 98.7759, Val Loss: 1.0757, Val Acc: 74.6466\n",
      "Epoch 71/1000, Tr Loss: 0.0280, Tr Acc: 99.1996, Val Loss: 0.1661, Val Acc: 92.3657\n",
      "Epoch 81/1000, Tr Loss: 0.0306, Tr Acc: 99.2467, Val Loss: 1.5281, Val Acc: 72.5730\n",
      "Epoch 91/1000, Tr Loss: 0.0347, Tr Acc: 98.9642, Val Loss: 1.6572, Val Acc: 73.0443\n",
      "Epoch 101/1000, Tr Loss: 0.0276, Tr Acc: 98.9642, Val Loss: 0.3538, Val Acc: 85.7681\n",
      "Epoch 111/1000, Tr Loss: 0.0220, Tr Acc: 99.3409, Val Loss: 0.2455, Val Acc: 90.7634\n",
      "Epoch 121/1000, Tr Loss: 0.0245, Tr Acc: 99.2467, Val Loss: 0.1405, Val Acc: 93.0254\n",
      "Epoch 131/1000, Tr Loss: 0.0343, Tr Acc: 99.1525, Val Loss: 0.2227, Val Acc: 90.3864\n",
      "Epoch 141/1000, Tr Loss: 0.0212, Tr Acc: 99.3879, Val Loss: 1.2669, Val Acc: 74.7408\n",
      "Epoch 781/1000, Tr Loss: 0.0112, Tr Acc: 99.8117, Val Loss: 2.0993, Val Acc: 71.3478\n",
      "Epoch 791/1000, Tr Loss: 0.0131, Tr Acc: 99.7175, Val Loss: 1.9027, Val Acc: 66.5410\n",
      "Epoch 801/1000, Tr Loss: 0.0105, Tr Acc: 99.8117, Val Loss: 0.1477, Val Acc: 92.5542\n",
      "Epoch 811/1000, Tr Loss: 0.0213, Tr Acc: 99.4350, Val Loss: 0.9480, Val Acc: 76.1546\n",
      "Epoch 821/1000, Tr Loss: 0.0132, Tr Acc: 99.6704, Val Loss: 3.6154, Val Acc: 62.3940\n",
      "Epoch 831/1000, Tr Loss: 0.0123, Tr Acc: 99.6234, Val Loss: 0.4294, Val Acc: 84.5429\n",
      "Epoch 841/1000, Tr Loss: 0.0149, Tr Acc: 99.7646, Val Loss: 4.8492, Val Acc: 60.8860\n",
      "Epoch 851/1000, Tr Loss: 0.0089, Tr Acc: 99.9529, Val Loss: 0.4867, Val Acc: 84.6371\n",
      "Epoch 861/1000, Tr Loss: 0.0152, Tr Acc: 99.8117, Val Loss: 2.4735, Val Acc: 65.8812\n",
      "Epoch 871/1000, Tr Loss: 0.0132, Tr Acc: 99.6704, Val Loss: 0.7945, Val Acc: 79.0763\n",
      "Epoch 881/1000, Tr Loss: 0.0162, Tr Acc: 99.6704, Val Loss: 6.5116, Val Acc: 56.1734\n",
      "Epoch 761/1000, Tr Loss: 0.0218, Tr Acc: 99.3879, Val Loss: 0.2384, Val Acc: 89.9152\n",
      "Epoch 771/1000, Tr Loss: 0.0112, Tr Acc: 99.5292, Val Loss: 0.2809, Val Acc: 88.6899\n",
      "Epoch 781/1000, Tr Loss: 0.0071, Tr Acc: 99.8588, Val Loss: 0.1942, Val Acc: 90.9519\n",
      "Epoch 791/1000, Tr Loss: 0.0175, Tr Acc: 99.4821, Val Loss: 1.1803, Val Acc: 78.2281\n",
      "Epoch 801/1000, Tr Loss: 0.0099, Tr Acc: 99.8117, Val Loss: 0.5707, Val Acc: 84.8256\n",
      "Epoch 811/1000, Tr Loss: 0.0115, Tr Acc: 99.7175, Val Loss: 0.7794, Val Acc: 78.6993\n",
      "Epoch 821/1000, Tr Loss: 0.0106, Tr Acc: 99.6704, Val Loss: 1.0662, Val Acc: 76.6258\n",
      "Epoch 831/1000, Tr Loss: 0.0141, Tr Acc: 99.7175, Val Loss: 0.4559, Val Acc: 84.5429\n",
      "Epoch 841/1000, Tr Loss: 0.0135, Tr Acc: 99.6234, Val Loss: 0.2916, Val Acc: 87.6532\n",
      "Epoch 851/1000, Tr Loss: 0.0135, Tr Acc: 99.6234, Val Loss: 1.8220, Val Acc: 70.8765\n",
      "Epoch 861/1000, Tr Loss: 0.0204, Tr Acc: 99.5763, Val Loss: 0.9013, Val Acc: 77.0028\n",
      "Epoch 871/1000, Tr Loss: 0.0119, Tr Acc: 99.7175, Val Loss: 0.7153, Val Acc: 79.3591\n",
      "Epoch 881/1000, Tr Loss: 0.0128, Tr Acc: 99.6704, Val Loss: 0.4245, Val Acc: 85.2969\n",
      "Epoch 891/1000, Tr Loss: 0.0136, Tr Acc: 99.5763, Val Loss: 0.8418, Val Acc: 81.9981\n",
      "Epoch 901/1000, Tr Loss: 0.0116, Tr Acc: 99.7646, Val Loss: 0.1580, Val Acc: 93.3082\n",
      "Epoch 911/1000, Tr Loss: 0.0137, Tr Acc: 99.6704, Val Loss: 0.3663, Val Acc: 86.8992\n",
      "Epoch 921/1000, Tr Loss: 0.0198, Tr Acc: 99.4821, Val Loss: 1.9479, Val Acc: 71.0650\n",
      "Epoch 931/1000, Tr Loss: 0.0096, Tr Acc: 99.7175, Val Loss: 0.1253, Val Acc: 94.7220\n",
      "Epoch 941/1000, Tr Loss: 0.0116, Tr Acc: 99.7646, Val Loss: 0.5778, Val Acc: 81.4326\n",
      "Epoch 951/1000, Tr Loss: 0.0108, Tr Acc: 99.8117, Val Loss: 2.1377, Val Acc: 68.5203\n",
      "Epoch 961/1000, Tr Loss: 0.0099, Tr Acc: 99.6234, Val Loss: 0.1660, Val Acc: 93.4025\n",
      "Epoch 971/1000, Tr Loss: 0.0132, Tr Acc: 99.6704, Val Loss: 1.3944, Val Acc: 76.1546\n",
      "Epoch 981/1000, Tr Loss: 0.0109, Tr Acc: 99.6704, Val Loss: 1.6173, Val Acc: 72.0075\n",
      "Epoch 991/1000, Tr Loss: 0.0080, Tr Acc: 99.8588, Val Loss: 0.3402, Val Acc: 87.2762\n",
      "Fold 1\n",
      "Epoch 1/1000, Tr Loss: 0.0406, Tr Acc: 98.8224, Val Loss: 1.1653, Val Acc: 77.4953\n",
      "Epoch 11/1000, Tr Loss: 0.0245, Tr Acc: 99.2934, Val Loss: 6.8404, Val Acc: 57.2505\n",
      "Epoch 21/1000, Tr Loss: 0.0327, Tr Acc: 98.9166, Val Loss: 3.4266, Val Acc: 63.9360\n",
      "Epoch 31/1000, Tr Loss: 0.0334, Tr Acc: 99.0579, Val Loss: 1.6614, Val Acc: 74.7646\n",
      "Epoch 41/1000, Tr Loss: 0.0220, Tr Acc: 99.1992, Val Loss: 0.5308, Val Acc: 83.8041\n",
      "Epoch 51/1000, Tr Loss: 0.0168, Tr Acc: 99.5290, Val Loss: 1.3005, Val Acc: 75.6121\n",
      "Epoch 61/1000, Tr Loss: 0.0158, Tr Acc: 99.5290, Val Loss: 0.2455, Val Acc: 87.2881\n",
      "Epoch 71/1000, Tr Loss: 0.0157, Tr Acc: 99.5761, Val Loss: 0.2524, Val Acc: 89.7363\n",
      "Epoch 81/1000, Tr Loss: 0.0159, Tr Acc: 99.4819, Val Loss: 0.2859, Val Acc: 88.3239\n",
      "Epoch 91/1000, Tr Loss: 0.0176, Tr Acc: 99.4819, Val Loss: 0.1466, Val Acc: 93.8795\n",
      "Epoch 101/1000, Tr Loss: 0.0234, Tr Acc: 99.3406, Val Loss: 2.3911, Val Acc: 66.3842\n",
      "Epoch 111/1000, Tr Loss: 0.0172, Tr Acc: 99.5761, Val Loss: 0.4981, Val Acc: 82.2034\n",
      "Epoch 121/1000, Tr Loss: 0.0207, Tr Acc: 99.3877, Val Loss: 1.2284, Val Acc: 78.5311\n",
      "Epoch 131/1000, Tr Loss: 0.0181, Tr Acc: 99.3877, Val Loss: 0.1668, Val Acc: 90.9605\n",
      "Epoch 141/1000, Tr Loss: 0.0170, Tr Acc: 99.4348, Val Loss: 0.6252, Val Acc: 82.2976\n",
      "Epoch 151/1000, Tr Loss: 0.0200, Tr Acc: 99.2934, Val Loss: 0.2850, Val Acc: 87.5706\n",
      "Epoch 161/1000, Tr Loss: 0.0209, Tr Acc: 99.5290, Val Loss: 0.4223, Val Acc: 84.2750\n",
      "Epoch 171/1000, Tr Loss: 0.0225, Tr Acc: 99.5290, Val Loss: 0.0545, Val Acc: 97.9284\n",
      "Epoch 181/1000, Tr Loss: 0.0173, Tr Acc: 99.5290, Val Loss: 0.3868, Val Acc: 86.6290\n",
      "Epoch 191/1000, Tr Loss: 0.0182, Tr Acc: 99.5761, Val Loss: 0.2225, Val Acc: 91.2429\n",
      "Epoch 201/1000, Tr Loss: 0.0162, Tr Acc: 99.3406, Val Loss: 0.4404, Val Acc: 85.6874\n",
      "Epoch 211/1000, Tr Loss: 0.0134, Tr Acc: 99.5761, Val Loss: 0.8203, Val Acc: 79.4727\n",
      "Epoch 221/1000, Tr Loss: 0.0236, Tr Acc: 99.3406, Val Loss: 3.3163, Val Acc: 65.4426\n",
      "Epoch 231/1000, Tr Loss: 0.0226, Tr Acc: 99.2934, Val Loss: 1.4850, Val Acc: 75.7062\n",
      "Epoch 241/1000, Tr Loss: 0.0215, Tr Acc: 99.4819, Val Loss: 0.3854, Val Acc: 85.9699\n",
      "Epoch 251/1000, Tr Loss: 0.0176, Tr Acc: 99.4819, Val Loss: 0.1154, Val Acc: 94.8211\n",
      "Epoch 261/1000, Tr Loss: 0.0147, Tr Acc: 99.5761, Val Loss: 1.3055, Val Acc: 72.0339\n",
      "Epoch 271/1000, Tr Loss: 0.0197, Tr Acc: 99.4348, Val Loss: 1.4749, Val Acc: 74.3879\n",
      "Epoch 281/1000, Tr Loss: 0.0142, Tr Acc: 99.6703, Val Loss: 0.7327, Val Acc: 80.9793\n",
      "Epoch 291/1000, Tr Loss: 0.0162, Tr Acc: 99.7174, Val Loss: 0.3112, Val Acc: 87.5706\n",
      "Epoch 301/1000, Tr Loss: 0.0131, Tr Acc: 99.7645, Val Loss: 0.1655, Val Acc: 93.0320\n",
      "Epoch 311/1000, Tr Loss: 0.0135, Tr Acc: 99.7645, Val Loss: 0.2010, Val Acc: 90.3955\n",
      "Epoch 321/1000, Tr Loss: 0.0154, Tr Acc: 99.5290, Val Loss: 0.4643, Val Acc: 84.0866\n",
      "Epoch 331/1000, Tr Loss: 0.0129, Tr Acc: 99.6703, Val Loss: 1.0272, Val Acc: 80.4143\n",
      "Epoch 341/1000, Tr Loss: 0.0140, Tr Acc: 99.3877, Val Loss: 0.3560, Val Acc: 86.7232\n",
      "Epoch 351/1000, Tr Loss: 0.0122, Tr Acc: 99.7645, Val Loss: 0.2832, Val Acc: 87.9473\n",
      "Epoch 361/1000, Tr Loss: 0.0114, Tr Acc: 99.6703, Val Loss: 0.5107, Val Acc: 84.1808\n",
      "Epoch 371/1000, Tr Loss: 0.0099, Tr Acc: 99.8116, Val Loss: 6.3308, Val Acc: 60.5461\n",
      "Epoch 381/1000, Tr Loss: 0.0143, Tr Acc: 99.5290, Val Loss: 0.6751, Val Acc: 82.6742\n",
      "Epoch 391/1000, Tr Loss: 0.0166, Tr Acc: 99.6232, Val Loss: 0.3216, Val Acc: 88.7006\n",
      "Epoch 401/1000, Tr Loss: 0.0110, Tr Acc: 99.7645, Val Loss: 0.2702, Val Acc: 89.6422\n",
      "Epoch 411/1000, Tr Loss: 0.0130, Tr Acc: 99.7174, Val Loss: 0.8808, Val Acc: 78.8136\n",
      "Epoch 421/1000, Tr Loss: 0.0126, Tr Acc: 99.6232, Val Loss: 0.2280, Val Acc: 90.3013\n",
      "Epoch 431/1000, Tr Loss: 0.0100, Tr Acc: 99.6703, Val Loss: 0.3145, Val Acc: 88.0414\n",
      "Epoch 441/1000, Tr Loss: 0.0198, Tr Acc: 99.6703, Val Loss: 0.2241, Val Acc: 90.6780\n",
      "Epoch 451/1000, Tr Loss: 0.0132, Tr Acc: 99.5290, Val Loss: 4.4343, Val Acc: 63.6535\n",
      "Epoch 461/1000, Tr Loss: 0.0115, Tr Acc: 99.6232, Val Loss: 0.3015, Val Acc: 87.3823\n",
      "Epoch 471/1000, Tr Loss: 0.0136, Tr Acc: 99.7174, Val Loss: 3.6231, Val Acc: 62.7119\n",
      "Epoch 481/1000, Tr Loss: 0.0115, Tr Acc: 99.6703, Val Loss: 3.8747, Val Acc: 58.4746\n",
      "Epoch 491/1000, Tr Loss: 0.0117, Tr Acc: 99.7645, Val Loss: 0.1732, Val Acc: 92.6554\n",
      "Epoch 501/1000, Tr Loss: 0.0099, Tr Acc: 99.8116, Val Loss: 0.7656, Val Acc: 81.2618\n",
      "Epoch 511/1000, Tr Loss: 0.0113, Tr Acc: 99.7645, Val Loss: 1.9514, Val Acc: 66.9492\n",
      "Epoch 521/1000, Tr Loss: 0.0134, Tr Acc: 99.7174, Val Loss: 1.6568, Val Acc: 73.0697\n",
      "Epoch 531/1000, Tr Loss: 0.0112, Tr Acc: 99.7174, Val Loss: 0.2527, Val Acc: 90.1130\n",
      "Epoch 541/1000, Tr Loss: 0.0133, Tr Acc: 99.7645, Val Loss: 0.9442, Val Acc: 78.7194\n",
      "Epoch 551/1000, Tr Loss: 0.0201, Tr Acc: 99.4819, Val Loss: 2.0944, Val Acc: 71.6573\n",
      "Epoch 561/1000, Tr Loss: 0.0125, Tr Acc: 99.4819, Val Loss: 1.3695, Val Acc: 74.7646\n",
      "Epoch 571/1000, Tr Loss: 0.0110, Tr Acc: 99.5761, Val Loss: 0.8769, Val Acc: 79.0019\n",
      "Epoch 581/1000, Tr Loss: 0.0139, Tr Acc: 99.6232, Val Loss: 0.3057, Val Acc: 86.9115\n",
      "Epoch 591/1000, Tr Loss: 0.0131, Tr Acc: 99.6232, Val Loss: 0.2138, Val Acc: 91.6196\n",
      "Epoch 601/1000, Tr Loss: 0.0137, Tr Acc: 99.6232, Val Loss: 0.1140, Val Acc: 94.9153\n",
      "Epoch 611/1000, Tr Loss: 0.0156, Tr Acc: 99.5761, Val Loss: 0.7150, Val Acc: 82.4859\n",
      "Epoch 621/1000, Tr Loss: 0.0120, Tr Acc: 99.4348, Val Loss: 1.5762, Val Acc: 72.4105\n",
      "Epoch 161/1000, Tr Loss: 0.0111, Tr Acc: 99.6703, Val Loss: 5.4821, Val Acc: 59.9812\n",
      "Epoch 171/1000, Tr Loss: 0.0163, Tr Acc: 99.6232, Val Loss: 1.2687, Val Acc: 76.1770\n",
      "Epoch 181/1000, Tr Loss: 0.0171, Tr Acc: 99.4348, Val Loss: 0.1183, Val Acc: 94.1620\n",
      "Epoch 191/1000, Tr Loss: 0.0161, Tr Acc: 99.6232, Val Loss: 1.3315, Val Acc: 72.6930\n",
      "Epoch 201/1000, Tr Loss: 0.0147, Tr Acc: 99.7174, Val Loss: 0.1649, Val Acc: 93.2203\n",
      "Epoch 211/1000, Tr Loss: 0.0162, Tr Acc: 99.6232, Val Loss: 0.2796, Val Acc: 89.8305\n",
      "Epoch 221/1000, Tr Loss: 0.0115, Tr Acc: 99.7174, Val Loss: 3.7713, Val Acc: 64.8776\n",
      "Epoch 231/1000, Tr Loss: 0.0141, Tr Acc: 99.5761, Val Loss: 5.3776, Val Acc: 60.9228\n",
      "Epoch 241/1000, Tr Loss: 0.0162, Tr Acc: 99.5761, Val Loss: 3.1215, Val Acc: 64.3126\n",
      "Epoch 251/1000, Tr Loss: 0.0154, Tr Acc: 99.6232, Val Loss: 3.4319, Val Acc: 65.4426\n",
      "Epoch 261/1000, Tr Loss: 0.0203, Tr Acc: 99.3406, Val Loss: 0.1301, Val Acc: 94.5386\n",
      "Epoch 271/1000, Tr Loss: 0.0182, Tr Acc: 99.2463, Val Loss: 1.9945, Val Acc: 72.5989\n",
      "Epoch 281/1000, Tr Loss: 0.0176, Tr Acc: 99.5761, Val Loss: 0.2450, Val Acc: 89.8305\n",
      "Epoch 291/1000, Tr Loss: 0.0103, Tr Acc: 99.7174, Val Loss: 0.1517, Val Acc: 92.7495\n",
      "Epoch 301/1000, Tr Loss: 0.0143, Tr Acc: 99.6703, Val Loss: 1.0550, Val Acc: 75.7062\n",
      "Epoch 311/1000, Tr Loss: 0.0118, Tr Acc: 99.7645, Val Loss: 1.9549, Val Acc: 73.6347\n",
      "Epoch 321/1000, Tr Loss: 0.0116, Tr Acc: 99.7174, Val Loss: 1.7190, Val Acc: 72.6930\n",
      "Epoch 331/1000, Tr Loss: 0.0152, Tr Acc: 99.6703, Val Loss: 0.2447, Val Acc: 90.1130\n",
      "Epoch 341/1000, Tr Loss: 0.0193, Tr Acc: 99.6232, Val Loss: 0.3368, Val Acc: 87.9473\n",
      "Epoch 351/1000, Tr Loss: 0.0086, Tr Acc: 99.7645, Val Loss: 0.2812, Val Acc: 90.6780\n",
      "Epoch 361/1000, Tr Loss: 0.0201, Tr Acc: 99.1521, Val Loss: 0.6775, Val Acc: 81.7326\n",
      "Epoch 371/1000, Tr Loss: 0.0141, Tr Acc: 99.7174, Val Loss: 0.2423, Val Acc: 90.2072\n",
      "Epoch 381/1000, Tr Loss: 0.0126, Tr Acc: 99.6232, Val Loss: 0.6293, Val Acc: 84.2750\n",
      "Epoch 391/1000, Tr Loss: 0.0100, Tr Acc: 99.7645, Val Loss: 3.0493, Val Acc: 62.9002\n",
      "Epoch 401/1000, Tr Loss: 0.0110, Tr Acc: 99.6232, Val Loss: 0.1732, Val Acc: 92.5612\n",
      "Epoch 411/1000, Tr Loss: 0.0168, Tr Acc: 99.4819, Val Loss: 0.3555, Val Acc: 87.3823\n",
      "Epoch 421/1000, Tr Loss: 0.0143, Tr Acc: 99.5290, Val Loss: 1.3051, Val Acc: 74.1996\n",
      "Epoch 431/1000, Tr Loss: 0.0172, Tr Acc: 99.6232, Val Loss: 2.4139, Val Acc: 67.4200\n",
      "Epoch 441/1000, Tr Loss: 0.0246, Tr Acc: 99.5290, Val Loss: 3.2727, Val Acc: 62.8060\n",
      "Epoch 451/1000, Tr Loss: 0.0198, Tr Acc: 99.5761, Val Loss: 1.1956, Val Acc: 73.7288\n",
      "Epoch 461/1000, Tr Loss: 0.0126, Tr Acc: 99.6232, Val Loss: 0.4955, Val Acc: 84.4633\n",
      "Epoch 471/1000, Tr Loss: 0.0145, Tr Acc: 99.6703, Val Loss: 1.3322, Val Acc: 77.3070\n",
      "Epoch 481/1000, Tr Loss: 0.0124, Tr Acc: 99.6232, Val Loss: 2.6636, Val Acc: 68.9266\n",
      "Epoch 491/1000, Tr Loss: 0.0102, Tr Acc: 99.6703, Val Loss: 0.3755, Val Acc: 86.8173\n",
      "Epoch 501/1000, Tr Loss: 0.0169, Tr Acc: 99.5761, Val Loss: 0.5367, Val Acc: 82.7684\n",
      "Epoch 511/1000, Tr Loss: 0.0114, Tr Acc: 99.5290, Val Loss: 0.2292, Val Acc: 90.8663\n",
      "Epoch 521/1000, Tr Loss: 0.0079, Tr Acc: 99.8587, Val Loss: 0.1613, Val Acc: 93.2203\n",
      "Epoch 531/1000, Tr Loss: 0.0136, Tr Acc: 99.5761, Val Loss: 0.1949, Val Acc: 91.1488\n",
      "Epoch 541/1000, Tr Loss: 0.0212, Tr Acc: 99.6232, Val Loss: 0.3182, Val Acc: 87.3823\n",
      "Epoch 551/1000, Tr Loss: 0.0114, Tr Acc: 99.6232, Val Loss: 0.1058, Val Acc: 95.2919\n",
      "Epoch 561/1000, Tr Loss: 0.0107, Tr Acc: 99.9058, Val Loss: 0.5978, Val Acc: 84.8399\n",
      "Epoch 571/1000, Tr Loss: 0.0134, Tr Acc: 99.6703, Val Loss: 0.1761, Val Acc: 91.4313\n",
      "Epoch 581/1000, Tr Loss: 0.0115, Tr Acc: 99.5761, Val Loss: 0.3803, Val Acc: 88.1356\n",
      "Epoch 591/1000, Tr Loss: 0.0106, Tr Acc: 99.5761, Val Loss: 2.9010, Val Acc: 66.6667\n",
      "Epoch 601/1000, Tr Loss: 0.0143, Tr Acc: 99.5761, Val Loss: 1.0900, Val Acc: 76.9303\n",
      "Epoch 611/1000, Tr Loss: 0.0173, Tr Acc: 99.4348, Val Loss: 3.2780, Val Acc: 64.4068\n",
      "Epoch 621/1000, Tr Loss: 0.0155, Tr Acc: 99.5761, Val Loss: 5.1168, Val Acc: 61.5819\n",
      "Epoch 631/1000, Tr Loss: 0.0098, Tr Acc: 99.8116, Val Loss: 1.6962, Val Acc: 71.4689\n",
      "Epoch 641/1000, Tr Loss: 0.0076, Tr Acc: 99.9058, Val Loss: 3.7055, Val Acc: 65.8192\n",
      "Epoch 651/1000, Tr Loss: 0.0108, Tr Acc: 99.8116, Val Loss: 0.3990, Val Acc: 88.0414\n",
      "Epoch 661/1000, Tr Loss: 0.0120, Tr Acc: 99.5290, Val Loss: 0.5773, Val Acc: 81.2618\n",
      "Epoch 671/1000, Tr Loss: 0.0159, Tr Acc: 99.4819, Val Loss: 1.0781, Val Acc: 77.6836\n",
      "Epoch 681/1000, Tr Loss: 0.0154, Tr Acc: 99.6232, Val Loss: 0.2382, Val Acc: 88.2298\n",
      "Epoch 691/1000, Tr Loss: 0.0129, Tr Acc: 99.5761, Val Loss: 0.0842, Val Acc: 96.4218\n",
      "Epoch 701/1000, Tr Loss: 0.0098, Tr Acc: 99.7645, Val Loss: 0.3625, Val Acc: 87.2881\n",
      "Epoch 711/1000, Tr Loss: 0.0115, Tr Acc: 99.8116, Val Loss: 0.3626, Val Acc: 87.3823\n",
      "Epoch 721/1000, Tr Loss: 0.0107, Tr Acc: 99.6232, Val Loss: 0.4404, Val Acc: 86.2524\n",
      "Epoch 731/1000, Tr Loss: 0.0137, Tr Acc: 99.5761, Val Loss: 2.1879, Val Acc: 69.0207\n",
      "Epoch 741/1000, Tr Loss: 0.0077, Tr Acc: 99.8116, Val Loss: 0.4355, Val Acc: 87.8531\n",
      "Epoch 751/1000, Tr Loss: 0.0111, Tr Acc: 99.6703, Val Loss: 1.7792, Val Acc: 71.8456\n",
      "Epoch 761/1000, Tr Loss: 0.0155, Tr Acc: 99.4819, Val Loss: 0.2780, Val Acc: 89.1714\n",
      "Epoch 771/1000, Tr Loss: 0.0138, Tr Acc: 99.8116, Val Loss: 1.0511, Val Acc: 79.5669\n",
      "Epoch 781/1000, Tr Loss: 0.0074, Tr Acc: 99.8587, Val Loss: 1.6975, Val Acc: 73.4463\n",
      "Epoch 791/1000, Tr Loss: 0.0071, Tr Acc: 99.8116, Val Loss: 0.2044, Val Acc: 91.8079\n",
      "Epoch 801/1000, Tr Loss: 0.0156, Tr Acc: 99.6703, Val Loss: 0.5619, Val Acc: 85.3107\n",
      "Epoch 811/1000, Tr Loss: 0.0124, Tr Acc: 99.7645, Val Loss: 1.2866, Val Acc: 72.5047\n",
      "Epoch 821/1000, Tr Loss: 0.0090, Tr Acc: 99.8116, Val Loss: 1.0236, Val Acc: 79.6610\n",
      "Epoch 831/1000, Tr Loss: 0.0083, Tr Acc: 99.8116, Val Loss: 0.2168, Val Acc: 89.9247\n",
      "Epoch 841/1000, Tr Loss: 0.0082, Tr Acc: 99.6703, Val Loss: 0.5872, Val Acc: 82.0151\n",
      "Epoch 851/1000, Tr Loss: 0.0087, Tr Acc: 99.8116, Val Loss: 0.4099, Val Acc: 86.4407\n",
      "Epoch 861/1000, Tr Loss: 0.0073, Tr Acc: 99.8116, Val Loss: 0.2976, Val Acc: 88.4181\n",
      "Epoch 871/1000, Tr Loss: 0.0108, Tr Acc: 99.6232, Val Loss: 0.4607, Val Acc: 85.9699\n",
      "Epoch 881/1000, Tr Loss: 0.0127, Tr Acc: 99.6703, Val Loss: 2.4864, Val Acc: 67.4200\n",
      "Epoch 891/1000, Tr Loss: 0.0123, Tr Acc: 99.6703, Val Loss: 0.9447, Val Acc: 78.3427\n",
      "Epoch 901/1000, Tr Loss: 0.0103, Tr Acc: 99.8587, Val Loss: 0.1321, Val Acc: 93.6911\n",
      "Epoch 911/1000, Tr Loss: 0.0133, Tr Acc: 99.7645, Val Loss: 0.4021, Val Acc: 84.8399\n",
      "Epoch 921/1000, Tr Loss: 0.0205, Tr Acc: 99.5290, Val Loss: 1.1627, Val Acc: 71.1864\n",
      "Epoch 931/1000, Tr Loss: 0.0113, Tr Acc: 99.7174, Val Loss: 1.7267, Val Acc: 70.2448\n",
      "Epoch 941/1000, Tr Loss: 0.0072, Tr Acc: 99.8587, Val Loss: 1.3586, Val Acc: 75.5179\n",
      "Epoch 951/1000, Tr Loss: 0.0148, Tr Acc: 99.7174, Val Loss: 3.0737, Val Acc: 69.0207\n",
      "Epoch 961/1000, Tr Loss: 0.0160, Tr Acc: 99.5761, Val Loss: 0.3640, Val Acc: 88.2298\n",
      "Epoch 971/1000, Tr Loss: 0.0092, Tr Acc: 99.7645, Val Loss: 0.5674, Val Acc: 84.5574\n",
      "Epoch 981/1000, Tr Loss: 0.0103, Tr Acc: 99.6232, Val Loss: 0.1254, Val Acc: 94.7269\n",
      "Epoch 991/1000, Tr Loss: 0.0094, Tr Acc: 99.7645, Val Loss: 1.0275, Val Acc: 79.2844\n",
      "Fold 3\n",
      "Epoch 1/1000, Tr Loss: 0.0326, Tr Acc: 99.0113, Val Loss: 0.6355, Val Acc: 79.4533\n",
      "Epoch 11/1000, Tr Loss: 0.0392, Tr Acc: 98.9171, Val Loss: 0.9402, Val Acc: 77.6626\n",
      "Epoch 21/1000, Tr Loss: 0.0229, Tr Acc: 99.1996, Val Loss: 7.5784, Val Acc: 57.2102\n",
      "Epoch 31/1000, Tr Loss: 0.0172, Tr Acc: 99.4350, Val Loss: 2.5968, Val Acc: 67.6720\n",
      "Epoch 41/1000, Tr Loss: 0.0240, Tr Acc: 99.2467, Val Loss: 0.9879, Val Acc: 77.0971\n",
      "Epoch 51/1000, Tr Loss: 0.0201, Tr Acc: 99.1996, Val Loss: 0.1795, Val Acc: 92.1772\n",
      "Epoch 61/1000, Tr Loss: 0.0221, Tr Acc: 99.3879, Val Loss: 3.1162, Val Acc: 64.5617\n",
      "Epoch 71/1000, Tr Loss: 0.0152, Tr Acc: 99.7175, Val Loss: 2.7888, Val Acc: 68.7088\n",
      "Epoch 81/1000, Tr Loss: 0.0174, Tr Acc: 99.5763, Val Loss: 4.9438, Val Acc: 59.9434\n",
      "Epoch 91/1000, Tr Loss: 0.0134, Tr Acc: 99.5763, Val Loss: 1.4792, Val Acc: 69.6513\n",
      "Epoch 101/1000, Tr Loss: 0.0107, Tr Acc: 99.8588, Val Loss: 2.0981, Val Acc: 70.2168\n",
      "Epoch 111/1000, Tr Loss: 0.0185, Tr Acc: 99.4350, Val Loss: 3.7629, Val Acc: 63.7135\n",
      "Epoch 121/1000, Tr Loss: 0.0151, Tr Acc: 99.7175, Val Loss: 0.2817, Val Acc: 89.5382\n",
      "Epoch 131/1000, Tr Loss: 0.0154, Tr Acc: 99.6704, Val Loss: 2.8821, Val Acc: 69.8398\n",
      "Epoch 141/1000, Tr Loss: 0.0155, Tr Acc: 99.5763, Val Loss: 1.9303, Val Acc: 67.2008\n",
      "Epoch 151/1000, Tr Loss: 0.0178, Tr Acc: 99.5292, Val Loss: 4.5734, Val Acc: 65.4100\n",
      "Epoch 161/1000, Tr Loss: 0.0182, Tr Acc: 99.3879, Val Loss: 2.6296, Val Acc: 67.4835\n",
      "Epoch 171/1000, Tr Loss: 0.0139, Tr Acc: 99.6234, Val Loss: 0.8318, Val Acc: 78.2281\n",
      "Epoch 181/1000, Tr Loss: 0.0159, Tr Acc: 99.4821, Val Loss: 0.5424, Val Acc: 84.3544\n",
      "Epoch 191/1000, Tr Loss: 0.0165, Tr Acc: 99.4350, Val Loss: 0.1191, Val Acc: 94.9105\n",
      "Epoch 201/1000, Tr Loss: 0.0185, Tr Acc: 99.5763, Val Loss: 1.3280, Val Acc: 76.0603\n",
      "Epoch 211/1000, Tr Loss: 0.0125, Tr Acc: 99.6234, Val Loss: 0.0712, Val Acc: 97.2667\n",
      "Epoch 221/1000, Tr Loss: 0.0149, Tr Acc: 99.5292, Val Loss: 2.0599, Val Acc: 69.4628\n",
      "Epoch 231/1000, Tr Loss: 0.0138, Tr Acc: 99.5292, Val Loss: 0.2449, Val Acc: 88.8784\n",
      "Epoch 241/1000, Tr Loss: 0.0201, Tr Acc: 99.5292, Val Loss: 0.6075, Val Acc: 81.6211\n",
      "Epoch 251/1000, Tr Loss: 0.0112, Tr Acc: 99.6704, Val Loss: 0.3200, Val Acc: 87.5589\n",
      "Epoch 261/1000, Tr Loss: 0.0119, Tr Acc: 99.7646, Val Loss: 0.7297, Val Acc: 79.8303\n",
      "Epoch 271/1000, Tr Loss: 0.0197, Tr Acc: 99.4821, Val Loss: 0.9124, Val Acc: 78.0396\n",
      "Epoch 281/1000, Tr Loss: 0.0205, Tr Acc: 99.3409, Val Loss: 0.3147, Val Acc: 87.8417\n",
      "Epoch 291/1000, Tr Loss: 0.0099, Tr Acc: 99.7646, Val Loss: 1.6210, Val Acc: 71.4420\n",
      "Epoch 301/1000, Tr Loss: 0.0107, Tr Acc: 99.8117, Val Loss: 0.3717, Val Acc: 86.0509\n",
      "Epoch 311/1000, Tr Loss: 0.0138, Tr Acc: 99.6704, Val Loss: 0.4359, Val Acc: 85.7681\n",
      "Epoch 321/1000, Tr Loss: 0.0125, Tr Acc: 99.6704, Val Loss: 2.1752, Val Acc: 69.4628\n",
      "Epoch 331/1000, Tr Loss: 0.0143, Tr Acc: 99.6234, Val Loss: 1.0484, Val Acc: 77.7568\n",
      "Epoch 341/1000, Tr Loss: 0.0132, Tr Acc: 99.6704, Val Loss: 4.5016, Val Acc: 61.0745\n",
      "Epoch 351/1000, Tr Loss: 0.0098, Tr Acc: 99.8588, Val Loss: 0.2385, Val Acc: 89.6324\n",
      "Epoch 361/1000, Tr Loss: 0.0140, Tr Acc: 99.4821, Val Loss: 0.1700, Val Acc: 92.3657\n",
      "Epoch 371/1000, Tr Loss: 0.0075, Tr Acc: 99.8588, Val Loss: 1.3452, Val Acc: 76.9086\n",
      "Epoch 381/1000, Tr Loss: 0.0169, Tr Acc: 99.6704, Val Loss: 9.5396, Val Acc: 52.9689\n",
      "Epoch 391/1000, Tr Loss: 0.0177, Tr Acc: 99.4350, Val Loss: 1.6717, Val Acc: 72.8558\n",
      "Epoch 401/1000, Tr Loss: 0.0156, Tr Acc: 99.5292, Val Loss: 11.7405, Val Acc: 54.2884\n",
      "Epoch 411/1000, Tr Loss: 0.0153, Tr Acc: 99.4821, Val Loss: 0.2213, Val Acc: 89.3497\n",
      "Epoch 421/1000, Tr Loss: 0.0255, Tr Acc: 99.5763, Val Loss: 0.0814, Val Acc: 96.5127\n",
      "Epoch 431/1000, Tr Loss: 0.0128, Tr Acc: 99.9058, Val Loss: 0.2985, Val Acc: 87.3704\n",
      "Epoch 441/1000, Tr Loss: 0.0083, Tr Acc: 99.8588, Val Loss: 0.9619, Val Acc: 76.4373\n",
      "Epoch 451/1000, Tr Loss: 0.0124, Tr Acc: 99.5763, Val Loss: 0.8898, Val Acc: 79.7361\n",
      "Epoch 461/1000, Tr Loss: 0.0129, Tr Acc: 99.6704, Val Loss: 1.0404, Val Acc: 77.2856\n",
      "Epoch 471/1000, Tr Loss: 0.0082, Tr Acc: 99.8588, Val Loss: 0.4732, Val Acc: 85.1084\n",
      "Epoch 481/1000, Tr Loss: 0.0098, Tr Acc: 99.8117, Val Loss: 2.6366, Val Acc: 67.9548\n",
      "Epoch 491/1000, Tr Loss: 0.0112, Tr Acc: 99.8117, Val Loss: 1.9594, Val Acc: 68.6145\n",
      "Epoch 501/1000, Tr Loss: 0.0114, Tr Acc: 99.6704, Val Loss: 0.2128, Val Acc: 90.5749\n",
      "Epoch 511/1000, Tr Loss: 0.0136, Tr Acc: 99.6234, Val Loss: 1.7770, Val Acc: 68.9915\n",
      "Epoch 521/1000, Tr Loss: 0.0169, Tr Acc: 99.6234, Val Loss: 0.9274, Val Acc: 78.1338\n",
      "Epoch 531/1000, Tr Loss: 0.0119, Tr Acc: 99.5763, Val Loss: 3.4348, Val Acc: 62.2055\n",
      "Epoch 541/1000, Tr Loss: 0.0093, Tr Acc: 99.7175, Val Loss: 0.1264, Val Acc: 93.5910\n",
      "Epoch 551/1000, Tr Loss: 0.0128, Tr Acc: 99.6704, Val Loss: 0.2326, Val Acc: 90.0094\n",
      "Epoch 561/1000, Tr Loss: 0.0159, Tr Acc: 99.4821, Val Loss: 2.2879, Val Acc: 70.4995\n",
      "Epoch 571/1000, Tr Loss: 0.0092, Tr Acc: 99.9529, Val Loss: 0.0832, Val Acc: 96.7955\n",
      "Epoch 581/1000, Tr Loss: 0.0142, Tr Acc: 99.6234, Val Loss: 1.2748, Val Acc: 72.1960\n",
      "Epoch 591/1000, Tr Loss: 0.0200, Tr Acc: 99.5292, Val Loss: 2.7216, Val Acc: 69.8398\n",
      "Epoch 601/1000, Tr Loss: 0.0091, Tr Acc: 99.7175, Val Loss: 0.2890, Val Acc: 88.4072\n",
      "Epoch 611/1000, Tr Loss: 0.0156, Tr Acc: 99.6704, Val Loss: 0.1804, Val Acc: 91.9887\n",
      "Epoch 621/1000, Tr Loss: 0.0138, Tr Acc: 99.4350, Val Loss: 0.7650, Val Acc: 78.7936\n",
      "Epoch 631/1000, Tr Loss: 0.0118, Tr Acc: 99.7175, Val Loss: 3.7698, Val Acc: 59.3779\n",
      "Epoch 641/1000, Tr Loss: 0.0162, Tr Acc: 99.5763, Val Loss: 1.8783, Val Acc: 69.1800\n",
      "Epoch 651/1000, Tr Loss: 0.0121, Tr Acc: 99.7646, Val Loss: 0.2260, Val Acc: 90.5749\n",
      "Epoch 661/1000, Tr Loss: 0.0101, Tr Acc: 99.7175, Val Loss: 1.6312, Val Acc: 71.5363\n",
      "Epoch 671/1000, Tr Loss: 0.0119, Tr Acc: 99.7175, Val Loss: 0.1940, Val Acc: 90.9519\n",
      "Epoch 681/1000, Tr Loss: 0.0092, Tr Acc: 99.7646, Val Loss: 1.0697, Val Acc: 77.0028\n",
      "Epoch 691/1000, Tr Loss: 0.0100, Tr Acc: 99.7175, Val Loss: 0.4925, Val Acc: 82.2809\n",
      "Epoch 701/1000, Tr Loss: 0.0111, Tr Acc: 99.5763, Val Loss: 0.3981, Val Acc: 86.7107\n",
      "Epoch 711/1000, Tr Loss: 0.0116, Tr Acc: 99.7175, Val Loss: 0.2369, Val Acc: 89.8209\n",
      "Epoch 721/1000, Tr Loss: 0.0136, Tr Acc: 99.5763, Val Loss: 1.0735, Val Acc: 80.6786\n",
      "Epoch 731/1000, Tr Loss: 0.0127, Tr Acc: 99.6704, Val Loss: 0.5076, Val Acc: 85.1084\n",
      "Epoch 741/1000, Tr Loss: 0.0264, Tr Acc: 99.6234, Val Loss: 0.7552, Val Acc: 78.4166\n",
      "Epoch 751/1000, Tr Loss: 0.0087, Tr Acc: 99.9058, Val Loss: 0.3044, Val Acc: 88.4072\n",
      "Epoch 761/1000, Tr Loss: 0.0105, Tr Acc: 99.8588, Val Loss: 1.0449, Val Acc: 76.9086\n",
      "Epoch 771/1000, Tr Loss: 0.0106, Tr Acc: 99.8117, Val Loss: 2.3749, Val Acc: 65.5042\n",
      "Epoch 781/1000, Tr Loss: 0.0076, Tr Acc: 99.8117, Val Loss: 3.5094, Val Acc: 66.6352\n",
      "Epoch 791/1000, Tr Loss: 0.0200, Tr Acc: 99.5763, Val Loss: 1.3320, Val Acc: 70.0283\n",
      "Epoch 801/1000, Tr Loss: 0.0099, Tr Acc: 99.7175, Val Loss: 0.2239, Val Acc: 90.6692\n",
      "Epoch 811/1000, Tr Loss: 0.0092, Tr Acc: 99.7175, Val Loss: 0.6286, Val Acc: 80.9614\n",
      "Epoch 821/1000, Tr Loss: 0.0140, Tr Acc: 99.5763, Val Loss: 0.7432, Val Acc: 78.4166\n",
      "Epoch 831/1000, Tr Loss: 0.0114, Tr Acc: 99.7175, Val Loss: 2.8197, Val Acc: 67.7663\n",
      "Epoch 841/1000, Tr Loss: 0.0096, Tr Acc: 99.8117, Val Loss: 0.2466, Val Acc: 89.3497\n",
      "Epoch 851/1000, Tr Loss: 0.0149, Tr Acc: 99.6704, Val Loss: 4.4727, Val Acc: 60.4147\n",
      "Epoch 861/1000, Tr Loss: 0.0107, Tr Acc: 99.7646, Val Loss: 2.8405, Val Acc: 69.5570\n",
      "Epoch 871/1000, Tr Loss: 0.0090, Tr Acc: 99.7175, Val Loss: 0.2110, Val Acc: 91.4232\n",
      "Epoch 881/1000, Tr Loss: 0.0093, Tr Acc: 99.8117, Val Loss: 0.4136, Val Acc: 83.6946\n",
      "Epoch 891/1000, Tr Loss: 0.0117, Tr Acc: 99.7646, Val Loss: 0.9625, Val Acc: 76.8143\n",
      "Epoch 901/1000, Tr Loss: 0.0122, Tr Acc: 99.7175, Val Loss: 2.5425, Val Acc: 70.1225\n",
      "Epoch 911/1000, Tr Loss: 0.0157, Tr Acc: 99.6704, Val Loss: 2.3210, Val Acc: 65.0330\n",
      "Epoch 921/1000, Tr Loss: 0.0088, Tr Acc: 99.7646, Val Loss: 0.2328, Val Acc: 90.0094\n",
      "Epoch 931/1000, Tr Loss: 0.0087, Tr Acc: 99.8117, Val Loss: 3.5618, Val Acc: 62.2055\n",
      "Epoch 941/1000, Tr Loss: 0.0106, Tr Acc: 99.8117, Val Loss: 0.7685, Val Acc: 79.3591\n",
      "Epoch 951/1000, Tr Loss: 0.0152, Tr Acc: 99.4350, Val Loss: 0.4694, Val Acc: 83.2234\n",
      "Epoch 961/1000, Tr Loss: 0.0089, Tr Acc: 99.8588, Val Loss: 0.0609, Val Acc: 97.6437\n",
      "Epoch 971/1000, Tr Loss: 0.0078, Tr Acc: 99.8588, Val Loss: 2.3224, Val Acc: 72.1018\n",
      "Epoch 981/1000, Tr Loss: 0.0100, Tr Acc: 99.7646, Val Loss: 0.7697, Val Acc: 81.9039\n",
      "Epoch 991/1000, Tr Loss: 0.0110, Tr Acc: 99.5763, Val Loss: 5.5258, Val Acc: 58.2469\n",
      "Fold 1\n",
      "Epoch 1/1000, Tr Loss: 0.0208, Tr Acc: 99.5290, Val Loss: 1.1306, Val Acc: 79.8493\n",
      "Epoch 11/1000, Tr Loss: 0.0277, Tr Acc: 99.3406, Val Loss: 1.2425, Val Acc: 75.3296\n",
      "Epoch 21/1000, Tr Loss: 0.0271, Tr Acc: 99.1992, Val Loss: 1.4921, Val Acc: 72.4105\n",
      "Epoch 31/1000, Tr Loss: 0.0206, Tr Acc: 99.2463, Val Loss: 2.3973, Val Acc: 70.4331\n",
      "Epoch 41/1000, Tr Loss: 0.0147, Tr Acc: 99.7174, Val Loss: 0.5029, Val Acc: 83.0508\n",
      "Epoch 51/1000, Tr Loss: 0.0256, Tr Acc: 99.5290, Val Loss: 1.1883, Val Acc: 75.7062\n",
      "Epoch 61/1000, Tr Loss: 0.0168, Tr Acc: 99.5761, Val Loss: 0.3466, Val Acc: 86.5348\n",
      "Epoch 71/1000, Tr Loss: 0.0161, Tr Acc: 99.3877, Val Loss: 6.9635, Val Acc: 54.8023\n",
      "Epoch 81/1000, Tr Loss: 0.0089, Tr Acc: 99.7645, Val Loss: 1.5784, Val Acc: 76.7420\n",
      "Epoch 91/1000, Tr Loss: 0.0250, Tr Acc: 99.5290, Val Loss: 0.5130, Val Acc: 84.5574\n",
      "Epoch 101/1000, Tr Loss: 0.0169, Tr Acc: 99.3877, Val Loss: 0.7087, Val Acc: 80.7910\n",
      "Epoch 111/1000, Tr Loss: 0.0166, Tr Acc: 99.4819, Val Loss: 1.0076, Val Acc: 78.6252\n",
      "Epoch 121/1000, Tr Loss: 0.0156, Tr Acc: 99.5761, Val Loss: 2.4591, Val Acc: 70.5273\n",
      "Epoch 131/1000, Tr Loss: 0.0153, Tr Acc: 99.6703, Val Loss: 0.2547, Val Acc: 90.3013\n",
      "Epoch 141/1000, Tr Loss: 0.0095, Tr Acc: 99.8587, Val Loss: 0.3580, Val Acc: 86.2524\n",
      "Epoch 151/1000, Tr Loss: 0.0120, Tr Acc: 99.7645, Val Loss: 0.3724, Val Acc: 88.8889\n",
      "Epoch 161/1000, Tr Loss: 0.0142, Tr Acc: 99.6703, Val Loss: 3.2082, Val Acc: 69.7740\n",
      "Epoch 171/1000, Tr Loss: 0.0124, Tr Acc: 99.8116, Val Loss: 1.9289, Val Acc: 69.7740\n",
      "Epoch 181/1000, Tr Loss: 0.0242, Tr Acc: 99.3877, Val Loss: 1.2548, Val Acc: 75.0471\n",
      "Epoch 191/1000, Tr Loss: 0.0134, Tr Acc: 99.7174, Val Loss: 2.5896, Val Acc: 70.2448\n",
      "Epoch 201/1000, Tr Loss: 0.0118, Tr Acc: 99.7174, Val Loss: 0.3176, Val Acc: 88.4181\n",
      "Epoch 211/1000, Tr Loss: 0.0164, Tr Acc: 99.6232, Val Loss: 4.7919, Val Acc: 60.1695\n",
      "Epoch 221/1000, Tr Loss: 0.0100, Tr Acc: 99.7174, Val Loss: 0.3830, Val Acc: 85.5932\n",
      "Epoch 231/1000, Tr Loss: 0.0136, Tr Acc: 99.6232, Val Loss: 0.1799, Val Acc: 92.5612\n",
      "Epoch 241/1000, Tr Loss: 0.0198, Tr Acc: 99.6232, Val Loss: 0.4130, Val Acc: 86.1582\n",
      "Epoch 251/1000, Tr Loss: 0.0135, Tr Acc: 99.5290, Val Loss: 1.5839, Val Acc: 73.3522\n",
      "Epoch 261/1000, Tr Loss: 0.0139, Tr Acc: 99.5761, Val Loss: 0.2838, Val Acc: 88.7006\n",
      "Epoch 271/1000, Tr Loss: 0.0110, Tr Acc: 99.6703, Val Loss: 0.3294, Val Acc: 86.9115\n",
      "Epoch 281/1000, Tr Loss: 0.0181, Tr Acc: 99.4819, Val Loss: 1.7503, Val Acc: 72.5047\n",
      "Epoch 291/1000, Tr Loss: 0.0165, Tr Acc: 99.4819, Val Loss: 1.3027, Val Acc: 78.1544\n",
      "Epoch 301/1000, Tr Loss: 0.0167, Tr Acc: 99.7174, Val Loss: 0.0628, Val Acc: 97.4576\n",
      "Epoch 311/1000, Tr Loss: 0.0157, Tr Acc: 99.6232, Val Loss: 0.3555, Val Acc: 85.2166\n",
      "Epoch 321/1000, Tr Loss: 0.0180, Tr Acc: 99.5290, Val Loss: 1.3936, Val Acc: 74.4821\n",
      "Epoch 331/1000, Tr Loss: 0.0098, Tr Acc: 99.7645, Val Loss: 1.1708, Val Acc: 77.1186\n",
      "Epoch 341/1000, Tr Loss: 0.0115, Tr Acc: 99.8116, Val Loss: 0.1260, Val Acc: 93.5028\n",
      "Epoch 351/1000, Tr Loss: 0.0147, Tr Acc: 99.8116, Val Loss: 0.2147, Val Acc: 90.7721\n",
      "Epoch 361/1000, Tr Loss: 0.0084, Tr Acc: 99.8587, Val Loss: 0.1757, Val Acc: 91.7137\n",
      "Epoch 371/1000, Tr Loss: 0.0123, Tr Acc: 99.6232, Val Loss: 0.2808, Val Acc: 88.8889\n",
      "Epoch 381/1000, Tr Loss: 0.0086, Tr Acc: 99.8587, Val Loss: 2.9762, Val Acc: 67.5141\n",
      "Epoch 391/1000, Tr Loss: 0.0144, Tr Acc: 99.4348, Val Loss: 0.2094, Val Acc: 89.9247\n",
      "Epoch 401/1000, Tr Loss: 0.0102, Tr Acc: 99.5761, Val Loss: 0.3512, Val Acc: 86.6290\n",
      "Epoch 411/1000, Tr Loss: 0.0102, Tr Acc: 99.8116, Val Loss: 0.1997, Val Acc: 90.8663\n",
      "Epoch 421/1000, Tr Loss: 0.0109, Tr Acc: 99.6703, Val Loss: 2.5530, Val Acc: 73.0697\n",
      "Epoch 431/1000, Tr Loss: 0.0117, Tr Acc: 99.8116, Val Loss: 0.3169, Val Acc: 88.4181\n",
      "Epoch 441/1000, Tr Loss: 0.0108, Tr Acc: 99.5761, Val Loss: 1.2426, Val Acc: 76.8362\n",
      "Epoch 451/1000, Tr Loss: 0.0152, Tr Acc: 99.6232, Val Loss: 3.0572, Val Acc: 66.1017\n",
      "Epoch 461/1000, Tr Loss: 0.0079, Tr Acc: 99.8587, Val Loss: 0.2809, Val Acc: 88.9831\n",
      "Epoch 471/1000, Tr Loss: 0.0101, Tr Acc: 99.8116, Val Loss: 0.2571, Val Acc: 89.6422\n",
      "Epoch 481/1000, Tr Loss: 0.0104, Tr Acc: 99.7174, Val Loss: 0.5838, Val Acc: 81.9209\n",
      "Epoch 491/1000, Tr Loss: 0.0139, Tr Acc: 99.6232, Val Loss: 0.2797, Val Acc: 89.6422\n",
      "Epoch 501/1000, Tr Loss: 0.0138, Tr Acc: 99.6703, Val Loss: 2.5435, Val Acc: 65.4426\n",
      "Epoch 511/1000, Tr Loss: 0.0105, Tr Acc: 99.6703, Val Loss: 2.2055, Val Acc: 69.8682\n",
      "Epoch 521/1000, Tr Loss: 0.0125, Tr Acc: 99.5761, Val Loss: 1.9632, Val Acc: 75.3296\n",
      "Epoch 531/1000, Tr Loss: 0.0078, Tr Acc: 99.8587, Val Loss: 0.0290, Val Acc: 99.4350\n",
      "Epoch 541/1000, Tr Loss: 0.0143, Tr Acc: 99.5761, Val Loss: 4.5418, Val Acc: 65.1601\n",
      "Epoch 551/1000, Tr Loss: 0.0108, Tr Acc: 99.6703, Val Loss: 0.7459, Val Acc: 80.5085\n",
      "Epoch 561/1000, Tr Loss: 0.0124, Tr Acc: 99.7174, Val Loss: 0.6166, Val Acc: 80.6026\n",
      "Epoch 571/1000, Tr Loss: 0.0096, Tr Acc: 99.7645, Val Loss: 0.2401, Val Acc: 90.5838\n",
      "Epoch 581/1000, Tr Loss: 0.0132, Tr Acc: 99.7174, Val Loss: 1.2803, Val Acc: 75.3296\n",
      "Epoch 591/1000, Tr Loss: 0.0100, Tr Acc: 99.7645, Val Loss: 0.1701, Val Acc: 92.4670\n",
      "Epoch 601/1000, Tr Loss: 0.0111, Tr Acc: 99.7645, Val Loss: 0.8975, Val Acc: 77.8719\n",
      "Epoch 611/1000, Tr Loss: 0.0095, Tr Acc: 99.6232, Val Loss: 0.4835, Val Acc: 82.4859\n",
      "Epoch 621/1000, Tr Loss: 0.0092, Tr Acc: 99.6703, Val Loss: 3.7969, Val Acc: 65.5367\n",
      "Epoch 631/1000, Tr Loss: 0.0196, Tr Acc: 99.5761, Val Loss: 0.4162, Val Acc: 86.3465\n",
      "Epoch 641/1000, Tr Loss: 0.0108, Tr Acc: 99.7174, Val Loss: 1.1376, Val Acc: 78.4369\n",
      "Epoch 651/1000, Tr Loss: 0.0084, Tr Acc: 99.8587, Val Loss: 0.1160, Val Acc: 95.1977\n",
      "Epoch 661/1000, Tr Loss: 0.0069, Tr Acc: 99.8116, Val Loss: 0.3137, Val Acc: 87.7589\n",
      "Epoch 671/1000, Tr Loss: 0.0092, Tr Acc: 99.7174, Val Loss: 1.3658, Val Acc: 74.6704\n",
      "Epoch 681/1000, Tr Loss: 0.0072, Tr Acc: 99.8587, Val Loss: 0.3605, Val Acc: 87.6648\n",
      "Epoch 691/1000, Tr Loss: 0.0098, Tr Acc: 99.6703, Val Loss: 0.2477, Val Acc: 89.3597\n",
      "Epoch 701/1000, Tr Loss: 0.0131, Tr Acc: 99.6232, Val Loss: 1.1240, Val Acc: 76.9303\n",
      "Epoch 711/1000, Tr Loss: 0.0074, Tr Acc: 99.8587, Val Loss: 1.6470, Val Acc: 74.1055\n",
      "Epoch 721/1000, Tr Loss: 0.0110, Tr Acc: 99.7645, Val Loss: 1.3161, Val Acc: 75.8004\n",
      "Epoch 731/1000, Tr Loss: 0.0115, Tr Acc: 99.5290, Val Loss: 0.4722, Val Acc: 84.7458\n",
      "Epoch 741/1000, Tr Loss: 0.0120, Tr Acc: 99.7174, Val Loss: 2.4389, Val Acc: 67.8908\n",
      "Epoch 751/1000, Tr Loss: 0.0097, Tr Acc: 99.8116, Val Loss: 0.2589, Val Acc: 89.3597\n",
      "Epoch 761/1000, Tr Loss: 0.0064, Tr Acc: 99.8116, Val Loss: 0.5844, Val Acc: 83.5217\n",
      "Epoch 771/1000, Tr Loss: 0.0130, Tr Acc: 99.6232, Val Loss: 1.8572, Val Acc: 75.6121\n",
      "Epoch 781/1000, Tr Loss: 0.0065, Tr Acc: 99.9058, Val Loss: 0.3191, Val Acc: 87.0998\n",
      "Epoch 791/1000, Tr Loss: 0.0105, Tr Acc: 99.7174, Val Loss: 0.9188, Val Acc: 76.7420\n",
      "Epoch 801/1000, Tr Loss: 0.0090, Tr Acc: 99.6703, Val Loss: 2.3223, Val Acc: 68.9266\n",
      "Epoch 811/1000, Tr Loss: 0.0113, Tr Acc: 99.6232, Val Loss: 0.0907, Val Acc: 96.2335\n",
      "Epoch 821/1000, Tr Loss: 0.0079, Tr Acc: 99.8587, Val Loss: 1.7227, Val Acc: 73.7288\n",
      "Epoch 831/1000, Tr Loss: 0.0098, Tr Acc: 99.6232, Val Loss: 0.2889, Val Acc: 87.2881\n",
      "Epoch 841/1000, Tr Loss: 0.0082, Tr Acc: 99.8116, Val Loss: 0.3861, Val Acc: 84.7458\n",
      "Epoch 851/1000, Tr Loss: 0.0085, Tr Acc: 99.9058, Val Loss: 5.6415, Val Acc: 56.9680\n",
      "Epoch 861/1000, Tr Loss: 0.0088, Tr Acc: 99.9058, Val Loss: 0.2889, Val Acc: 88.0414\n",
      "Epoch 871/1000, Tr Loss: 0.0105, Tr Acc: 99.7174, Val Loss: 3.6474, Val Acc: 63.3710\n",
      "Epoch 881/1000, Tr Loss: 0.0080, Tr Acc: 99.8116, Val Loss: 0.7924, Val Acc: 81.3559\n",
      "Epoch 891/1000, Tr Loss: 0.0080, Tr Acc: 99.7645, Val Loss: 0.6480, Val Acc: 81.6384\n",
      "Epoch 901/1000, Tr Loss: 0.0102, Tr Acc: 99.8116, Val Loss: 0.3216, Val Acc: 86.7232\n",
      "Epoch 911/1000, Tr Loss: 0.0099, Tr Acc: 99.6703, Val Loss: 5.8524, Val Acc: 56.1205\n",
      "Epoch 921/1000, Tr Loss: 0.0067, Tr Acc: 99.7645, Val Loss: 0.2204, Val Acc: 90.8663\n",
      "Epoch 931/1000, Tr Loss: 0.0087, Tr Acc: 99.7174, Val Loss: 1.0690, Val Acc: 73.2580\n",
      "Epoch 941/1000, Tr Loss: 0.0097, Tr Acc: 99.7645, Val Loss: 1.1170, Val Acc: 75.0471\n",
      "Epoch 951/1000, Tr Loss: 0.0082, Tr Acc: 99.8587, Val Loss: 0.8462, Val Acc: 77.0245\n",
      "Epoch 961/1000, Tr Loss: 0.0055, Tr Acc: 99.9058, Val Loss: 1.9029, Val Acc: 68.1733\n",
      "Epoch 971/1000, Tr Loss: 0.0069, Tr Acc: 99.8116, Val Loss: 0.1206, Val Acc: 93.9736\n",
      "Epoch 981/1000, Tr Loss: 0.0068, Tr Acc: 99.8587, Val Loss: 0.5838, Val Acc: 81.3559\n",
      "Epoch 991/1000, Tr Loss: 0.0110, Tr Acc: 99.6703, Val Loss: 1.9328, Val Acc: 72.6930\n",
      "Fold 2\n",
      "Epoch 1/1000, Tr Loss: 0.0251, Tr Acc: 99.3877, Val Loss: 1.4190, Val Acc: 73.9171\n",
      "Epoch 11/1000, Tr Loss: 0.0182, Tr Acc: 99.4348, Val Loss: 0.3579, Val Acc: 88.4181\n",
      "Epoch 21/1000, Tr Loss: 0.0246, Tr Acc: 99.2934, Val Loss: 2.9364, Val Acc: 65.5367\n",
      "Epoch 31/1000, Tr Loss: 0.0128, Tr Acc: 99.6703, Val Loss: 0.5020, Val Acc: 84.8399\n",
      "Epoch 41/1000, Tr Loss: 0.0165, Tr Acc: 99.4348, Val Loss: 1.3271, Val Acc: 74.6704\n",
      "Epoch 51/1000, Tr Loss: 0.0127, Tr Acc: 99.4819, Val Loss: 0.3281, Val Acc: 87.8531\n",
      "Epoch 61/1000, Tr Loss: 0.0140, Tr Acc: 99.4348, Val Loss: 0.2130, Val Acc: 90.6780\n",
      "Epoch 71/1000, Tr Loss: 0.0147, Tr Acc: 99.4819, Val Loss: 0.3646, Val Acc: 85.9699\n",
      "Epoch 81/1000, Tr Loss: 0.0204, Tr Acc: 99.3406, Val Loss: 0.9679, Val Acc: 77.5895\n",
      "Epoch 91/1000, Tr Loss: 0.0125, Tr Acc: 99.5761, Val Loss: 0.6453, Val Acc: 84.2750\n",
      "Epoch 101/1000, Tr Loss: 0.0187, Tr Acc: 99.5761, Val Loss: 0.0969, Val Acc: 95.2919\n",
      "Epoch 111/1000, Tr Loss: 0.0128, Tr Acc: 99.5761, Val Loss: 7.0301, Val Acc: 53.9548\n",
      "Epoch 121/1000, Tr Loss: 0.0219, Tr Acc: 99.5290, Val Loss: 0.3509, Val Acc: 85.9699\n",
      "Epoch 131/1000, Tr Loss: 0.0174, Tr Acc: 99.5761, Val Loss: 1.8691, Val Acc: 72.5989\n",
      "Epoch 141/1000, Tr Loss: 0.0125, Tr Acc: 99.5761, Val Loss: 0.5567, Val Acc: 82.6742\n",
      "Epoch 151/1000, Tr Loss: 0.0156, Tr Acc: 99.6232, Val Loss: 0.9053, Val Acc: 77.8719\n",
      "Epoch 161/1000, Tr Loss: 0.0068, Tr Acc: 99.9529, Val Loss: 3.2617, Val Acc: 66.5725\n",
      "Epoch 171/1000, Tr Loss: 0.0182, Tr Acc: 99.3406, Val Loss: 1.0199, Val Acc: 74.1055\n",
      "Epoch 181/1000, Tr Loss: 0.0114, Tr Acc: 99.6703, Val Loss: 3.4842, Val Acc: 62.4294\n",
      "Epoch 191/1000, Tr Loss: 0.0131, Tr Acc: 99.6703, Val Loss: 0.1316, Val Acc: 94.1620\n",
      "Epoch 201/1000, Tr Loss: 0.0133, Tr Acc: 99.6232, Val Loss: 0.8756, Val Acc: 81.4501\n",
      "Epoch 211/1000, Tr Loss: 0.0086, Tr Acc: 99.9058, Val Loss: 0.1879, Val Acc: 92.3729\n",
      "Epoch 221/1000, Tr Loss: 0.0147, Tr Acc: 99.4348, Val Loss: 4.7197, Val Acc: 58.7571\n",
      "Epoch 231/1000, Tr Loss: 0.0091, Tr Acc: 99.6703, Val Loss: 1.5420, Val Acc: 77.7778\n",
      "Epoch 241/1000, Tr Loss: 0.0096, Tr Acc: 99.7645, Val Loss: 0.8623, Val Acc: 78.9077\n",
      "Epoch 251/1000, Tr Loss: 0.0173, Tr Acc: 99.5290, Val Loss: 0.6634, Val Acc: 81.9209\n",
      "Epoch 261/1000, Tr Loss: 0.0166, Tr Acc: 99.4819, Val Loss: 5.4975, Val Acc: 58.4746\n",
      "Epoch 271/1000, Tr Loss: 0.0153, Tr Acc: 99.7174, Val Loss: 0.3732, Val Acc: 87.7589\n",
      "Epoch 281/1000, Tr Loss: 0.0070, Tr Acc: 99.8116, Val Loss: 0.0482, Val Acc: 98.0226\n",
      "Epoch 291/1000, Tr Loss: 0.0076, Tr Acc: 99.8587, Val Loss: 0.2770, Val Acc: 89.1714\n",
      "Epoch 301/1000, Tr Loss: 0.0130, Tr Acc: 99.6232, Val Loss: 0.2553, Val Acc: 90.2072\n",
      "Epoch 311/1000, Tr Loss: 0.0084, Tr Acc: 99.7174, Val Loss: 0.2127, Val Acc: 91.8079\n",
      "Epoch 321/1000, Tr Loss: 0.0105, Tr Acc: 99.7645, Val Loss: 0.1768, Val Acc: 92.5612\n",
      "Epoch 331/1000, Tr Loss: 0.0098, Tr Acc: 99.8587, Val Loss: 0.4327, Val Acc: 85.9699\n",
      "Epoch 341/1000, Tr Loss: 0.0083, Tr Acc: 99.8587, Val Loss: 0.1188, Val Acc: 94.9153\n",
      "Epoch 351/1000, Tr Loss: 0.0107, Tr Acc: 99.7645, Val Loss: 1.4270, Val Acc: 75.0471\n",
      "Epoch 361/1000, Tr Loss: 0.0126, Tr Acc: 99.7645, Val Loss: 0.4257, Val Acc: 87.7589\n",
      "Epoch 371/1000, Tr Loss: 0.0119, Tr Acc: 99.6703, Val Loss: 0.3816, Val Acc: 88.3239\n",
      "Epoch 381/1000, Tr Loss: 0.0097, Tr Acc: 99.7645, Val Loss: 0.3718, Val Acc: 86.3465\n",
      "Epoch 391/1000, Tr Loss: 0.0100, Tr Acc: 99.7645, Val Loss: 0.6969, Val Acc: 83.5217\n",
      "Epoch 401/1000, Tr Loss: 0.0111, Tr Acc: 99.7645, Val Loss: 0.2086, Val Acc: 91.2429\n",
      "Epoch 411/1000, Tr Loss: 0.0111, Tr Acc: 99.7174, Val Loss: 0.3017, Val Acc: 89.0772\n",
      "Epoch 421/1000, Tr Loss: 0.0102, Tr Acc: 99.8116, Val Loss: 0.0733, Val Acc: 96.4218\n",
      "Epoch 431/1000, Tr Loss: 0.0119, Tr Acc: 99.8116, Val Loss: 3.5522, Val Acc: 65.4426\n",
      "Epoch 441/1000, Tr Loss: 0.0089, Tr Acc: 99.8116, Val Loss: 0.4555, Val Acc: 86.5348\n",
      "Epoch 451/1000, Tr Loss: 0.0103, Tr Acc: 99.7174, Val Loss: 0.2179, Val Acc: 88.9831\n",
      "Epoch 461/1000, Tr Loss: 0.0068, Tr Acc: 99.8587, Val Loss: 0.1365, Val Acc: 94.1620\n",
      "Epoch 471/1000, Tr Loss: 0.0065, Tr Acc: 99.8587, Val Loss: 0.6563, Val Acc: 83.1450\n",
      "Epoch 481/1000, Tr Loss: 0.0150, Tr Acc: 99.6703, Val Loss: 0.1247, Val Acc: 95.1977\n",
      "Epoch 491/1000, Tr Loss: 0.0065, Tr Acc: 99.9529, Val Loss: 0.1102, Val Acc: 94.8211\n",
      "Epoch 501/1000, Tr Loss: 0.0114, Tr Acc: 99.7174, Val Loss: 1.1349, Val Acc: 78.2486\n",
      "Epoch 511/1000, Tr Loss: 0.0188, Tr Acc: 99.4819, Val Loss: 7.3780, Val Acc: 56.8738\n",
      "Epoch 521/1000, Tr Loss: 0.0126, Tr Acc: 99.6703, Val Loss: 3.2525, Val Acc: 65.8192\n",
      "Epoch 531/1000, Tr Loss: 0.0074, Tr Acc: 99.7645, Val Loss: 0.2154, Val Acc: 90.6780\n",
      "Epoch 541/1000, Tr Loss: 0.0081, Tr Acc: 99.7174, Val Loss: 0.6318, Val Acc: 85.4049\n",
      "Epoch 551/1000, Tr Loss: 0.0071, Tr Acc: 99.9529, Val Loss: 0.0462, Val Acc: 97.9284\n",
      "Epoch 561/1000, Tr Loss: 0.0131, Tr Acc: 99.5290, Val Loss: 0.9447, Val Acc: 81.3559\n",
      "Epoch 571/1000, Tr Loss: 0.0078, Tr Acc: 99.8116, Val Loss: 0.1836, Val Acc: 92.3729\n",
      "Epoch 581/1000, Tr Loss: 0.0136, Tr Acc: 99.7174, Val Loss: 2.8467, Val Acc: 69.4915\n",
      "Epoch 591/1000, Tr Loss: 0.0091, Tr Acc: 99.8587, Val Loss: 0.5945, Val Acc: 84.2750\n",
      "Epoch 601/1000, Tr Loss: 0.0121, Tr Acc: 99.6703, Val Loss: 0.4776, Val Acc: 84.4633\n",
      "Epoch 611/1000, Tr Loss: 0.0096, Tr Acc: 99.7645, Val Loss: 0.3329, Val Acc: 87.8531\n",
      "Epoch 621/1000, Tr Loss: 0.0081, Tr Acc: 99.7645, Val Loss: 4.1193, Val Acc: 60.9228\n",
      "Epoch 631/1000, Tr Loss: 0.0103, Tr Acc: 99.8587, Val Loss: 0.6729, Val Acc: 77.2128\n",
      "Epoch 641/1000, Tr Loss: 0.0088, Tr Acc: 99.6703, Val Loss: 0.3182, Val Acc: 87.4765\n",
      "Epoch 651/1000, Tr Loss: 0.0119, Tr Acc: 99.7174, Val Loss: 1.9065, Val Acc: 67.7966\n",
      "Epoch 661/1000, Tr Loss: 0.0118, Tr Acc: 99.8116, Val Loss: 3.5697, Val Acc: 65.5367\n",
      "Epoch 671/1000, Tr Loss: 0.0065, Tr Acc: 99.8116, Val Loss: 4.2249, Val Acc: 66.3842\n",
      "Epoch 681/1000, Tr Loss: 0.0062, Tr Acc: 99.9529, Val Loss: 0.2599, Val Acc: 89.7363\n",
      "Epoch 691/1000, Tr Loss: 0.0104, Tr Acc: 99.7174, Val Loss: 1.4273, Val Acc: 70.3390\n",
      "Epoch 701/1000, Tr Loss: 0.0085, Tr Acc: 99.8116, Val Loss: 0.4661, Val Acc: 84.5574\n",
      "Epoch 711/1000, Tr Loss: 0.0100, Tr Acc: 99.8116, Val Loss: 0.5788, Val Acc: 85.7815\n",
      "Epoch 721/1000, Tr Loss: 0.0084, Tr Acc: 99.8116, Val Loss: 0.3666, Val Acc: 85.4049\n",
      "Epoch 731/1000, Tr Loss: 0.0113, Tr Acc: 99.6703, Val Loss: 0.8780, Val Acc: 81.1676\n",
      "Epoch 741/1000, Tr Loss: 0.0083, Tr Acc: 99.9058, Val Loss: 0.1183, Val Acc: 94.7269\n",
      "Epoch 751/1000, Tr Loss: 0.0121, Tr Acc: 99.6703, Val Loss: 3.4422, Val Acc: 67.7966\n",
      "Epoch 761/1000, Tr Loss: 0.0079, Tr Acc: 99.7645, Val Loss: 2.1812, Val Acc: 65.7250\n",
      "Epoch 771/1000, Tr Loss: 0.0103, Tr Acc: 99.6232, Val Loss: 4.4255, Val Acc: 63.6535\n",
      "Epoch 781/1000, Tr Loss: 0.0104, Tr Acc: 99.7174, Val Loss: 0.5828, Val Acc: 83.8983\n",
      "Epoch 791/1000, Tr Loss: 0.0138, Tr Acc: 99.7645, Val Loss: 1.0832, Val Acc: 73.8230\n",
      "Epoch 801/1000, Tr Loss: 0.0086, Tr Acc: 99.7645, Val Loss: 0.4420, Val Acc: 84.7458\n",
      "Epoch 811/1000, Tr Loss: 0.0097, Tr Acc: 99.7645, Val Loss: 0.6942, Val Acc: 78.6252\n",
      "Epoch 821/1000, Tr Loss: 0.0135, Tr Acc: 99.3877, Val Loss: 0.4393, Val Acc: 85.5932\n",
      "Epoch 831/1000, Tr Loss: 0.0084, Tr Acc: 99.7645, Val Loss: 3.1119, Val Acc: 64.2185\n",
      "Epoch 841/1000, Tr Loss: 0.0085, Tr Acc: 99.7174, Val Loss: 0.9085, Val Acc: 79.1902\n",
      "Epoch 851/1000, Tr Loss: 0.0100, Tr Acc: 99.9529, Val Loss: 0.3904, Val Acc: 86.0640\n",
      "Epoch 861/1000, Tr Loss: 0.0081, Tr Acc: 99.8116, Val Loss: 0.8117, Val Acc: 79.2844\n",
      "Epoch 871/1000, Tr Loss: 0.0085, Tr Acc: 99.8587, Val Loss: 0.7197, Val Acc: 81.3559\n",
      "Epoch 881/1000, Tr Loss: 0.0186, Tr Acc: 99.5290, Val Loss: 5.3914, Val Acc: 56.7797\n",
      "Epoch 891/1000, Tr Loss: 0.0090, Tr Acc: 99.8116, Val Loss: 0.2642, Val Acc: 90.4896\n",
      "Epoch 901/1000, Tr Loss: 0.0066, Tr Acc: 99.8587, Val Loss: 0.4620, Val Acc: 84.3691\n",
      "Epoch 911/1000, Tr Loss: 0.0079, Tr Acc: 99.7645, Val Loss: 5.3373, Val Acc: 62.1469\n",
      "Epoch 921/1000, Tr Loss: 0.0081, Tr Acc: 99.8116, Val Loss: 0.2182, Val Acc: 89.5480\n",
      "Epoch 931/1000, Tr Loss: 0.0085, Tr Acc: 99.8116, Val Loss: 0.2901, Val Acc: 88.4181\n",
      "Epoch 941/1000, Tr Loss: 0.0087, Tr Acc: 99.8116, Val Loss: 1.2259, Val Acc: 78.1544\n",
      "Epoch 951/1000, Tr Loss: 0.0079, Tr Acc: 99.8116, Val Loss: 0.5598, Val Acc: 83.5217\n",
      "Epoch 961/1000, Tr Loss: 0.0092, Tr Acc: 99.6703, Val Loss: 0.7208, Val Acc: 85.1224\n",
      "Epoch 971/1000, Tr Loss: 0.0090, Tr Acc: 99.7645, Val Loss: 0.6394, Val Acc: 82.7684\n",
      "Epoch 981/1000, Tr Loss: 0.0065, Tr Acc: 99.7645, Val Loss: 0.2520, Val Acc: 90.4896\n",
      "Epoch 991/1000, Tr Loss: 0.0137, Tr Acc: 99.6703, Val Loss: 0.2788, Val Acc: 89.1714\n",
      "Fold 3\n",
      "Epoch 1/1000, Tr Loss: 0.0197, Tr Acc: 99.4821, Val Loss: 0.8727, Val Acc: 79.7361\n",
      "Epoch 11/1000, Tr Loss: 0.0336, Tr Acc: 98.9642, Val Loss: 13.2260, Val Acc: 52.6861\n",
      "Epoch 21/1000, Tr Loss: 0.0222, Tr Acc: 99.0113, Val Loss: 2.6253, Val Acc: 66.5410\n",
      "Epoch 31/1000, Tr Loss: 0.0237, Tr Acc: 99.4350, Val Loss: 0.6167, Val Acc: 80.4901\n",
      "Epoch 41/1000, Tr Loss: 0.0234, Tr Acc: 99.5292, Val Loss: 0.4313, Val Acc: 83.7889\n",
      "Epoch 51/1000, Tr Loss: 0.0238, Tr Acc: 99.4350, Val Loss: 1.0570, Val Acc: 77.8511\n",
      "Epoch 61/1000, Tr Loss: 0.0202, Tr Acc: 99.2938, Val Loss: 8.0437, Val Acc: 55.4194\n",
      "Epoch 71/1000, Tr Loss: 0.0146, Tr Acc: 99.4350, Val Loss: 2.7295, Val Acc: 67.9548\n",
      "Epoch 81/1000, Tr Loss: 0.0177, Tr Acc: 99.3409, Val Loss: 0.1078, Val Acc: 94.8162\n",
      "Epoch 91/1000, Tr Loss: 0.0196, Tr Acc: 99.4821, Val Loss: 0.4161, Val Acc: 85.2026\n",
      "Epoch 101/1000, Tr Loss: 0.0161, Tr Acc: 99.7646, Val Loss: 0.2776, Val Acc: 88.7842\n",
      "Epoch 111/1000, Tr Loss: 0.0104, Tr Acc: 99.7646, Val Loss: 0.7219, Val Acc: 83.0349\n",
      "Epoch 121/1000, Tr Loss: 0.0087, Tr Acc: 99.9058, Val Loss: 0.6210, Val Acc: 83.7889\n",
      "Epoch 131/1000, Tr Loss: 0.0202, Tr Acc: 99.4350, Val Loss: 4.7422, Val Acc: 58.1527\n",
      "Epoch 141/1000, Tr Loss: 0.0138, Tr Acc: 99.5292, Val Loss: 0.2397, Val Acc: 90.3864\n",
      "Epoch 151/1000, Tr Loss: 0.0122, Tr Acc: 99.8117, Val Loss: 3.0918, Val Acc: 62.2055\n",
      "Epoch 161/1000, Tr Loss: 0.0192, Tr Acc: 99.5763, Val Loss: 0.9218, Val Acc: 79.6418\n",
      "Epoch 171/1000, Tr Loss: 0.0135, Tr Acc: 99.5763, Val Loss: 0.1249, Val Acc: 93.9680\n",
      "Epoch 181/1000, Tr Loss: 0.0145, Tr Acc: 99.5763, Val Loss: 0.5968, Val Acc: 83.1291\n",
      "Epoch 191/1000, Tr Loss: 0.0096, Tr Acc: 99.8117, Val Loss: 0.2073, Val Acc: 91.6117\n",
      "Epoch 201/1000, Tr Loss: 0.0153, Tr Acc: 99.5763, Val Loss: 0.3299, Val Acc: 86.6164\n",
      "Epoch 211/1000, Tr Loss: 0.0131, Tr Acc: 99.7646, Val Loss: 0.1627, Val Acc: 93.3082\n",
      "Epoch 221/1000, Tr Loss: 0.0154, Tr Acc: 99.4821, Val Loss: 1.3166, Val Acc: 76.2488\n",
      "Epoch 231/1000, Tr Loss: 0.0126, Tr Acc: 99.7175, Val Loss: 0.5458, Val Acc: 84.1659\n",
      "Epoch 241/1000, Tr Loss: 0.0124, Tr Acc: 99.6704, Val Loss: 3.4548, Val Acc: 65.6927\n",
      "Epoch 251/1000, Tr Loss: 0.0157, Tr Acc: 99.6704, Val Loss: 0.2222, Val Acc: 90.4807\n",
      "Epoch 261/1000, Tr Loss: 0.0128, Tr Acc: 99.6704, Val Loss: 0.2762, Val Acc: 87.8417\n",
      "Epoch 271/1000, Tr Loss: 0.0128, Tr Acc: 99.7646, Val Loss: 0.8876, Val Acc: 81.4326\n",
      "Epoch 281/1000, Tr Loss: 0.0125, Tr Acc: 99.8117, Val Loss: 2.8665, Val Acc: 67.4835\n",
      "Epoch 291/1000, Tr Loss: 0.0174, Tr Acc: 99.6234, Val Loss: 0.7468, Val Acc: 79.1706\n",
      "Epoch 301/1000, Tr Loss: 0.0157, Tr Acc: 99.7175, Val Loss: 0.2760, Val Acc: 89.3497\n",
      "Epoch 311/1000, Tr Loss: 0.0104, Tr Acc: 99.8117, Val Loss: 0.2368, Val Acc: 90.4807\n",
      "Epoch 321/1000, Tr Loss: 0.0095, Tr Acc: 99.8117, Val Loss: 0.3790, Val Acc: 86.5221\n",
      "Epoch 331/1000, Tr Loss: 0.0070, Tr Acc: 99.8117, Val Loss: 2.5664, Val Acc: 69.0858\n",
      "Epoch 341/1000, Tr Loss: 0.0120, Tr Acc: 99.7175, Val Loss: 2.3315, Val Acc: 63.9962\n",
      "Epoch 351/1000, Tr Loss: 0.0139, Tr Acc: 99.5292, Val Loss: 1.2275, Val Acc: 75.5891\n",
      "Epoch 361/1000, Tr Loss: 0.0069, Tr Acc: 99.9058, Val Loss: 0.4798, Val Acc: 82.9406\n",
      "Epoch 371/1000, Tr Loss: 0.0122, Tr Acc: 99.5292, Val Loss: 0.2303, Val Acc: 89.9152\n",
      "Epoch 381/1000, Tr Loss: 0.0132, Tr Acc: 99.8588, Val Loss: 0.1472, Val Acc: 93.3082\n",
      "Epoch 391/1000, Tr Loss: 0.0082, Tr Acc: 99.7646, Val Loss: 0.1960, Val Acc: 91.6117\n",
      "Epoch 401/1000, Tr Loss: 0.0083, Tr Acc: 99.8588, Val Loss: 0.6364, Val Acc: 81.0556\n",
      "Epoch 411/1000, Tr Loss: 0.0102, Tr Acc: 99.8117, Val Loss: 0.2455, Val Acc: 89.4439\n",
      "Epoch 421/1000, Tr Loss: 0.0191, Tr Acc: 99.6234, Val Loss: 1.5550, Val Acc: 73.3270\n",
      "Epoch 431/1000, Tr Loss: 0.0098, Tr Acc: 99.6234, Val Loss: 0.4808, Val Acc: 83.1291\n",
      "Epoch 441/1000, Tr Loss: 0.0127, Tr Acc: 99.7175, Val Loss: 3.4209, Val Acc: 61.2630\n",
      "Epoch 451/1000, Tr Loss: 0.0100, Tr Acc: 99.8117, Val Loss: 0.0943, Val Acc: 96.2300\n",
      "Epoch 461/1000, Tr Loss: 0.0125, Tr Acc: 99.6234, Val Loss: 0.2000, Val Acc: 90.5749\n",
      "Epoch 471/1000, Tr Loss: 0.0172, Tr Acc: 99.4350, Val Loss: 0.8646, Val Acc: 79.7361\n",
      "Epoch 481/1000, Tr Loss: 0.0110, Tr Acc: 99.8117, Val Loss: 2.2685, Val Acc: 69.4628\n",
      "Epoch 491/1000, Tr Loss: 0.0160, Tr Acc: 99.4821, Val Loss: 0.2144, Val Acc: 89.5382\n",
      "Epoch 501/1000, Tr Loss: 0.0094, Tr Acc: 99.7175, Val Loss: 2.2183, Val Acc: 73.5156\n",
      "Epoch 511/1000, Tr Loss: 0.0072, Tr Acc: 99.8588, Val Loss: 0.9879, Val Acc: 77.8511\n",
      "Epoch 521/1000, Tr Loss: 0.0142, Tr Acc: 99.5292, Val Loss: 1.1336, Val Acc: 76.1546\n",
      "Epoch 531/1000, Tr Loss: 0.0116, Tr Acc: 99.7175, Val Loss: 0.2144, Val Acc: 91.0462\n",
      "Epoch 541/1000, Tr Loss: 0.0070, Tr Acc: 99.9058, Val Loss: 0.1542, Val Acc: 93.2139\n",
      "Epoch 551/1000, Tr Loss: 0.0126, Tr Acc: 99.6234, Val Loss: 1.8858, Val Acc: 72.5730\n",
      "Epoch 561/1000, Tr Loss: 0.0104, Tr Acc: 99.7175, Val Loss: 1.4515, Val Acc: 76.7201\n",
      "Epoch 571/1000, Tr Loss: 0.0089, Tr Acc: 99.7646, Val Loss: 0.6396, Val Acc: 82.2809\n",
      "Epoch 581/1000, Tr Loss: 0.0095, Tr Acc: 99.6704, Val Loss: 1.6222, Val Acc: 74.5523\n",
      "Epoch 591/1000, Tr Loss: 0.0173, Tr Acc: 99.5763, Val Loss: 1.2818, Val Acc: 71.7248\n",
      "Epoch 601/1000, Tr Loss: 0.0164, Tr Acc: 99.5292, Val Loss: 1.5702, Val Acc: 72.1960\n",
      "Epoch 611/1000, Tr Loss: 0.0078, Tr Acc: 99.8588, Val Loss: 0.2487, Val Acc: 90.1037\n",
      "Epoch 621/1000, Tr Loss: 0.0069, Tr Acc: 99.8588, Val Loss: 0.0962, Val Acc: 95.2875\n",
      "Epoch 631/1000, Tr Loss: 0.0104, Tr Acc: 99.7646, Val Loss: 1.9456, Val Acc: 71.3478\n",
      "Epoch 641/1000, Tr Loss: 0.0098, Tr Acc: 99.6704, Val Loss: 1.2991, Val Acc: 72.3845\n",
      "Epoch 651/1000, Tr Loss: 0.0113, Tr Acc: 99.6234, Val Loss: 0.7648, Val Acc: 80.1131\n",
      "Epoch 661/1000, Tr Loss: 0.0074, Tr Acc: 99.7646, Val Loss: 1.1130, Val Acc: 77.1913\n",
      "Epoch 671/1000, Tr Loss: 0.0078, Tr Acc: 99.7646, Val Loss: 0.4639, Val Acc: 84.5429\n",
      "Epoch 681/1000, Tr Loss: 0.0111, Tr Acc: 99.6704, Val Loss: 4.2429, Val Acc: 62.4882\n",
      "Epoch 691/1000, Tr Loss: 0.0084, Tr Acc: 99.8117, Val Loss: 0.6811, Val Acc: 81.3384\n",
      "Epoch 701/1000, Tr Loss: 0.0131, Tr Acc: 99.5763, Val Loss: 0.2637, Val Acc: 90.1979\n",
      "Epoch 711/1000, Tr Loss: 0.0100, Tr Acc: 99.7175, Val Loss: 0.5591, Val Acc: 82.5636\n",
      "Epoch 721/1000, Tr Loss: 0.0074, Tr Acc: 99.8588, Val Loss: 0.2858, Val Acc: 88.7842\n",
      "Epoch 731/1000, Tr Loss: 0.0115, Tr Acc: 99.7175, Val Loss: 0.8422, Val Acc: 81.2441\n",
      "Epoch 741/1000, Tr Loss: 0.0106, Tr Acc: 99.6704, Val Loss: 0.7709, Val Acc: 79.8303\n",
      "Epoch 751/1000, Tr Loss: 0.0095, Tr Acc: 99.7646, Val Loss: 0.1518, Val Acc: 93.4967\n",
      "Epoch 761/1000, Tr Loss: 0.0070, Tr Acc: 99.8588, Val Loss: 0.8978, Val Acc: 76.7201\n",
      "Epoch 771/1000, Tr Loss: 0.0111, Tr Acc: 99.8117, Val Loss: 0.1035, Val Acc: 95.0990\n",
      "Epoch 781/1000, Tr Loss: 0.0179, Tr Acc: 99.5763, Val Loss: 2.1175, Val Acc: 69.7455\n",
      "Epoch 791/1000, Tr Loss: 0.0096, Tr Acc: 99.7175, Val Loss: 1.6138, Val Acc: 71.5363\n",
      "Epoch 801/1000, Tr Loss: 0.0092, Tr Acc: 99.8117, Val Loss: 3.7350, Val Acc: 64.0905\n",
      "Epoch 811/1000, Tr Loss: 0.0074, Tr Acc: 99.9058, Val Loss: 0.3793, Val Acc: 85.2026\n",
      "Epoch 821/1000, Tr Loss: 0.0062, Tr Acc: 99.8117, Val Loss: 0.8747, Val Acc: 79.8303\n",
      "Epoch 831/1000, Tr Loss: 0.0070, Tr Acc: 99.8588, Val Loss: 0.2303, Val Acc: 89.2554\n",
      "Epoch 841/1000, Tr Loss: 0.0133, Tr Acc: 99.7646, Val Loss: 2.1507, Val Acc: 69.8398\n",
      "Epoch 851/1000, Tr Loss: 0.0080, Tr Acc: 99.9058, Val Loss: 0.1819, Val Acc: 91.6117\n",
      "Epoch 861/1000, Tr Loss: 0.0092, Tr Acc: 99.8588, Val Loss: 0.3529, Val Acc: 84.1659\n",
      "Epoch 871/1000, Tr Loss: 0.0091, Tr Acc: 99.6704, Val Loss: 1.6223, Val Acc: 73.5156\n",
      "Epoch 881/1000, Tr Loss: 0.0113, Tr Acc: 99.8117, Val Loss: 0.5135, Val Acc: 82.5636\n",
      "Epoch 891/1000, Tr Loss: 0.0093, Tr Acc: 99.7175, Val Loss: 1.4550, Val Acc: 71.6305\n",
      "Epoch 901/1000, Tr Loss: 0.0121, Tr Acc: 99.7646, Val Loss: 0.3823, Val Acc: 83.8831\n",
      "Epoch 911/1000, Tr Loss: 0.0086, Tr Acc: 99.7175, Val Loss: 0.3649, Val Acc: 86.3336\n",
      "Epoch 921/1000, Tr Loss: 0.0092, Tr Acc: 99.7646, Val Loss: 3.4231, Val Acc: 63.2422\n",
      "Epoch 931/1000, Tr Loss: 0.0092, Tr Acc: 99.6704, Val Loss: 0.4295, Val Acc: 83.4119\n",
      "Epoch 941/1000, Tr Loss: 0.0072, Tr Acc: 99.8588, Val Loss: 4.8721, Val Acc: 62.2997\n",
      "Epoch 951/1000, Tr Loss: 0.0071, Tr Acc: 99.8588, Val Loss: 3.1733, Val Acc: 62.2055\n",
      "Epoch 961/1000, Tr Loss: 0.0057, Tr Acc: 99.9058, Val Loss: 1.0023, Val Acc: 78.0396\n",
      "Epoch 971/1000, Tr Loss: 0.0069, Tr Acc: 99.8588, Val Loss: 2.8368, Val Acc: 65.9755\n",
      "Epoch 981/1000, Tr Loss: 0.0109, Tr Acc: 99.6704, Val Loss: 0.7313, Val Acc: 80.9614\n",
      "Epoch 991/1000, Tr Loss: 0.0085, Tr Acc: 99.9058, Val Loss: 3.0614, Val Acc: 61.5457\n",
      "Fold 1\n",
      "Epoch 1/1000, Tr Loss: 0.0208, Tr Acc: 99.4348, Val Loss: 2.2584, Val Acc: 66.1017\n",
      "Epoch 11/1000, Tr Loss: 0.0159, Tr Acc: 99.5761, Val Loss: 5.1323, Val Acc: 64.2185\n",
      "Epoch 21/1000, Tr Loss: 0.0190, Tr Acc: 99.3406, Val Loss: 2.2024, Val Acc: 72.5047\n",
      "Epoch 31/1000, Tr Loss: 0.0175, Tr Acc: 99.3877, Val Loss: 0.4095, Val Acc: 85.8757\n",
      "Epoch 41/1000, Tr Loss: 0.0190, Tr Acc: 99.2934, Val Loss: 0.2542, Val Acc: 89.8305\n",
      "Epoch 51/1000, Tr Loss: 0.0102, Tr Acc: 99.7645, Val Loss: 0.4440, Val Acc: 85.8757\n",
      "Epoch 61/1000, Tr Loss: 0.0270, Tr Acc: 99.1521, Val Loss: 1.1549, Val Acc: 76.8362\n",
      "Epoch 71/1000, Tr Loss: 0.0148, Tr Acc: 99.7174, Val Loss: 0.9017, Val Acc: 78.5311\n",
      "Epoch 81/1000, Tr Loss: 0.0202, Tr Acc: 99.4348, Val Loss: 0.2316, Val Acc: 90.7721\n",
      "Epoch 91/1000, Tr Loss: 0.0200, Tr Acc: 99.3406, Val Loss: 6.6076, Val Acc: 58.3804\n",
      "Epoch 101/1000, Tr Loss: 0.0192, Tr Acc: 99.4819, Val Loss: 0.3376, Val Acc: 86.9115\n",
      "Epoch 111/1000, Tr Loss: 0.0152, Tr Acc: 99.3406, Val Loss: 1.9521, Val Acc: 68.4557\n",
      "Epoch 121/1000, Tr Loss: 0.0133, Tr Acc: 99.6232, Val Loss: 2.6160, Val Acc: 65.2542\n",
      "Epoch 131/1000, Tr Loss: 0.0118, Tr Acc: 99.6703, Val Loss: 1.4938, Val Acc: 71.1864\n",
      "Epoch 141/1000, Tr Loss: 0.0155, Tr Acc: 99.5290, Val Loss: 1.5374, Val Acc: 72.6930\n",
      "Epoch 151/1000, Tr Loss: 0.0180, Tr Acc: 99.3877, Val Loss: 1.2309, Val Acc: 72.3164\n",
      "Epoch 161/1000, Tr Loss: 0.0127, Tr Acc: 99.6703, Val Loss: 0.6288, Val Acc: 83.3333\n",
      "Epoch 171/1000, Tr Loss: 0.0092, Tr Acc: 99.9058, Val Loss: 0.2582, Val Acc: 89.0772\n",
      "Epoch 181/1000, Tr Loss: 0.0071, Tr Acc: 99.9058, Val Loss: 0.1135, Val Acc: 94.7269\n",
      "Epoch 191/1000, Tr Loss: 0.0118, Tr Acc: 99.6703, Val Loss: 0.4051, Val Acc: 85.2166\n",
      "Epoch 201/1000, Tr Loss: 0.0124, Tr Acc: 99.6232, Val Loss: 0.3085, Val Acc: 88.7947\n",
      "Epoch 211/1000, Tr Loss: 0.0119, Tr Acc: 99.6703, Val Loss: 3.0393, Val Acc: 65.1601\n",
      "Epoch 221/1000, Tr Loss: 0.0136, Tr Acc: 99.7645, Val Loss: 0.1825, Val Acc: 92.2787\n",
      "Epoch 231/1000, Tr Loss: 0.0162, Tr Acc: 99.5290, Val Loss: 1.6284, Val Acc: 76.1770\n",
      "Epoch 241/1000, Tr Loss: 0.0122, Tr Acc: 99.7174, Val Loss: 0.4529, Val Acc: 85.6874\n",
      "Epoch 251/1000, Tr Loss: 0.0149, Tr Acc: 99.5290, Val Loss: 0.5462, Val Acc: 84.2750\n",
      "Epoch 261/1000, Tr Loss: 0.0157, Tr Acc: 99.4819, Val Loss: 0.6922, Val Acc: 81.4501\n",
      "Epoch 271/1000, Tr Loss: 0.0088, Tr Acc: 99.6703, Val Loss: 0.2413, Val Acc: 90.3955\n",
      "Epoch 281/1000, Tr Loss: 0.0142, Tr Acc: 99.6232, Val Loss: 2.0181, Val Acc: 69.3032\n",
      "Epoch 291/1000, Tr Loss: 0.0114, Tr Acc: 99.6703, Val Loss: 0.7335, Val Acc: 79.6610\n",
      "Epoch 301/1000, Tr Loss: 0.0084, Tr Acc: 99.8116, Val Loss: 0.4149, Val Acc: 87.6648\n",
      "Epoch 311/1000, Tr Loss: 0.0114, Tr Acc: 99.7174, Val Loss: 0.2974, Val Acc: 88.7947\n",
      "Epoch 321/1000, Tr Loss: 0.0112, Tr Acc: 99.6232, Val Loss: 0.6119, Val Acc: 81.2618\n",
      "Epoch 331/1000, Tr Loss: 0.0056, Tr Acc: 99.8587, Val Loss: 0.2291, Val Acc: 90.7721\n",
      "Epoch 341/1000, Tr Loss: 0.0148, Tr Acc: 99.5761, Val Loss: 0.1555, Val Acc: 93.5970\n",
      "Epoch 351/1000, Tr Loss: 0.0092, Tr Acc: 99.8116, Val Loss: 1.3934, Val Acc: 76.4595\n",
      "Epoch 361/1000, Tr Loss: 0.0106, Tr Acc: 99.7645, Val Loss: 0.1256, Val Acc: 94.2561\n",
      "Epoch 371/1000, Tr Loss: 0.0120, Tr Acc: 99.6703, Val Loss: 0.8880, Val Acc: 79.4727\n",
      "Epoch 381/1000, Tr Loss: 0.0076, Tr Acc: 99.8587, Val Loss: 3.2427, Val Acc: 64.3126\n",
      "Epoch 391/1000, Tr Loss: 0.0127, Tr Acc: 99.6232, Val Loss: 0.8927, Val Acc: 80.4143\n",
      "Epoch 401/1000, Tr Loss: 0.0081, Tr Acc: 99.7174, Val Loss: 0.3628, Val Acc: 88.2298\n",
      "Epoch 411/1000, Tr Loss: 0.0050, Tr Acc: 99.9529, Val Loss: 0.6279, Val Acc: 84.0866\n",
      "Epoch 421/1000, Tr Loss: 0.0153, Tr Acc: 99.4819, Val Loss: 0.2405, Val Acc: 90.9605\n",
      "Epoch 431/1000, Tr Loss: 0.0090, Tr Acc: 99.7645, Val Loss: 0.5722, Val Acc: 81.7326\n",
      "Epoch 441/1000, Tr Loss: 0.0136, Tr Acc: 99.5761, Val Loss: 1.6128, Val Acc: 73.4463\n",
      "Epoch 451/1000, Tr Loss: 0.0079, Tr Acc: 99.8116, Val Loss: 0.2086, Val Acc: 90.3013\n",
      "Epoch 461/1000, Tr Loss: 0.0096, Tr Acc: 99.8116, Val Loss: 0.7459, Val Acc: 83.2392\n",
      "Epoch 471/1000, Tr Loss: 0.0073, Tr Acc: 99.7174, Val Loss: 0.2381, Val Acc: 90.0188\n",
      "Epoch 481/1000, Tr Loss: 0.0113, Tr Acc: 99.5761, Val Loss: 0.3698, Val Acc: 88.2298\n",
      "Epoch 491/1000, Tr Loss: 0.0074, Tr Acc: 99.7645, Val Loss: 0.2195, Val Acc: 90.7721\n",
      "Epoch 501/1000, Tr Loss: 0.0084, Tr Acc: 99.9058, Val Loss: 0.3100, Val Acc: 87.8531\n",
      "Epoch 511/1000, Tr Loss: 0.0124, Tr Acc: 99.6703, Val Loss: 0.2214, Val Acc: 89.7363\n",
      "Epoch 521/1000, Tr Loss: 0.0055, Tr Acc: 99.8587, Val Loss: 0.3605, Val Acc: 85.5932\n",
      "Epoch 531/1000, Tr Loss: 0.0096, Tr Acc: 99.8116, Val Loss: 0.9117, Val Acc: 77.7778\n",
      "Epoch 541/1000, Tr Loss: 0.0088, Tr Acc: 99.7174, Val Loss: 1.1311, Val Acc: 77.4953\n",
      "Epoch 551/1000, Tr Loss: 0.0074, Tr Acc: 99.8116, Val Loss: 0.2072, Val Acc: 91.4313\n",
      "Epoch 561/1000, Tr Loss: 0.0137, Tr Acc: 99.7645, Val Loss: 0.3449, Val Acc: 85.7815\n",
      "Epoch 571/1000, Tr Loss: 0.0124, Tr Acc: 99.7174, Val Loss: 2.9298, Val Acc: 63.4652\n",
      "Epoch 581/1000, Tr Loss: 0.0127, Tr Acc: 99.6232, Val Loss: 2.8026, Val Acc: 72.4105\n",
      "Epoch 591/1000, Tr Loss: 0.0110, Tr Acc: 99.6232, Val Loss: 2.2821, Val Acc: 74.1055\n",
      "Epoch 601/1000, Tr Loss: 0.0116, Tr Acc: 99.7174, Val Loss: 1.9091, Val Acc: 68.7382\n",
      "Epoch 611/1000, Tr Loss: 0.0085, Tr Acc: 99.7174, Val Loss: 0.8233, Val Acc: 80.7910\n",
      "Epoch 621/1000, Tr Loss: 0.0148, Tr Acc: 99.5290, Val Loss: 0.5015, Val Acc: 83.3333\n",
      "Epoch 631/1000, Tr Loss: 0.0149, Tr Acc: 99.5290, Val Loss: 0.1744, Val Acc: 91.6196\n",
      "Epoch 641/1000, Tr Loss: 0.0130, Tr Acc: 99.6703, Val Loss: 0.4272, Val Acc: 85.5932\n",
      "Epoch 651/1000, Tr Loss: 0.0108, Tr Acc: 99.6703, Val Loss: 3.9163, Val Acc: 64.4068\n",
      "Epoch 661/1000, Tr Loss: 0.0090, Tr Acc: 99.7645, Val Loss: 0.3483, Val Acc: 86.1582\n",
      "Epoch 671/1000, Tr Loss: 0.0115, Tr Acc: 99.9058, Val Loss: 0.1929, Val Acc: 91.4313\n",
      "Epoch 681/1000, Tr Loss: 0.0064, Tr Acc: 99.9529, Val Loss: 1.2697, Val Acc: 75.4237\n",
      "Epoch 691/1000, Tr Loss: 0.0139, Tr Acc: 99.7174, Val Loss: 0.1533, Val Acc: 92.5612\n",
      "Epoch 701/1000, Tr Loss: 0.0075, Tr Acc: 99.7174, Val Loss: 0.7494, Val Acc: 81.6384\n",
      "Epoch 711/1000, Tr Loss: 0.0095, Tr Acc: 99.7645, Val Loss: 0.6579, Val Acc: 79.5669\n",
      "Epoch 721/1000, Tr Loss: 0.0101, Tr Acc: 99.5290, Val Loss: 1.1083, Val Acc: 76.3653\n",
      "Epoch 731/1000, Tr Loss: 0.0090, Tr Acc: 99.7645, Val Loss: 0.7660, Val Acc: 82.2976\n",
      "Epoch 741/1000, Tr Loss: 0.0058, Tr Acc: 99.9058, Val Loss: 0.3490, Val Acc: 87.0056\n",
      "Epoch 751/1000, Tr Loss: 0.0051, Tr Acc: 99.9529, Val Loss: 0.2220, Val Acc: 90.7721\n",
      "Epoch 761/1000, Tr Loss: 0.0103, Tr Acc: 99.8587, Val Loss: 0.1446, Val Acc: 94.5386\n",
      "Epoch 771/1000, Tr Loss: 0.0080, Tr Acc: 99.8587, Val Loss: 0.2677, Val Acc: 88.9831\n",
      "Epoch 781/1000, Tr Loss: 0.0117, Tr Acc: 99.7645, Val Loss: 0.5671, Val Acc: 82.4859\n",
      "Epoch 791/1000, Tr Loss: 0.0071, Tr Acc: 99.8587, Val Loss: 0.6029, Val Acc: 82.1092\n",
      "Epoch 801/1000, Tr Loss: 0.0080, Tr Acc: 99.7645, Val Loss: 0.1640, Val Acc: 91.2429\n",
      "Epoch 811/1000, Tr Loss: 0.0175, Tr Acc: 99.6232, Val Loss: 2.3294, Val Acc: 72.0339\n",
      "Epoch 821/1000, Tr Loss: 0.0082, Tr Acc: 99.8587, Val Loss: 1.7712, Val Acc: 70.5273\n",
      "Epoch 831/1000, Tr Loss: 0.0104, Tr Acc: 99.6232, Val Loss: 0.1944, Val Acc: 92.5612\n",
      "Epoch 841/1000, Tr Loss: 0.0072, Tr Acc: 99.8587, Val Loss: 1.9372, Val Acc: 71.3748\n",
      "Epoch 851/1000, Tr Loss: 0.0108, Tr Acc: 99.8587, Val Loss: 5.3794, Val Acc: 61.6761\n",
      "Epoch 861/1000, Tr Loss: 0.0109, Tr Acc: 99.7645, Val Loss: 0.3255, Val Acc: 88.4181\n",
      "Epoch 871/1000, Tr Loss: 0.0182, Tr Acc: 99.8116, Val Loss: 0.9801, Val Acc: 80.0377\n",
      "Epoch 881/1000, Tr Loss: 0.0072, Tr Acc: 99.8116, Val Loss: 0.2469, Val Acc: 91.5254\n",
      "Epoch 891/1000, Tr Loss: 0.0079, Tr Acc: 99.7174, Val Loss: 0.2103, Val Acc: 91.0546\n",
      "Epoch 901/1000, Tr Loss: 0.0069, Tr Acc: 99.8116, Val Loss: 2.8264, Val Acc: 65.3484\n",
      "Epoch 911/1000, Tr Loss: 0.0050, Tr Acc: 100.0000, Val Loss: 1.0610, Val Acc: 78.3427\n",
      "Epoch 921/1000, Tr Loss: 0.0098, Tr Acc: 99.8116, Val Loss: 0.2101, Val Acc: 91.0546\n",
      "Epoch 931/1000, Tr Loss: 0.0058, Tr Acc: 99.8116, Val Loss: 0.2621, Val Acc: 89.0772\n",
      "Epoch 941/1000, Tr Loss: 0.0064, Tr Acc: 99.8587, Val Loss: 2.2703, Val Acc: 70.1507\n",
      "Epoch 951/1000, Tr Loss: 0.0087, Tr Acc: 99.6703, Val Loss: 2.4777, Val Acc: 66.7608\n",
      "Epoch 961/1000, Tr Loss: 0.0088, Tr Acc: 99.7174, Val Loss: 5.7912, Val Acc: 60.7345\n",
      "Epoch 971/1000, Tr Loss: 0.0075, Tr Acc: 99.9058, Val Loss: 1.0602, Val Acc: 79.2844\n",
      "Epoch 981/1000, Tr Loss: 0.0079, Tr Acc: 99.7645, Val Loss: 0.4907, Val Acc: 82.4859\n",
      "Epoch 991/1000, Tr Loss: 0.0083, Tr Acc: 99.6703, Val Loss: 1.8241, Val Acc: 73.2580\n",
      "Fold 2\n",
      "Epoch 1/1000, Tr Loss: 0.0154, Tr Acc: 99.5290, Val Loss: 1.9006, Val Acc: 70.9981\n",
      "Epoch 11/1000, Tr Loss: 0.0242, Tr Acc: 99.2934, Val Loss: 1.0037, Val Acc: 76.8362\n",
      "Epoch 21/1000, Tr Loss: 0.0158, Tr Acc: 99.5290, Val Loss: 5.0097, Val Acc: 60.4520\n",
      "Epoch 31/1000, Tr Loss: 0.0230, Tr Acc: 99.3406, Val Loss: 0.3600, Val Acc: 88.3239\n",
      "Epoch 41/1000, Tr Loss: 0.0096, Tr Acc: 99.8587, Val Loss: 1.9171, Val Acc: 72.2222\n",
      "Epoch 51/1000, Tr Loss: 0.0210, Tr Acc: 99.5761, Val Loss: 0.3502, Val Acc: 87.7589\n",
      "Epoch 61/1000, Tr Loss: 0.0119, Tr Acc: 99.6703, Val Loss: 0.6242, Val Acc: 81.8267\n",
      "Epoch 71/1000, Tr Loss: 0.0099, Tr Acc: 99.8587, Val Loss: 0.1061, Val Acc: 95.0094\n",
      "Epoch 81/1000, Tr Loss: 0.0186, Tr Acc: 99.5290, Val Loss: 2.7245, Val Acc: 67.0433\n",
      "Epoch 91/1000, Tr Loss: 0.0126, Tr Acc: 99.5761, Val Loss: 0.2041, Val Acc: 91.0546\n",
      "Epoch 101/1000, Tr Loss: 0.0102, Tr Acc: 99.8116, Val Loss: 0.1214, Val Acc: 94.8211\n",
      "Epoch 111/1000, Tr Loss: 0.0090, Tr Acc: 99.8116, Val Loss: 0.3770, Val Acc: 85.4991\n",
      "Epoch 121/1000, Tr Loss: 0.0094, Tr Acc: 99.8116, Val Loss: 0.2081, Val Acc: 91.0546\n",
      "Epoch 131/1000, Tr Loss: 0.0202, Tr Acc: 99.5290, Val Loss: 0.3164, Val Acc: 87.0998\n",
      "Epoch 141/1000, Tr Loss: 0.0139, Tr Acc: 99.7174, Val Loss: 1.4604, Val Acc: 75.1412\n",
      "Epoch 151/1000, Tr Loss: 0.0105, Tr Acc: 99.7645, Val Loss: 0.6977, Val Acc: 78.9077\n",
      "Epoch 161/1000, Tr Loss: 0.0069, Tr Acc: 99.8587, Val Loss: 0.4235, Val Acc: 86.6290\n",
      "Epoch 171/1000, Tr Loss: 0.0101, Tr Acc: 99.6232, Val Loss: 0.1742, Val Acc: 92.2787\n",
      "Epoch 181/1000, Tr Loss: 0.0103, Tr Acc: 99.7174, Val Loss: 2.1904, Val Acc: 76.2712\n",
      "Epoch 191/1000, Tr Loss: 0.0130, Tr Acc: 99.6232, Val Loss: 0.3040, Val Acc: 88.7006\n",
      "Epoch 201/1000, Tr Loss: 0.0181, Tr Acc: 99.6703, Val Loss: 2.3615, Val Acc: 70.9040\n",
      "Epoch 211/1000, Tr Loss: 0.0132, Tr Acc: 99.6232, Val Loss: 2.3702, Val Acc: 68.6441\n",
      "Epoch 221/1000, Tr Loss: 0.0057, Tr Acc: 99.9058, Val Loss: 2.5394, Val Acc: 70.5273\n",
      "Epoch 231/1000, Tr Loss: 0.0133, Tr Acc: 99.6703, Val Loss: 0.6605, Val Acc: 82.9567\n",
      "Epoch 241/1000, Tr Loss: 0.0170, Tr Acc: 99.4819, Val Loss: 0.2176, Val Acc: 90.8663\n",
      "Epoch 251/1000, Tr Loss: 0.0133, Tr Acc: 99.6703, Val Loss: 0.0655, Val Acc: 97.0810\n",
      "Epoch 261/1000, Tr Loss: 0.0140, Tr Acc: 99.6703, Val Loss: 0.5508, Val Acc: 84.7458\n",
      "Epoch 271/1000, Tr Loss: 0.0089, Tr Acc: 99.7645, Val Loss: 0.1329, Val Acc: 94.2561\n",
      "Epoch 281/1000, Tr Loss: 0.0103, Tr Acc: 99.7645, Val Loss: 0.9750, Val Acc: 77.7778\n",
      "Epoch 291/1000, Tr Loss: 0.0160, Tr Acc: 99.7645, Val Loss: 0.3203, Val Acc: 89.6422\n",
      "Epoch 301/1000, Tr Loss: 0.0116, Tr Acc: 99.7174, Val Loss: 1.1691, Val Acc: 76.0829\n",
      "Epoch 311/1000, Tr Loss: 0.0143, Tr Acc: 99.5290, Val Loss: 0.8720, Val Acc: 79.2844\n",
      "Epoch 321/1000, Tr Loss: 0.0077, Tr Acc: 99.7174, Val Loss: 0.1603, Val Acc: 93.2203\n",
      "Epoch 331/1000, Tr Loss: 0.0099, Tr Acc: 99.8116, Val Loss: 0.6528, Val Acc: 83.2392\n",
      "Epoch 341/1000, Tr Loss: 0.0097, Tr Acc: 99.6703, Val Loss: 0.3754, Val Acc: 87.4765\n",
      "Epoch 351/1000, Tr Loss: 0.0082, Tr Acc: 99.8116, Val Loss: 0.3721, Val Acc: 86.5348\n",
      "Epoch 361/1000, Tr Loss: 0.0113, Tr Acc: 99.9058, Val Loss: 0.3939, Val Acc: 87.2881\n",
      "Epoch 371/1000, Tr Loss: 0.0127, Tr Acc: 99.6232, Val Loss: 8.2739, Val Acc: 56.8738\n",
      "Epoch 381/1000, Tr Loss: 0.0134, Tr Acc: 99.7174, Val Loss: 0.9138, Val Acc: 82.9567\n",
      "Epoch 391/1000, Tr Loss: 0.0088, Tr Acc: 99.8587, Val Loss: 0.8761, Val Acc: 79.9435\n",
      "Epoch 401/1000, Tr Loss: 0.0111, Tr Acc: 99.6232, Val Loss: 0.6808, Val Acc: 81.3559\n",
      "Epoch 411/1000, Tr Loss: 0.0108, Tr Acc: 99.7174, Val Loss: 0.1598, Val Acc: 92.5612\n",
      "Epoch 421/1000, Tr Loss: 0.0122, Tr Acc: 99.6703, Val Loss: 1.8739, Val Acc: 74.1996\n",
      "Epoch 431/1000, Tr Loss: 0.0156, Tr Acc: 99.8587, Val Loss: 1.3697, Val Acc: 77.1186\n",
      "Epoch 441/1000, Tr Loss: 0.0113, Tr Acc: 99.6703, Val Loss: 0.1962, Val Acc: 90.8663\n",
      "Epoch 451/1000, Tr Loss: 0.0098, Tr Acc: 99.7645, Val Loss: 0.3195, Val Acc: 88.1356\n",
      "Epoch 461/1000, Tr Loss: 0.0134, Tr Acc: 99.6232, Val Loss: 2.3859, Val Acc: 71.9397\n",
      "Epoch 471/1000, Tr Loss: 0.0090, Tr Acc: 99.7645, Val Loss: 0.9399, Val Acc: 79.2844\n",
      "Epoch 481/1000, Tr Loss: 0.0109, Tr Acc: 99.7174, Val Loss: 0.4584, Val Acc: 84.2750\n",
      "Epoch 491/1000, Tr Loss: 0.0086, Tr Acc: 99.6703, Val Loss: 0.1401, Val Acc: 94.0678\n",
      "Epoch 501/1000, Tr Loss: 0.0167, Tr Acc: 99.5761, Val Loss: 0.3340, Val Acc: 87.1940\n",
      "Epoch 511/1000, Tr Loss: 0.0070, Tr Acc: 99.8587, Val Loss: 0.6979, Val Acc: 81.2618\n",
      "Epoch 521/1000, Tr Loss: 0.0061, Tr Acc: 99.9529, Val Loss: 0.4207, Val Acc: 87.1940\n",
      "Epoch 531/1000, Tr Loss: 0.0057, Tr Acc: 99.9058, Val Loss: 0.5539, Val Acc: 83.8983\n",
      "Epoch 541/1000, Tr Loss: 0.0070, Tr Acc: 99.8116, Val Loss: 0.1623, Val Acc: 93.0320\n",
      "Epoch 551/1000, Tr Loss: 0.0075, Tr Acc: 99.8116, Val Loss: 1.3232, Val Acc: 74.7646\n",
      "Epoch 561/1000, Tr Loss: 0.0046, Tr Acc: 99.9058, Val Loss: 0.1313, Val Acc: 94.2561\n",
      "Epoch 571/1000, Tr Loss: 0.0070, Tr Acc: 99.7645, Val Loss: 0.1955, Val Acc: 91.3371\n",
      "Epoch 581/1000, Tr Loss: 0.0084, Tr Acc: 99.8116, Val Loss: 0.3269, Val Acc: 87.7589\n",
      "Epoch 591/1000, Tr Loss: 0.0100, Tr Acc: 99.8116, Val Loss: 0.1967, Val Acc: 91.1488\n",
      "Epoch 601/1000, Tr Loss: 0.0098, Tr Acc: 99.5761, Val Loss: 3.9226, Val Acc: 62.5235\n",
      "Epoch 611/1000, Tr Loss: 0.0095, Tr Acc: 99.7174, Val Loss: 0.6713, Val Acc: 81.6384\n",
      "Epoch 621/1000, Tr Loss: 0.0125, Tr Acc: 99.7645, Val Loss: 1.8891, Val Acc: 74.0113\n",
      "Epoch 631/1000, Tr Loss: 0.0095, Tr Acc: 99.9058, Val Loss: 0.3697, Val Acc: 86.5348\n",
      "Epoch 641/1000, Tr Loss: 0.0132, Tr Acc: 99.7645, Val Loss: 0.3021, Val Acc: 87.9473\n",
      "Epoch 651/1000, Tr Loss: 0.0068, Tr Acc: 99.7645, Val Loss: 0.2702, Val Acc: 88.4181\n",
      "Epoch 661/1000, Tr Loss: 0.0044, Tr Acc: 99.9529, Val Loss: 0.4427, Val Acc: 85.9699\n",
      "Epoch 671/1000, Tr Loss: 0.0091, Tr Acc: 99.8587, Val Loss: 0.2820, Val Acc: 87.6648\n",
      "Epoch 681/1000, Tr Loss: 0.0163, Tr Acc: 99.4348, Val Loss: 0.9345, Val Acc: 80.9793\n",
      "Epoch 691/1000, Tr Loss: 0.0145, Tr Acc: 99.7174, Val Loss: 5.1410, Val Acc: 60.2637\n",
      "Epoch 701/1000, Tr Loss: 0.0092, Tr Acc: 99.7174, Val Loss: 3.6548, Val Acc: 59.0395\n",
      "Epoch 711/1000, Tr Loss: 0.0082, Tr Acc: 99.7645, Val Loss: 0.5440, Val Acc: 83.5217\n",
      "Epoch 721/1000, Tr Loss: 0.0076, Tr Acc: 99.8116, Val Loss: 0.2778, Val Acc: 88.0414\n",
      "Epoch 731/1000, Tr Loss: 0.0084, Tr Acc: 99.8116, Val Loss: 1.6892, Val Acc: 71.8456\n",
      "Epoch 741/1000, Tr Loss: 0.0101, Tr Acc: 99.7645, Val Loss: 0.9884, Val Acc: 74.7646\n",
      "Epoch 751/1000, Tr Loss: 0.0068, Tr Acc: 99.8116, Val Loss: 0.2351, Val Acc: 90.1130\n",
      "Epoch 761/1000, Tr Loss: 0.0067, Tr Acc: 99.8116, Val Loss: 0.4359, Val Acc: 85.4991\n",
      "Epoch 771/1000, Tr Loss: 0.0093, Tr Acc: 99.7174, Val Loss: 0.3595, Val Acc: 88.1356\n",
      "Epoch 781/1000, Tr Loss: 0.0099, Tr Acc: 99.6232, Val Loss: 1.9589, Val Acc: 73.8230\n",
      "Epoch 791/1000, Tr Loss: 0.0057, Tr Acc: 99.9058, Val Loss: 0.0435, Val Acc: 98.7759\n",
      "Epoch 801/1000, Tr Loss: 0.0053, Tr Acc: 99.9058, Val Loss: 0.3446, Val Acc: 87.6648\n",
      "Epoch 811/1000, Tr Loss: 0.0109, Tr Acc: 99.7174, Val Loss: 0.9527, Val Acc: 79.3785\n",
      "Epoch 821/1000, Tr Loss: 0.0131, Tr Acc: 99.5761, Val Loss: 0.3767, Val Acc: 86.1582\n",
      "Epoch 831/1000, Tr Loss: 0.0119, Tr Acc: 99.8116, Val Loss: 0.8861, Val Acc: 79.8493\n",
      "Epoch 841/1000, Tr Loss: 0.0058, Tr Acc: 99.9058, Val Loss: 1.2433, Val Acc: 76.4595\n",
      "Epoch 851/1000, Tr Loss: 0.0074, Tr Acc: 99.8116, Val Loss: 6.5964, Val Acc: 57.9096\n",
      "Epoch 861/1000, Tr Loss: 0.0078, Tr Acc: 99.6232, Val Loss: 3.2041, Val Acc: 64.0301\n",
      "Epoch 871/1000, Tr Loss: 0.0084, Tr Acc: 99.7645, Val Loss: 1.0818, Val Acc: 78.8136\n",
      "Epoch 881/1000, Tr Loss: 0.0085, Tr Acc: 99.6703, Val Loss: 4.2047, Val Acc: 64.1243\n",
      "Epoch 891/1000, Tr Loss: 0.0058, Tr Acc: 99.9058, Val Loss: 0.9162, Val Acc: 80.9793\n",
      "Epoch 901/1000, Tr Loss: 0.0078, Tr Acc: 99.7645, Val Loss: 0.1648, Val Acc: 92.0904\n",
      "Epoch 911/1000, Tr Loss: 0.0065, Tr Acc: 99.9529, Val Loss: 0.6737, Val Acc: 81.8267\n",
      "Epoch 921/1000, Tr Loss: 0.0067, Tr Acc: 99.8587, Val Loss: 0.9329, Val Acc: 76.3653\n",
      "Epoch 931/1000, Tr Loss: 0.0120, Tr Acc: 99.6703, Val Loss: 0.3100, Val Acc: 88.2298\n",
      "Epoch 941/1000, Tr Loss: 0.0068, Tr Acc: 99.7645, Val Loss: 0.2208, Val Acc: 89.3597\n",
      "Epoch 951/1000, Tr Loss: 0.0073, Tr Acc: 99.8587, Val Loss: 0.7569, Val Acc: 81.9209\n",
      "Epoch 961/1000, Tr Loss: 0.0046, Tr Acc: 99.9058, Val Loss: 1.8312, Val Acc: 73.9171\n",
      "Epoch 971/1000, Tr Loss: 0.0053, Tr Acc: 99.9058, Val Loss: 0.9129, Val Acc: 78.1544\n",
      "Epoch 981/1000, Tr Loss: 0.0084, Tr Acc: 99.8116, Val Loss: 0.2652, Val Acc: 89.9247\n",
      "Epoch 991/1000, Tr Loss: 0.0143, Tr Acc: 99.6232, Val Loss: 0.7369, Val Acc: 80.8851\n",
      "Fold 3\n",
      "Epoch 1/1000, Tr Loss: 0.0304, Tr Acc: 99.0584, Val Loss: 0.7772, Val Acc: 78.6051\n",
      "Epoch 11/1000, Tr Loss: 0.0130, Tr Acc: 99.4350, Val Loss: 3.0381, Val Acc: 62.7710\n",
      "Epoch 21/1000, Tr Loss: 0.0283, Tr Acc: 99.2467, Val Loss: 2.6798, Val Acc: 64.0905\n",
      "Epoch 31/1000, Tr Loss: 0.0236, Tr Acc: 99.5763, Val Loss: 8.2353, Val Acc: 54.0057\n",
      "Epoch 41/1000, Tr Loss: 0.0172, Tr Acc: 99.6234, Val Loss: 0.2175, Val Acc: 91.0462\n",
      "Epoch 51/1000, Tr Loss: 0.0169, Tr Acc: 99.4821, Val Loss: 1.3463, Val Acc: 78.0396\n",
      "Epoch 61/1000, Tr Loss: 0.0130, Tr Acc: 99.8117, Val Loss: 0.4284, Val Acc: 83.8831\n",
      "Epoch 71/1000, Tr Loss: 0.0155, Tr Acc: 99.6704, Val Loss: 0.8171, Val Acc: 77.0971\n",
      "Epoch 81/1000, Tr Loss: 0.0182, Tr Acc: 99.2938, Val Loss: 0.3414, Val Acc: 86.7107\n",
      "Epoch 91/1000, Tr Loss: 0.0248, Tr Acc: 99.4350, Val Loss: 0.9019, Val Acc: 78.8878\n",
      "Epoch 101/1000, Tr Loss: 0.0131, Tr Acc: 99.6234, Val Loss: 1.1620, Val Acc: 77.5683\n",
      "Epoch 111/1000, Tr Loss: 0.0116, Tr Acc: 99.7646, Val Loss: 0.0943, Val Acc: 95.9472\n",
      "Epoch 121/1000, Tr Loss: 0.0140, Tr Acc: 99.7175, Val Loss: 0.7963, Val Acc: 81.8096\n",
      "Epoch 131/1000, Tr Loss: 0.0127, Tr Acc: 99.6234, Val Loss: 1.4456, Val Acc: 76.8143\n",
      "Epoch 141/1000, Tr Loss: 0.0100, Tr Acc: 99.7175, Val Loss: 0.2487, Val Acc: 89.6324\n",
      "Epoch 151/1000, Tr Loss: 0.0113, Tr Acc: 99.7175, Val Loss: 0.4166, Val Acc: 87.5589\n",
      "Epoch 161/1000, Tr Loss: 0.0114, Tr Acc: 99.7175, Val Loss: 0.6179, Val Acc: 84.8256\n",
      "Epoch 171/1000, Tr Loss: 0.0190, Tr Acc: 99.5763, Val Loss: 0.5083, Val Acc: 83.6946\n",
      "Epoch 181/1000, Tr Loss: 0.0132, Tr Acc: 99.7175, Val Loss: 0.1144, Val Acc: 94.7220\n",
      "Epoch 191/1000, Tr Loss: 0.0130, Tr Acc: 99.6704, Val Loss: 0.3885, Val Acc: 86.2394\n",
      "Epoch 201/1000, Tr Loss: 0.0183, Tr Acc: 99.5292, Val Loss: 0.1959, Val Acc: 90.9519\n",
      "Epoch 211/1000, Tr Loss: 0.0151, Tr Acc: 99.5292, Val Loss: 1.4059, Val Acc: 76.1546\n",
      "Epoch 221/1000, Tr Loss: 0.0087, Tr Acc: 99.7646, Val Loss: 0.2644, Val Acc: 89.3497\n",
      "Epoch 231/1000, Tr Loss: 0.0091, Tr Acc: 99.6704, Val Loss: 0.6387, Val Acc: 82.7521\n",
      "Epoch 241/1000, Tr Loss: 0.0128, Tr Acc: 99.7175, Val Loss: 0.5483, Val Acc: 81.5269\n",
      "Epoch 251/1000, Tr Loss: 0.0164, Tr Acc: 99.5763, Val Loss: 1.7242, Val Acc: 72.0075\n",
      "Epoch 261/1000, Tr Loss: 0.0104, Tr Acc: 99.7175, Val Loss: 0.9758, Val Acc: 76.2488\n",
      "Epoch 271/1000, Tr Loss: 0.0189, Tr Acc: 99.5763, Val Loss: 1.4092, Val Acc: 75.8718\n",
      "Epoch 281/1000, Tr Loss: 0.0141, Tr Acc: 99.6234, Val Loss: 1.6141, Val Acc: 74.6466\n",
      "Epoch 291/1000, Tr Loss: 0.0112, Tr Acc: 99.5763, Val Loss: 0.1194, Val Acc: 94.5335\n",
      "Epoch 301/1000, Tr Loss: 0.0101, Tr Acc: 99.7646, Val Loss: 0.1598, Val Acc: 93.6852\n",
      "Epoch 311/1000, Tr Loss: 0.0104, Tr Acc: 99.7175, Val Loss: 1.5051, Val Acc: 72.6673\n",
      "Epoch 321/1000, Tr Loss: 0.0176, Tr Acc: 99.6234, Val Loss: 0.5146, Val Acc: 82.5636\n",
      "Epoch 331/1000, Tr Loss: 0.0126, Tr Acc: 99.6704, Val Loss: 0.1793, Val Acc: 92.8369\n",
      "Epoch 341/1000, Tr Loss: 0.0114, Tr Acc: 99.6704, Val Loss: 3.1439, Val Acc: 65.1272\n",
      "Epoch 351/1000, Tr Loss: 0.0080, Tr Acc: 99.8117, Val Loss: 1.3126, Val Acc: 74.6466\n",
      "Epoch 361/1000, Tr Loss: 0.0122, Tr Acc: 99.3879, Val Loss: 0.6668, Val Acc: 80.3016\n",
      "Epoch 371/1000, Tr Loss: 0.0102, Tr Acc: 99.7646, Val Loss: 0.2447, Val Acc: 89.8209\n",
      "Epoch 381/1000, Tr Loss: 0.0089, Tr Acc: 99.7646, Val Loss: 0.2596, Val Acc: 88.3129\n",
      "Epoch 391/1000, Tr Loss: 0.0107, Tr Acc: 99.6704, Val Loss: 0.1523, Val Acc: 93.4967\n",
      "Epoch 401/1000, Tr Loss: 0.0127, Tr Acc: 99.6704, Val Loss: 0.9483, Val Acc: 78.6051\n",
      "Epoch 411/1000, Tr Loss: 0.0107, Tr Acc: 99.7175, Val Loss: 4.5216, Val Acc: 61.7342\n",
      "Epoch 421/1000, Tr Loss: 0.0108, Tr Acc: 99.8588, Val Loss: 0.1181, Val Acc: 95.0047\n",
      "Epoch 431/1000, Tr Loss: 0.0082, Tr Acc: 99.8117, Val Loss: 0.3493, Val Acc: 87.0877\n",
      "Epoch 441/1000, Tr Loss: 0.0095, Tr Acc: 99.7175, Val Loss: 1.4867, Val Acc: 73.0443\n",
      "Epoch 451/1000, Tr Loss: 0.0128, Tr Acc: 99.7175, Val Loss: 3.6175, Val Acc: 64.3732\n",
      "Epoch 461/1000, Tr Loss: 0.0098, Tr Acc: 99.7175, Val Loss: 0.9500, Val Acc: 76.4373\n",
      "Epoch 471/1000, Tr Loss: 0.0145, Tr Acc: 99.8117, Val Loss: 0.3878, Val Acc: 84.9199\n",
      "Epoch 481/1000, Tr Loss: 0.0064, Tr Acc: 99.8588, Val Loss: 0.6758, Val Acc: 78.7936\n",
      "Epoch 491/1000, Tr Loss: 0.0069, Tr Acc: 99.8117, Val Loss: 1.3782, Val Acc: 72.1960\n",
      "Epoch 501/1000, Tr Loss: 0.0121, Tr Acc: 99.7646, Val Loss: 0.3932, Val Acc: 86.3336\n",
      "Epoch 511/1000, Tr Loss: 0.0137, Tr Acc: 99.7175, Val Loss: 1.9476, Val Acc: 69.1800\n",
      "Epoch 521/1000, Tr Loss: 0.0112, Tr Acc: 99.6704, Val Loss: 0.2527, Val Acc: 90.0094\n",
      "Epoch 531/1000, Tr Loss: 0.0094, Tr Acc: 99.6704, Val Loss: 0.3278, Val Acc: 87.1819\n",
      "Epoch 541/1000, Tr Loss: 0.0081, Tr Acc: 99.8117, Val Loss: 6.8330, Val Acc: 55.9849\n",
      "Epoch 551/1000, Tr Loss: 0.0097, Tr Acc: 99.8117, Val Loss: 0.1170, Val Acc: 94.5335\n",
      "Epoch 561/1000, Tr Loss: 0.0142, Tr Acc: 99.7646, Val Loss: 0.5292, Val Acc: 84.1659\n",
      "Epoch 571/1000, Tr Loss: 0.0083, Tr Acc: 99.7175, Val Loss: 3.7427, Val Acc: 65.1272\n",
      "Epoch 581/1000, Tr Loss: 0.0071, Tr Acc: 99.7646, Val Loss: 0.2270, Val Acc: 90.5749\n",
      "Epoch 591/1000, Tr Loss: 0.0113, Tr Acc: 99.7175, Val Loss: 0.6592, Val Acc: 83.0349\n",
      "Epoch 601/1000, Tr Loss: 0.0090, Tr Acc: 99.8117, Val Loss: 1.1518, Val Acc: 77.0971\n",
      "Epoch 611/1000, Tr Loss: 0.0110, Tr Acc: 99.6704, Val Loss: 0.8697, Val Acc: 79.9246\n",
      "Epoch 621/1000, Tr Loss: 0.0069, Tr Acc: 99.8117, Val Loss: 0.8624, Val Acc: 76.4373\n",
      "Epoch 631/1000, Tr Loss: 0.0105, Tr Acc: 99.7646, Val Loss: 6.0665, Val Acc: 59.0009\n",
      "Epoch 641/1000, Tr Loss: 0.0099, Tr Acc: 99.6704, Val Loss: 0.3429, Val Acc: 87.3704\n",
      "Epoch 651/1000, Tr Loss: 0.0075, Tr Acc: 99.8117, Val Loss: 0.3476, Val Acc: 85.9566\n",
      "Epoch 661/1000, Tr Loss: 0.0096, Tr Acc: 99.7646, Val Loss: 3.3297, Val Acc: 63.1480\n",
      "Epoch 671/1000, Tr Loss: 0.0117, Tr Acc: 99.7175, Val Loss: 0.2875, Val Acc: 87.3704\n",
      "Epoch 681/1000, Tr Loss: 0.0077, Tr Acc: 99.6704, Val Loss: 1.2358, Val Acc: 75.5891\n",
      "Epoch 691/1000, Tr Loss: 0.0068, Tr Acc: 99.8588, Val Loss: 0.2825, Val Acc: 89.1612\n",
      "Epoch 701/1000, Tr Loss: 0.0075, Tr Acc: 99.9058, Val Loss: 0.4512, Val Acc: 84.4486\n",
      "Epoch 711/1000, Tr Loss: 0.0095, Tr Acc: 99.7646, Val Loss: 0.4918, Val Acc: 83.5061\n",
      "Epoch 721/1000, Tr Loss: 0.0052, Tr Acc: 99.9529, Val Loss: 1.1634, Val Acc: 77.2856\n",
      "Epoch 731/1000, Tr Loss: 0.0065, Tr Acc: 99.7646, Val Loss: 0.4964, Val Acc: 82.7521\n",
      "Epoch 741/1000, Tr Loss: 0.0125, Tr Acc: 99.7175, Val Loss: 2.1725, Val Acc: 72.2903\n",
      "Epoch 751/1000, Tr Loss: 0.0054, Tr Acc: 99.9058, Val Loss: 0.1557, Val Acc: 93.4967\n",
      "Epoch 761/1000, Tr Loss: 0.0100, Tr Acc: 99.8117, Val Loss: 1.7038, Val Acc: 72.1018\n",
      "Epoch 771/1000, Tr Loss: 0.0108, Tr Acc: 99.6704, Val Loss: 3.9449, Val Acc: 66.1640\n",
      "Epoch 781/1000, Tr Loss: 0.0094, Tr Acc: 99.6704, Val Loss: 4.4368, Val Acc: 60.3205\n",
      "Epoch 791/1000, Tr Loss: 0.0079, Tr Acc: 99.8117, Val Loss: 1.4705, Val Acc: 71.5363\n",
      "Epoch 801/1000, Tr Loss: 0.0087, Tr Acc: 99.7175, Val Loss: 1.6390, Val Acc: 71.5363\n",
      "Epoch 811/1000, Tr Loss: 0.0092, Tr Acc: 99.6234, Val Loss: 2.2410, Val Acc: 71.1593\n",
      "Epoch 821/1000, Tr Loss: 0.0079, Tr Acc: 99.8117, Val Loss: 1.0359, Val Acc: 77.5683\n",
      "Epoch 831/1000, Tr Loss: 0.0045, Tr Acc: 99.9058, Val Loss: 1.1311, Val Acc: 78.7936\n",
      "Epoch 841/1000, Tr Loss: 0.0090, Tr Acc: 99.7175, Val Loss: 1.7044, Val Acc: 70.9708\n",
      "Epoch 851/1000, Tr Loss: 0.0061, Tr Acc: 99.9058, Val Loss: 0.2680, Val Acc: 89.7267\n",
      "Epoch 861/1000, Tr Loss: 0.0117, Tr Acc: 99.7175, Val Loss: 0.5607, Val Acc: 81.5269\n",
      "Epoch 871/1000, Tr Loss: 0.0100, Tr Acc: 99.7646, Val Loss: 2.0416, Val Acc: 72.2903\n",
      "Epoch 881/1000, Tr Loss: 0.0054, Tr Acc: 99.9529, Val Loss: 0.2183, Val Acc: 91.7059\n",
      "Epoch 891/1000, Tr Loss: 0.0069, Tr Acc: 99.8588, Val Loss: 0.2758, Val Acc: 89.3497\n",
      "Epoch 901/1000, Tr Loss: 0.0086, Tr Acc: 99.7646, Val Loss: 0.1969, Val Acc: 91.3289\n",
      "Epoch 911/1000, Tr Loss: 0.0063, Tr Acc: 99.7646, Val Loss: 3.0524, Val Acc: 60.8860\n",
      "Epoch 921/1000, Tr Loss: 0.0063, Tr Acc: 99.8588, Val Loss: 3.9128, Val Acc: 59.4722\n",
      "Epoch 931/1000, Tr Loss: 0.0047, Tr Acc: 99.9529, Val Loss: 0.7713, Val Acc: 77.5683\n",
      "Epoch 941/1000, Tr Loss: 0.0091, Tr Acc: 99.8117, Val Loss: 2.8687, Val Acc: 67.1065\n",
      "Epoch 951/1000, Tr Loss: 0.0097, Tr Acc: 99.6234, Val Loss: 0.5679, Val Acc: 82.4694\n",
      "Epoch 961/1000, Tr Loss: 0.0100, Tr Acc: 99.6704, Val Loss: 0.5381, Val Acc: 81.7154\n",
      "Epoch 971/1000, Tr Loss: 0.0056, Tr Acc: 99.9058, Val Loss: 1.3361, Val Acc: 75.3063\n",
      "Epoch 981/1000, Tr Loss: 0.0041, Tr Acc: 99.9529, Val Loss: 0.5170, Val Acc: 82.8464\n",
      "Epoch 991/1000, Tr Loss: 0.0116, Tr Acc: 99.9058, Val Loss: 1.1474, Val Acc: 78.0396\n",
      "Fold 1\n",
      "Epoch 1/1000, Tr Loss: 0.0177, Tr Acc: 99.3877, Val Loss: 3.9228, Val Acc: 62.4294\n",
      "Epoch 11/1000, Tr Loss: 0.0168, Tr Acc: 99.2934, Val Loss: 1.5228, Val Acc: 74.0113\n",
      "Epoch 21/1000, Tr Loss: 0.0177, Tr Acc: 99.4348, Val Loss: 0.6868, Val Acc: 80.5085\n",
      "Epoch 31/1000, Tr Loss: 0.0131, Tr Acc: 99.7174, Val Loss: 2.1759, Val Acc: 71.9397\n",
      "Epoch 41/1000, Tr Loss: 0.0142, Tr Acc: 99.6703, Val Loss: 7.0410, Val Acc: 55.2731\n",
      "Epoch 51/1000, Tr Loss: 0.0128, Tr Acc: 99.8116, Val Loss: 1.9255, Val Acc: 77.4011\n",
      "Epoch 61/1000, Tr Loss: 0.0148, Tr Acc: 99.5290, Val Loss: 5.6963, Val Acc: 54.6139\n",
      "Epoch 71/1000, Tr Loss: 0.0142, Tr Acc: 99.7645, Val Loss: 0.0615, Val Acc: 97.7401\n",
      "Epoch 81/1000, Tr Loss: 0.0097, Tr Acc: 99.7174, Val Loss: 0.1157, Val Acc: 95.1036\n",
      "Epoch 91/1000, Tr Loss: 0.0077, Tr Acc: 99.8116, Val Loss: 0.7812, Val Acc: 79.7552\n",
      "Epoch 101/1000, Tr Loss: 0.0127, Tr Acc: 99.5290, Val Loss: 3.2535, Val Acc: 63.1827\n",
      "Epoch 111/1000, Tr Loss: 0.0129, Tr Acc: 99.4348, Val Loss: 0.2995, Val Acc: 88.7006\n",
      "Epoch 121/1000, Tr Loss: 0.0144, Tr Acc: 99.5290, Val Loss: 0.1328, Val Acc: 94.5386\n",
      "Epoch 131/1000, Tr Loss: 0.0093, Tr Acc: 99.7174, Val Loss: 0.6491, Val Acc: 81.4501\n",
      "Epoch 141/1000, Tr Loss: 0.0099, Tr Acc: 99.7174, Val Loss: 2.9561, Val Acc: 68.7382\n",
      "Epoch 151/1000, Tr Loss: 0.0112, Tr Acc: 99.7174, Val Loss: 0.2848, Val Acc: 88.2298\n",
      "Epoch 161/1000, Tr Loss: 0.0094, Tr Acc: 99.8116, Val Loss: 4.6802, Val Acc: 63.2768\n",
      "Epoch 171/1000, Tr Loss: 0.0171, Tr Acc: 99.5290, Val Loss: 2.4884, Val Acc: 67.7966\n",
      "Epoch 181/1000, Tr Loss: 0.0095, Tr Acc: 99.7645, Val Loss: 0.2155, Val Acc: 91.7137\n",
      "Epoch 191/1000, Tr Loss: 0.0089, Tr Acc: 99.7645, Val Loss: 1.7679, Val Acc: 75.0471\n"
     ]
    }
   ],
   "source": [
    "results_per_fold = []\n",
    "from common import train\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch_B in range(100):\n",
    "    print(\"Epoch Num {}\".format(epoch_B))\n",
    "    for fold_idx, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "\n",
    "        print('Fold {}'.format(fold_idx + 1))\n",
    "\n",
    "        train_loader, val_loader = setup_dataflow(X_tensor,y_tensor, train_idx, val_idx)\n",
    "        num_step =math.ceil(len(train_loader.dataset) / batch_size)\n",
    "        config.num_step_per_epoch=num_step\n",
    "        train_loss,valid_loss,train_accuracy,valid_accuracy =train(\n",
    "                                                                model = net,\n",
    "                                                                gpu_num = 0,\n",
    "                                                                train_loader = train_loader,\n",
    "                                                                test_loader = val_loader,\n",
    "                                                                optimizer = optimizer  ,\n",
    "                                                                criterion = criterion ,\n",
    "                                                                wand = wand\n",
    "                                                                     )\n",
    "        results_per_fold.append([train_accuracy, valid_accuracy])\n",
    "wandb.alert(\n",
    "            title='Finish',\n",
    "            text=f'Finishing training',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c830d0a-2606-431c-95c8-e459d7e24b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nutapolt/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from braindecode.preprocessing import (\n",
    "    exponential_moving_standardize,\n",
    "    preprocess,\n",
    "    Preprocessor)\n",
    "from numpy import multiply\n",
    "from braindecode.datasets import MOABBDataset\n",
    "from braindecode.preprocessing import create_windows_from_events\n",
    "import torch\n",
    "from braindecode.util import set_random_seeds\n",
    "\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from braindecode.visualization import plot_confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import os\n",
    "import sys\n",
    "from mne.datasets import eegbci\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from mne.datasets import eegbci\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import wandb\n",
    "import os\n",
    "import neptune.new.integrations.sklearn as npt_utils\n",
    "import neptune.new as neptune\n",
    "\n",
    "\n",
    "\n",
    "os.environ['http_proxy'] = \"http://192.41.170.23:3128\"\n",
    "os.environ['https_proxy'] = \"http://192.41.170.23:3128\"\n",
    "\n",
    "from braindecode.models import ShallowFBCSPNet, Deep4Net ,EEGNetv4,HybridNet,EEGInceptionMI,EEGITNet,ATCNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from utility import EEG_fif\n",
    "from utility import create_dataloader\n",
    "from utility import multi_network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def create_dataloader(X, y, batch_size):\n",
    "    X_tensor = torch.tensor(X).float()\n",
    "    y_tensor = torch.tensor(y).long()\n",
    "    dataset_tensor = TensorDataset(X_tensor, y_tensor)\n",
    "    dl = torch.utils.data.DataLoader(dataset_tensor, batch_size=batch_size, shuffle=True)\n",
    "    return dl\n",
    "\n",
    "def early_stopping(train_loss, validation_loss, min_delta, tolerance):\n",
    "    counter = 0\n",
    "    if (validation_loss - train_loss) > min_delta:\n",
    "        counter +=1\n",
    "        if counter >= tolerance:\n",
    "          return True\n",
    "\n",
    "def extrack_dataset(dataset):\n",
    "    for x, y, window_ind in dataset:\n",
    "        x_shape = x.shape\n",
    "        y_shape = len(dataset.get_metadata().target)\n",
    "        break\n",
    "    X = np.zeros((y_shape,x_shape[0],x_shape[1]))\n",
    "    y_=[]\n",
    "    i=0\n",
    "    for x, y, window_ind in dataset:\n",
    "        X[i]=x\n",
    "        y_.append(y)\n",
    "        i+=1\n",
    "    #X2 = X[:, 7:8, :]\n",
    "    #X3= X[:, 11:12, :]\n",
    "    #(288, 22, 1125)\n",
    "    #X = np.concatenate((X2,X3), axis=1)\n",
    "    print(X.shape)\n",
    "\n",
    "    #X = X.reshape(y_shape,X.shape[2],X.shape[1])\n",
    "    return X,np.array(y_).T\n",
    "\n",
    "\n",
    "def extrack_dataset_2class(dataset):\n",
    "    for x, y, window_ind in dataset:\n",
    "        x_shape = x.shape\n",
    "        y_shape = len(dataset.get_metadata().target)\n",
    "        break\n",
    "    X = np.zeros((y_shape,x_shape[0],x_shape[1]))\n",
    "    y_=[]\n",
    "    i=0\n",
    "    for x, y, window_ind in dataset:\n",
    "        X[i]=x\n",
    "        y_.append(y)\n",
    "        i+=1\n",
    "    y = np.array(y_).T\n",
    "    X_ = np.zeros((int(y_shape/2) ,x_shape[0],x_shape[1])) #for 2 class\n",
    "    y_for_2class =[]\n",
    "    j=0\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == 0 or y[i] == 1:\n",
    "            y_for_2class.append(y[i])\n",
    "            X_[j]=X[i]\n",
    "            j +=1\n",
    "    y_for_2class = np.array(y_for_2class).T\n",
    "    X_ = X_.reshape(y_for_2class.shape[0],X_.shape[2],X_.shape[1])\n",
    "    return X_,y_for_2class\n",
    "\n",
    "def extrack_dataset_2class_cut(dataset):\n",
    "    for x, y, window_ind in dataset:\n",
    "        x_shape = x.shape\n",
    "        y_shape = len(dataset.get_metadata().target)\n",
    "        break\n",
    "    X = np.zeros((y_shape,x_shape[0],x_shape[1]))\n",
    "    y_=[]\n",
    "    i=0\n",
    "    for x, y, window_ind in dataset:\n",
    "        X[i]=x\n",
    "        y_.append(y)\n",
    "        i+=1\n",
    "    y = np.array(y_).T\n",
    "    X_ = np.zeros((int(y_shape/2) ,x_shape[0],x_shape[1])) #for 2 class\n",
    "    y_for_2class =[]\n",
    "    j=0\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == 0 or y[i] == 1:\n",
    "            y_for_2class.append(y[i])\n",
    "            X_[j]=X[i]\n",
    "            j +=1\n",
    "    y_for_2class = np.array(y_for_2class).T\n",
    "    X_ = X_.reshape(y_for_2class.shape[0],X_.shape[2],X_.shape[1])\n",
    "    #100, 1, 576, 22\n",
    "    X_ = X_[:,64:320,:]\n",
    "    print(X_.shape)\n",
    "    return X_,y_for_2class\n",
    "\n",
    "\n",
    "def train(model,gpu_num,train_loader,test_loader,\n",
    "          weights_name=False,\n",
    "          optimizer = None,\n",
    "          criterion = None,\n",
    "          num_epochs=500,\n",
    "          vail_loader= None,\n",
    "          save_weights = False,\n",
    "          neptune = True,\n",
    "          lr = None\n",
    "         ):\n",
    "\n",
    "    # Train the model\n",
    "\n",
    "\n",
    "    train_loss = []\n",
    "    valid_loss = [10,11]\n",
    "    train_accuracy = []\n",
    "    valid_accuracy = []\n",
    "\n",
    "    old_loss = 100\n",
    "    old_acc = 0\n",
    "    valid_loss_vail = []\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        iter_loss = 0.0\n",
    "        correct = 0\n",
    "        iterations = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for i, (items, classes) in enumerate(train_loader):\n",
    "            items = Variable(items)\n",
    "            classes = classes.type(torch.LongTensor)\n",
    "            classes = Variable(classes)\n",
    "\n",
    "            if cuda.is_available():\n",
    "                items = items.cuda(gpu_num)\n",
    "                classes = classes.cuda(gpu_num)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(items)\n",
    "            #avg_pedic = torch.mean(outputs, 1, True).float()\n",
    "            #avg_pedic = avg_pedic.reshape(classes.shape[0])\n",
    "            #print(avg_pedic.shape)\n",
    "            #print(classes.shape)\n",
    "            loss = criterion(outputs, classes)\n",
    "\n",
    "            iter_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            metrics = {\"train/train_loss\": loss}\n",
    "\n",
    "            #print(loss)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == classes.data).sum()\n",
    "            iterations += 1\n",
    "\n",
    "        train_loss.append(iter_loss/iterations)\n",
    "\n",
    "\n",
    "        train_accuracy.append(( correct.float() / len(train_loader.dataset)))\n",
    "        train_metrics = {\"train/train_loss\": iter_loss/iterations,\n",
    "                       \"train/train_accuracy\": (100 * correct.float() / len(train_loader.dataset))}\n",
    "\n",
    "\n",
    "\n",
    "        loss = 0.0\n",
    "        correct = 0\n",
    "        iterations = 0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for i, (items, classes) in enumerate(test_loader):\n",
    "            classes = classes.type(torch.LongTensor)\n",
    "            items = Variable(items)\n",
    "            classes = Variable(classes)\n",
    "\n",
    "            if cuda.is_available():\n",
    "                items = items.cuda(gpu_num)\n",
    "                classes = classes.cuda(gpu_num)\n",
    "\n",
    "\n",
    "            outputs = model(items)\n",
    "            #avg_pedic = torch.mean(outputs, 1, True).float()\n",
    "            #avg_pedic = avg_pedic.reshape(classes.shape[0])\n",
    "\n",
    "            loss += criterion(outputs, classes).item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            correct += (predicted == classes.data).sum()\n",
    "            #print(\"correct : {}\".format(classes.data))\n",
    "            #print(\"predicted : {}\".format(predicted))\n",
    "            iterations += 1\n",
    "\n",
    "        valid_loss.append(loss/iterations)\n",
    "        correct_scalar = np.array([correct.clone().cpu()])[0]\n",
    "        valid_accuracy.append(correct_scalar / len(test_loader.dataset) )\n",
    "\n",
    "        test_metrics = {\"Test/Test_loss\": loss/iterations,\n",
    "                       \"Test/Test_accuracy\": correct_scalar / len(test_loader.dataset) }\n",
    "\n",
    "        if save_weights is True:\n",
    "            if epoch+1 > 2 and valid_loss[-1] < old_loss and old_acc <= valid_accuracy[-1] :\n",
    "                    newpath = r'./{}'.format(weights_name)\n",
    "                    if not os.path.exists(newpath):\n",
    "                        os.makedirs(newpath)\n",
    "                    torch.save(model.state_dict(),'./{}/{:.4f}_{}_{:.4f}_{:.4f}'.format(weights_name,valid_loss[-1],weights_name,valid_loss[-1],valid_accuracy[-1]))\n",
    "                    part_weights = './{}/{:.4f}_{}_{:.4f}_{:.4f}'.format(weights_name,valid_loss[-1],weights_name,valid_loss[-1],valid_accuracy[-1])\n",
    "                    old_loss = valid_loss[-1]\n",
    "                    old_acc = valid_accuracy[-1]\n",
    "\n",
    "        print ('Epoch %d/%d, Tr Loss: %.4f, Tr Acc: %.4f, Val Loss: %.4f, Val Acc: %.4f , le : %f'\n",
    "                       %(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], valid_loss[-1], valid_accuracy[-1],lr))\n",
    "        if early_stopping(train_loss[-1], valid_loss[-1], min_delta=10, tolerance = 20):\n",
    "            print(\"We are at epoch:\", epoch+1)\n",
    "            run[f\"epoch/valid_accuracy\"].append(valid_accuracy[-1])\n",
    "            run[f\"epoch/train_accuracy\"].append(train_accuracy[-1])\n",
    "            run[f\"epoch/train_loss\"].append(train_loss[-1])\n",
    "            run[f\"epoch/valid_loss\"].append(valid_loss[-1])\n",
    "\n",
    "            break\n",
    "        if neptune is True:\n",
    "            run[f\"epoch/valid_accuracy\"].append(valid_accuracy[-1])\n",
    "            run[f\"epoch/train_accuracy\"].append(train_accuracy[-1])\n",
    "            run[f\"epoch/train_loss\"].append(train_loss[-1])\n",
    "            run[f\"epoch/valid_loss\"].append(valid_loss[-1])\n",
    "        if epoch+1 == 1:\n",
    "            stop_loss = valid_loss[-1]\n",
    "\n",
    "        if  (epoch+1)//500 == 0 and epoch+1 > 1400 :\n",
    "            if valid_loss[-1] > valid_loss[-500]:\n",
    "                print(\"Stop\")\n",
    "                break\n",
    "\n",
    "    return train_loss,valid_loss,train_accuracy,valid_accuracy,part_weights\n",
    "def eval(model,\n",
    "         gpu_num,\n",
    "          vail_loader= None,\n",
    "         labels=None,\n",
    "         ):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    evaluate_accuracy = []\n",
    "    correct=0\n",
    "\n",
    "    for i, (items, classes) in enumerate(vail_loader):\n",
    "        classes = classes.type(torch.LongTensor)\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        if cuda.is_available():\n",
    "            items = items.cuda(gpu_num)\n",
    "            classes = classes.cuda(gpu_num)\n",
    "\n",
    "        outputs = model(items)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.append(predicted.clone().cpu().numpy())\n",
    "        y_true.append(classes.data.clone().cpu().numpy())\n",
    "        correct += (predicted == classes.data).sum()\n",
    "    correct_scalar = np.array([correct.clone().cpu()])[0]\n",
    "    evaluate_accuracy.append(correct_scalar / classes.shape[0] )\n",
    "\n",
    "    confusion_mat = confusion_matrix(np.array(y_true).T,np.array(y_pred).T )\n",
    "    run[f\"epoch/eval_ACC\"].append(evaluate_accuracy[-1])\n",
    "    run[\"confusion matrices subject_id : {0}\".format(subject_id)].upload(plot_confusion_matrix(confusion_mat, class_names=labels,rotate_row_labels=0,rotate_col_labels=90,with_f1_score=True))\n",
    "    print(confusion_mat)\n",
    "    return y_pred,y_true,correct_scalar,valid_accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nutapolt/.local/lib/python3.8/site-packages/braindecode/preprocessing/preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
      "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
     ]
    }
   ],
   "source": [
    "subjects = list(range(1,10))\n",
    "\n",
    "low_cut_hz = 8.  # low cut frequency for filtering\n",
    "high_cut_hz = 35.  # high cut frequency for filtering\n",
    "resample = 128\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "# Factor to convert from V to uV\n",
    "factor = 1e6\n",
    "preprocessors = [\n",
    "                    Preprocessor('pick_types', eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
    "                    Preprocessor(lambda data: multiply(data, factor)),  # Convert from V to uV\n",
    "                    Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n",
    "                    #Preprocessor('resample', sfreq=resample),\n",
    "                    Preprocessor(exponential_moving_standardize,  # Exponential movin standardization\n",
    "                                factor_new=factor_new,\n",
    "                                 init_block_size=init_block_size)\n",
    "                    ]\n",
    "n_epochs = 10000\n",
    "lr = 0.0001\n",
    "\n",
    "weight_decay = 0.5 * 0.0001\n",
    "\n",
    "#task_list = ['foot']\n",
    "network_list = ['EEGITNet','ShallowFBCSPNet','Deep4Net']\n",
    "\n",
    "#network_list = [''ATCNet'']\n",
    "#percent_list = [72]\n",
    "percent_list = [18,36,54,72]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eeg_mi\n"
     ]
    }
   ],
   "source": [
    "# home directory + datasets folder\n",
    "#path = '/content/drive/MyDrive/MNE-eegbci-data/files/eegmmidb/'\n",
    "path = 'eeg_mi'\n",
    "base_url = 'https://physionet.org/files/eegmmidb/'\n",
    "#runs = [4, 6, 8, 10, 12,14]\n",
    "#runs = [3,5,7,9,11,13]\n",
    "#runs = [3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "\n",
    "\n",
    "#runs=[3]\n",
    "runs = [3,5,7,9]\n",
    "#runs = [4,6,8,10]\n",
    "#runs = [3,5,7,9,4,6,8,10]\n",
    "subjects = [20]\n",
    "\n",
    "print(path)\n",
    "\n",
    "\n",
    "\n",
    "dataset_name=\"S20\"\n",
    "types_list = ['left','right']\n",
    "load =  True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/AitBrainLab/Synthetic/e/SYNTHET-800\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "eeg_mi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nutapolt/utility.py:628: RuntimeWarning: This filename (eeg_mi/S020/S020R03.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/home/nutapolt/utility.py:628: RuntimeWarning: This filename (eeg_mi/S020/S020R05.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/home/nutapolt/utility.py:628: RuntimeWarning: This filename (eeg_mi/S020/S020R07.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 4 contiguous segments\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nutapolt/utility.py:628: RuntimeWarning: This filename (eeg_mi/S020/S020R09.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/home/nutapolt/utility.py:628: RuntimeWarning: This filename (eeg_mi/S020/S020R03.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/home/nutapolt/utility.py:628: RuntimeWarning: This filename (eeg_mi/S020/S020R05.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/home/nutapolt/utility.py:628: RuntimeWarning: This filename (eeg_mi/S020/S020R07.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/home/nutapolt/utility.py:628: RuntimeWarning: This filename (eeg_mi/S020/S020R09.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 events found\n",
      "Event IDs: [1 2 4]\n",
      "Not setting metadata\n",
      "120 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 120 events and 1751 original time points ...\n",
      "4 bad epochs dropped\n",
      "250\n",
      "train size (106, 8, 1751) (106,)\n",
      "test size (46, 8, 1751) (46,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "/home/nutapolt/utility.py:658: RuntimeWarning: No matching events found for 3 (event id 3)\n",
      "  epochs = mne.Epochs(\n",
      "/home/nutapolt/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:453: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ../aten/src/ATen/native/Convolution.cpp:882.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000, Tr Loss: 0.7072, Tr Acc: 0.5377, Val Loss: 0.6920, Val Acc: 0.5652 , le : 0.000100\n",
      "Epoch 2/10000, Tr Loss: 0.7026, Tr Acc: 0.5189, Val Loss: 0.6921, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 3/10000, Tr Loss: 0.6954, Tr Acc: 0.5189, Val Loss: 0.6921, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 4/10000, Tr Loss: 0.6948, Tr Acc: 0.5283, Val Loss: 0.6921, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 5/10000, Tr Loss: 0.6961, Tr Acc: 0.4623, Val Loss: 0.6921, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 6/10000, Tr Loss: 0.6946, Tr Acc: 0.5472, Val Loss: 0.6920, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 7/10000, Tr Loss: 0.6779, Tr Acc: 0.5849, Val Loss: 0.6919, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 8/10000, Tr Loss: 0.7045, Tr Acc: 0.5000, Val Loss: 0.6918, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 9/10000, Tr Loss: 0.6826, Tr Acc: 0.5566, Val Loss: 0.6917, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 10/10000, Tr Loss: 0.6935, Tr Acc: 0.5094, Val Loss: 0.6916, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 11/10000, Tr Loss: 0.6938, Tr Acc: 0.4811, Val Loss: 0.6914, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 12/10000, Tr Loss: 0.6847, Tr Acc: 0.5849, Val Loss: 0.6912, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 13/10000, Tr Loss: 0.6823, Tr Acc: 0.5849, Val Loss: 0.6911, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 14/10000, Tr Loss: 0.7009, Tr Acc: 0.4811, Val Loss: 0.6909, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 15/10000, Tr Loss: 0.6830, Tr Acc: 0.5283, Val Loss: 0.6908, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 16/10000, Tr Loss: 0.7030, Tr Acc: 0.4717, Val Loss: 0.6906, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 17/10000, Tr Loss: 0.6885, Tr Acc: 0.5000, Val Loss: 0.6904, Val Acc: 0.5435 , le : 0.000100\n",
      "Epoch 18/10000, Tr Loss: 0.6709, Tr Acc: 0.6604, Val Loss: 0.6902, Val Acc: 0.5435 , le : 0.000100\n",
      "Epoch 19/10000, Tr Loss: 0.6937, Tr Acc: 0.5189, Val Loss: 0.6901, Val Acc: 0.5435 , le : 0.000100\n",
      "Epoch 20/10000, Tr Loss: 0.6822, Tr Acc: 0.5472, Val Loss: 0.6899, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 21/10000, Tr Loss: 0.6846, Tr Acc: 0.5849, Val Loss: 0.6898, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 22/10000, Tr Loss: 0.6998, Tr Acc: 0.5094, Val Loss: 0.6897, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 23/10000, Tr Loss: 0.6917, Tr Acc: 0.4811, Val Loss: 0.6896, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 24/10000, Tr Loss: 0.6909, Tr Acc: 0.4811, Val Loss: 0.6895, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 25/10000, Tr Loss: 0.6900, Tr Acc: 0.4717, Val Loss: 0.6894, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 26/10000, Tr Loss: 0.6844, Tr Acc: 0.5377, Val Loss: 0.6894, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 27/10000, Tr Loss: 0.7068, Tr Acc: 0.4717, Val Loss: 0.6893, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 28/10000, Tr Loss: 0.6910, Tr Acc: 0.4340, Val Loss: 0.6893, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 29/10000, Tr Loss: 0.6831, Tr Acc: 0.5094, Val Loss: 0.6893, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 30/10000, Tr Loss: 0.6772, Tr Acc: 0.5472, Val Loss: 0.6893, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 31/10000, Tr Loss: 0.6896, Tr Acc: 0.5000, Val Loss: 0.6893, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 32/10000, Tr Loss: 0.6869, Tr Acc: 0.5000, Val Loss: 0.6893, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 33/10000, Tr Loss: 0.6764, Tr Acc: 0.6132, Val Loss: 0.6894, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 34/10000, Tr Loss: 0.6886, Tr Acc: 0.5472, Val Loss: 0.6894, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 35/10000, Tr Loss: 0.6815, Tr Acc: 0.5094, Val Loss: 0.6894, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 36/10000, Tr Loss: 0.6712, Tr Acc: 0.6509, Val Loss: 0.6895, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 37/10000, Tr Loss: 0.6805, Tr Acc: 0.5849, Val Loss: 0.6895, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 38/10000, Tr Loss: 0.7018, Tr Acc: 0.4623, Val Loss: 0.6895, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 39/10000, Tr Loss: 0.6812, Tr Acc: 0.5094, Val Loss: 0.6895, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 40/10000, Tr Loss: 0.7019, Tr Acc: 0.4811, Val Loss: 0.6895, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 41/10000, Tr Loss: 0.6842, Tr Acc: 0.5660, Val Loss: 0.6894, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 42/10000, Tr Loss: 0.6844, Tr Acc: 0.4906, Val Loss: 0.6894, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 43/10000, Tr Loss: 0.6738, Tr Acc: 0.5472, Val Loss: 0.6893, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 44/10000, Tr Loss: 0.6856, Tr Acc: 0.5472, Val Loss: 0.6892, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 45/10000, Tr Loss: 0.6855, Tr Acc: 0.5377, Val Loss: 0.6891, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 46/10000, Tr Loss: 0.6967, Tr Acc: 0.5000, Val Loss: 0.6890, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 47/10000, Tr Loss: 0.6830, Tr Acc: 0.5189, Val Loss: 0.6889, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 48/10000, Tr Loss: 0.6954, Tr Acc: 0.4717, Val Loss: 0.6888, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 49/10000, Tr Loss: 0.6814, Tr Acc: 0.6038, Val Loss: 0.6887, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 50/10000, Tr Loss: 0.6839, Tr Acc: 0.5189, Val Loss: 0.6887, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 51/10000, Tr Loss: 0.6876, Tr Acc: 0.5283, Val Loss: 0.6887, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 52/10000, Tr Loss: 0.6930, Tr Acc: 0.4623, Val Loss: 0.6886, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 53/10000, Tr Loss: 0.6904, Tr Acc: 0.5189, Val Loss: 0.6886, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 54/10000, Tr Loss: 0.6974, Tr Acc: 0.5094, Val Loss: 0.6886, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 55/10000, Tr Loss: 0.6798, Tr Acc: 0.5472, Val Loss: 0.6886, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 56/10000, Tr Loss: 0.6841, Tr Acc: 0.5755, Val Loss: 0.6887, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 57/10000, Tr Loss: 0.6764, Tr Acc: 0.5189, Val Loss: 0.6887, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 58/10000, Tr Loss: 0.6820, Tr Acc: 0.5283, Val Loss: 0.6889, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 59/10000, Tr Loss: 0.6782, Tr Acc: 0.5094, Val Loss: 0.6889, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 60/10000, Tr Loss: 0.6687, Tr Acc: 0.5943, Val Loss: 0.6890, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 61/10000, Tr Loss: 0.6724, Tr Acc: 0.6132, Val Loss: 0.6891, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 62/10000, Tr Loss: 0.6690, Tr Acc: 0.5189, Val Loss: 0.6892, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 63/10000, Tr Loss: 0.6792, Tr Acc: 0.5377, Val Loss: 0.6893, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 64/10000, Tr Loss: 0.6923, Tr Acc: 0.4717, Val Loss: 0.6895, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 65/10000, Tr Loss: 0.6651, Tr Acc: 0.6321, Val Loss: 0.6896, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 66/10000, Tr Loss: 0.6934, Tr Acc: 0.4717, Val Loss: 0.6897, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 67/10000, Tr Loss: 0.6701, Tr Acc: 0.5566, Val Loss: 0.6897, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 68/10000, Tr Loss: 0.6761, Tr Acc: 0.5283, Val Loss: 0.6898, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 69/10000, Tr Loss: 0.6752, Tr Acc: 0.5000, Val Loss: 0.6898, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 70/10000, Tr Loss: 0.6805, Tr Acc: 0.5000, Val Loss: 0.6898, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 71/10000, Tr Loss: 0.6866, Tr Acc: 0.5660, Val Loss: 0.6898, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 72/10000, Tr Loss: 0.6817, Tr Acc: 0.5660, Val Loss: 0.6898, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 73/10000, Tr Loss: 0.6768, Tr Acc: 0.5000, Val Loss: 0.6898, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 74/10000, Tr Loss: 0.6578, Tr Acc: 0.6226, Val Loss: 0.6897, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 75/10000, Tr Loss: 0.6749, Tr Acc: 0.6509, Val Loss: 0.6897, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 76/10000, Tr Loss: 0.6790, Tr Acc: 0.5377, Val Loss: 0.6896, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 77/10000, Tr Loss: 0.6623, Tr Acc: 0.6321, Val Loss: 0.6896, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 78/10000, Tr Loss: 0.6810, Tr Acc: 0.5472, Val Loss: 0.6895, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 79/10000, Tr Loss: 0.6665, Tr Acc: 0.5943, Val Loss: 0.6894, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 80/10000, Tr Loss: 0.6898, Tr Acc: 0.5755, Val Loss: 0.6894, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 81/10000, Tr Loss: 0.6555, Tr Acc: 0.6415, Val Loss: 0.6893, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 82/10000, Tr Loss: 0.6726, Tr Acc: 0.6038, Val Loss: 0.6892, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 83/10000, Tr Loss: 0.6752, Tr Acc: 0.5283, Val Loss: 0.6892, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 84/10000, Tr Loss: 0.6782, Tr Acc: 0.5943, Val Loss: 0.6891, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 85/10000, Tr Loss: 0.6627, Tr Acc: 0.5849, Val Loss: 0.6891, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 86/10000, Tr Loss: 0.6751, Tr Acc: 0.5660, Val Loss: 0.6891, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 87/10000, Tr Loss: 0.6783, Tr Acc: 0.5377, Val Loss: 0.6890, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 88/10000, Tr Loss: 0.6782, Tr Acc: 0.5377, Val Loss: 0.6890, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 89/10000, Tr Loss: 0.6661, Tr Acc: 0.6321, Val Loss: 0.6889, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 90/10000, Tr Loss: 0.6778, Tr Acc: 0.5755, Val Loss: 0.6888, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 91/10000, Tr Loss: 0.6694, Tr Acc: 0.5566, Val Loss: 0.6888, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 92/10000, Tr Loss: 0.6714, Tr Acc: 0.5660, Val Loss: 0.6887, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 93/10000, Tr Loss: 0.6688, Tr Acc: 0.5189, Val Loss: 0.6887, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 94/10000, Tr Loss: 0.6821, Tr Acc: 0.5189, Val Loss: 0.6887, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 95/10000, Tr Loss: 0.6775, Tr Acc: 0.5660, Val Loss: 0.6887, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 96/10000, Tr Loss: 0.6795, Tr Acc: 0.4906, Val Loss: 0.6887, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 97/10000, Tr Loss: 0.6763, Tr Acc: 0.5472, Val Loss: 0.6887, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 98/10000, Tr Loss: 0.6836, Tr Acc: 0.5189, Val Loss: 0.6888, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 99/10000, Tr Loss: 0.6875, Tr Acc: 0.4811, Val Loss: 0.6888, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 100/10000, Tr Loss: 0.6688, Tr Acc: 0.5943, Val Loss: 0.6888, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 101/10000, Tr Loss: 0.6761, Tr Acc: 0.5943, Val Loss: 0.6889, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 102/10000, Tr Loss: 0.6705, Tr Acc: 0.5755, Val Loss: 0.6889, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 103/10000, Tr Loss: 0.6738, Tr Acc: 0.5283, Val Loss: 0.6890, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 104/10000, Tr Loss: 0.6656, Tr Acc: 0.6038, Val Loss: 0.6891, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 105/10000, Tr Loss: 0.6692, Tr Acc: 0.5283, Val Loss: 0.6891, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 106/10000, Tr Loss: 0.6741, Tr Acc: 0.5189, Val Loss: 0.6892, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 107/10000, Tr Loss: 0.6633, Tr Acc: 0.7075, Val Loss: 0.6892, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 108/10000, Tr Loss: 0.6768, Tr Acc: 0.5000, Val Loss: 0.6893, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 109/10000, Tr Loss: 0.6589, Tr Acc: 0.5849, Val Loss: 0.6894, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 110/10000, Tr Loss: 0.6749, Tr Acc: 0.4811, Val Loss: 0.6894, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 111/10000, Tr Loss: 0.6715, Tr Acc: 0.5943, Val Loss: 0.6895, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 112/10000, Tr Loss: 0.6621, Tr Acc: 0.5755, Val Loss: 0.6895, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 113/10000, Tr Loss: 0.6687, Tr Acc: 0.6226, Val Loss: 0.6895, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 114/10000, Tr Loss: 0.6629, Tr Acc: 0.6038, Val Loss: 0.6896, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 115/10000, Tr Loss: 0.6729, Tr Acc: 0.5189, Val Loss: 0.6896, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 116/10000, Tr Loss: 0.6680, Tr Acc: 0.6132, Val Loss: 0.6896, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 117/10000, Tr Loss: 0.6668, Tr Acc: 0.4811, Val Loss: 0.6895, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 118/10000, Tr Loss: 0.6656, Tr Acc: 0.5849, Val Loss: 0.6895, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 119/10000, Tr Loss: 0.6734, Tr Acc: 0.6132, Val Loss: 0.6894, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 120/10000, Tr Loss: 0.6641, Tr Acc: 0.5849, Val Loss: 0.6893, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 121/10000, Tr Loss: 0.6727, Tr Acc: 0.5849, Val Loss: 0.6892, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 122/10000, Tr Loss: 0.6669, Tr Acc: 0.5660, Val Loss: 0.6892, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 123/10000, Tr Loss: 0.6625, Tr Acc: 0.6132, Val Loss: 0.6891, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 124/10000, Tr Loss: 0.6616, Tr Acc: 0.6226, Val Loss: 0.6890, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 125/10000, Tr Loss: 0.6592, Tr Acc: 0.5849, Val Loss: 0.6890, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 126/10000, Tr Loss: 0.6590, Tr Acc: 0.5849, Val Loss: 0.6889, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 127/10000, Tr Loss: 0.6653, Tr Acc: 0.5566, Val Loss: 0.6888, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 128/10000, Tr Loss: 0.6684, Tr Acc: 0.5283, Val Loss: 0.6888, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 129/10000, Tr Loss: 0.6689, Tr Acc: 0.6132, Val Loss: 0.6888, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 130/10000, Tr Loss: 0.6619, Tr Acc: 0.6226, Val Loss: 0.6887, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 131/10000, Tr Loss: 0.6554, Tr Acc: 0.6038, Val Loss: 0.6887, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 132/10000, Tr Loss: 0.6495, Tr Acc: 0.6509, Val Loss: 0.6887, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 133/10000, Tr Loss: 0.6697, Tr Acc: 0.5472, Val Loss: 0.6887, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 134/10000, Tr Loss: 0.6632, Tr Acc: 0.5472, Val Loss: 0.6886, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 135/10000, Tr Loss: 0.6570, Tr Acc: 0.5472, Val Loss: 0.6886, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 136/10000, Tr Loss: 0.6554, Tr Acc: 0.5755, Val Loss: 0.6886, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 137/10000, Tr Loss: 0.6529, Tr Acc: 0.6038, Val Loss: 0.6886, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 138/10000, Tr Loss: 0.6722, Tr Acc: 0.5566, Val Loss: 0.6886, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 139/10000, Tr Loss: 0.6543, Tr Acc: 0.6226, Val Loss: 0.6887, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 140/10000, Tr Loss: 0.6663, Tr Acc: 0.5660, Val Loss: 0.6887, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 141/10000, Tr Loss: 0.6499, Tr Acc: 0.6226, Val Loss: 0.6888, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 142/10000, Tr Loss: 0.6790, Tr Acc: 0.4528, Val Loss: 0.6889, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 143/10000, Tr Loss: 0.6593, Tr Acc: 0.5566, Val Loss: 0.6890, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 144/10000, Tr Loss: 0.6720, Tr Acc: 0.5566, Val Loss: 0.6890, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 145/10000, Tr Loss: 0.6541, Tr Acc: 0.6038, Val Loss: 0.6891, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 146/10000, Tr Loss: 0.6644, Tr Acc: 0.5283, Val Loss: 0.6891, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 147/10000, Tr Loss: 0.6695, Tr Acc: 0.5094, Val Loss: 0.6891, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 148/10000, Tr Loss: 0.6575, Tr Acc: 0.5943, Val Loss: 0.6891, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 149/10000, Tr Loss: 0.6671, Tr Acc: 0.5849, Val Loss: 0.6891, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 150/10000, Tr Loss: 0.6582, Tr Acc: 0.5849, Val Loss: 0.6891, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 151/10000, Tr Loss: 0.6587, Tr Acc: 0.5943, Val Loss: 0.6891, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 152/10000, Tr Loss: 0.6537, Tr Acc: 0.5472, Val Loss: 0.6891, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 153/10000, Tr Loss: 0.6446, Tr Acc: 0.5943, Val Loss: 0.6892, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 154/10000, Tr Loss: 0.6651, Tr Acc: 0.5755, Val Loss: 0.6892, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 155/10000, Tr Loss: 0.6400, Tr Acc: 0.6132, Val Loss: 0.6892, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 156/10000, Tr Loss: 0.6608, Tr Acc: 0.5660, Val Loss: 0.6892, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 157/10000, Tr Loss: 0.6555, Tr Acc: 0.6132, Val Loss: 0.6892, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 158/10000, Tr Loss: 0.6625, Tr Acc: 0.6132, Val Loss: 0.6892, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 159/10000, Tr Loss: 0.6518, Tr Acc: 0.6132, Val Loss: 0.6891, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 160/10000, Tr Loss: 0.6495, Tr Acc: 0.5849, Val Loss: 0.6891, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 161/10000, Tr Loss: 0.6565, Tr Acc: 0.5283, Val Loss: 0.6890, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 162/10000, Tr Loss: 0.6620, Tr Acc: 0.5472, Val Loss: 0.6890, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 163/10000, Tr Loss: 0.6655, Tr Acc: 0.5189, Val Loss: 0.6889, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 164/10000, Tr Loss: 0.6565, Tr Acc: 0.6226, Val Loss: 0.6888, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 165/10000, Tr Loss: 0.6491, Tr Acc: 0.5660, Val Loss: 0.6887, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 166/10000, Tr Loss: 0.6488, Tr Acc: 0.6509, Val Loss: 0.6886, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 167/10000, Tr Loss: 0.6681, Tr Acc: 0.5660, Val Loss: 0.6886, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 168/10000, Tr Loss: 0.6573, Tr Acc: 0.5377, Val Loss: 0.6886, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 169/10000, Tr Loss: 0.6612, Tr Acc: 0.5755, Val Loss: 0.6886, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 170/10000, Tr Loss: 0.6567, Tr Acc: 0.5755, Val Loss: 0.6887, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 171/10000, Tr Loss: 0.6597, Tr Acc: 0.5472, Val Loss: 0.6888, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 172/10000, Tr Loss: 0.6651, Tr Acc: 0.5755, Val Loss: 0.6889, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 173/10000, Tr Loss: 0.6575, Tr Acc: 0.5849, Val Loss: 0.6890, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 174/10000, Tr Loss: 0.6540, Tr Acc: 0.5472, Val Loss: 0.6892, Val Acc: 0.4783 , le : 0.000100\n",
      "Epoch 175/10000, Tr Loss: 0.6615, Tr Acc: 0.5000, Val Loss: 0.6893, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 176/10000, Tr Loss: 0.6611, Tr Acc: 0.5377, Val Loss: 0.6895, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 177/10000, Tr Loss: 0.6441, Tr Acc: 0.6038, Val Loss: 0.6895, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 178/10000, Tr Loss: 0.6448, Tr Acc: 0.6321, Val Loss: 0.6897, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 179/10000, Tr Loss: 0.6627, Tr Acc: 0.5566, Val Loss: 0.6898, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 180/10000, Tr Loss: 0.6560, Tr Acc: 0.6226, Val Loss: 0.6899, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 181/10000, Tr Loss: 0.6503, Tr Acc: 0.6038, Val Loss: 0.6901, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 182/10000, Tr Loss: 0.6427, Tr Acc: 0.6698, Val Loss: 0.6903, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 183/10000, Tr Loss: 0.6379, Tr Acc: 0.6509, Val Loss: 0.6905, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 184/10000, Tr Loss: 0.6613, Tr Acc: 0.5849, Val Loss: 0.6906, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 185/10000, Tr Loss: 0.6604, Tr Acc: 0.6038, Val Loss: 0.6907, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 186/10000, Tr Loss: 0.6607, Tr Acc: 0.5566, Val Loss: 0.6908, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 187/10000, Tr Loss: 0.6492, Tr Acc: 0.5472, Val Loss: 0.6908, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 188/10000, Tr Loss: 0.6568, Tr Acc: 0.5472, Val Loss: 0.6909, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 189/10000, Tr Loss: 0.6412, Tr Acc: 0.6415, Val Loss: 0.6908, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 190/10000, Tr Loss: 0.6628, Tr Acc: 0.5943, Val Loss: 0.6908, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 191/10000, Tr Loss: 0.6572, Tr Acc: 0.5755, Val Loss: 0.6908, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 192/10000, Tr Loss: 0.6425, Tr Acc: 0.6415, Val Loss: 0.6907, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 193/10000, Tr Loss: 0.6509, Tr Acc: 0.5566, Val Loss: 0.6905, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 194/10000, Tr Loss: 0.6480, Tr Acc: 0.6509, Val Loss: 0.6904, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 195/10000, Tr Loss: 0.6481, Tr Acc: 0.5849, Val Loss: 0.6903, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 196/10000, Tr Loss: 0.6621, Tr Acc: 0.5660, Val Loss: 0.6902, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 197/10000, Tr Loss: 0.6697, Tr Acc: 0.5377, Val Loss: 0.6900, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 198/10000, Tr Loss: 0.6490, Tr Acc: 0.5755, Val Loss: 0.6898, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 199/10000, Tr Loss: 0.6533, Tr Acc: 0.5189, Val Loss: 0.6896, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 200/10000, Tr Loss: 0.6498, Tr Acc: 0.5849, Val Loss: 0.6895, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 201/10000, Tr Loss: 0.6503, Tr Acc: 0.5660, Val Loss: 0.6893, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 202/10000, Tr Loss: 0.6534, Tr Acc: 0.5660, Val Loss: 0.6891, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 203/10000, Tr Loss: 0.6558, Tr Acc: 0.5755, Val Loss: 0.6891, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 204/10000, Tr Loss: 0.6398, Tr Acc: 0.6415, Val Loss: 0.6890, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 205/10000, Tr Loss: 0.6512, Tr Acc: 0.5755, Val Loss: 0.6891, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 206/10000, Tr Loss: 0.6399, Tr Acc: 0.6226, Val Loss: 0.6891, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 207/10000, Tr Loss: 0.6495, Tr Acc: 0.6038, Val Loss: 0.6892, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 208/10000, Tr Loss: 0.6531, Tr Acc: 0.5566, Val Loss: 0.6893, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 209/10000, Tr Loss: 0.6535, Tr Acc: 0.5755, Val Loss: 0.6894, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 210/10000, Tr Loss: 0.6630, Tr Acc: 0.5566, Val Loss: 0.6896, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 211/10000, Tr Loss: 0.6396, Tr Acc: 0.6604, Val Loss: 0.6897, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 212/10000, Tr Loss: 0.6422, Tr Acc: 0.6321, Val Loss: 0.6899, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 213/10000, Tr Loss: 0.6511, Tr Acc: 0.5849, Val Loss: 0.6900, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 214/10000, Tr Loss: 0.6575, Tr Acc: 0.5566, Val Loss: 0.6901, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 215/10000, Tr Loss: 0.6422, Tr Acc: 0.6604, Val Loss: 0.6902, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 216/10000, Tr Loss: 0.6483, Tr Acc: 0.6038, Val Loss: 0.6903, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 217/10000, Tr Loss: 0.6517, Tr Acc: 0.6226, Val Loss: 0.6903, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 218/10000, Tr Loss: 0.6455, Tr Acc: 0.6604, Val Loss: 0.6903, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 219/10000, Tr Loss: 0.6415, Tr Acc: 0.7642, Val Loss: 0.6902, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 220/10000, Tr Loss: 0.6499, Tr Acc: 0.5943, Val Loss: 0.6902, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 221/10000, Tr Loss: 0.6334, Tr Acc: 0.5943, Val Loss: 0.6902, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 222/10000, Tr Loss: 0.6476, Tr Acc: 0.6132, Val Loss: 0.6902, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 223/10000, Tr Loss: 0.6444, Tr Acc: 0.6226, Val Loss: 0.6902, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 224/10000, Tr Loss: 0.6493, Tr Acc: 0.5472, Val Loss: 0.6902, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 225/10000, Tr Loss: 0.6576, Tr Acc: 0.6132, Val Loss: 0.6901, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 226/10000, Tr Loss: 0.6470, Tr Acc: 0.5849, Val Loss: 0.6901, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 227/10000, Tr Loss: 0.6406, Tr Acc: 0.6698, Val Loss: 0.6901, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 228/10000, Tr Loss: 0.6544, Tr Acc: 0.5849, Val Loss: 0.6901, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 229/10000, Tr Loss: 0.6402, Tr Acc: 0.6509, Val Loss: 0.6902, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 230/10000, Tr Loss: 0.6508, Tr Acc: 0.5660, Val Loss: 0.6901, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 231/10000, Tr Loss: 0.6421, Tr Acc: 0.5943, Val Loss: 0.6900, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 232/10000, Tr Loss: 0.6430, Tr Acc: 0.5755, Val Loss: 0.6899, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 233/10000, Tr Loss: 0.6426, Tr Acc: 0.6321, Val Loss: 0.6898, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 234/10000, Tr Loss: 0.6548, Tr Acc: 0.5094, Val Loss: 0.6896, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 235/10000, Tr Loss: 0.6422, Tr Acc: 0.6038, Val Loss: 0.6895, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 236/10000, Tr Loss: 0.6306, Tr Acc: 0.6604, Val Loss: 0.6894, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 237/10000, Tr Loss: 0.6523, Tr Acc: 0.5377, Val Loss: 0.6894, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 238/10000, Tr Loss: 0.6310, Tr Acc: 0.6981, Val Loss: 0.6893, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 239/10000, Tr Loss: 0.6367, Tr Acc: 0.6415, Val Loss: 0.6893, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 240/10000, Tr Loss: 0.6414, Tr Acc: 0.6132, Val Loss: 0.6894, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 241/10000, Tr Loss: 0.6348, Tr Acc: 0.7170, Val Loss: 0.6894, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 242/10000, Tr Loss: 0.6440, Tr Acc: 0.5660, Val Loss: 0.6894, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 243/10000, Tr Loss: 0.6361, Tr Acc: 0.6415, Val Loss: 0.6894, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 244/10000, Tr Loss: 0.6451, Tr Acc: 0.6132, Val Loss: 0.6895, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 245/10000, Tr Loss: 0.6605, Tr Acc: 0.5660, Val Loss: 0.6895, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 246/10000, Tr Loss: 0.6502, Tr Acc: 0.5189, Val Loss: 0.6895, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 247/10000, Tr Loss: 0.6404, Tr Acc: 0.5849, Val Loss: 0.6896, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 248/10000, Tr Loss: 0.6371, Tr Acc: 0.5849, Val Loss: 0.6896, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 249/10000, Tr Loss: 0.6397, Tr Acc: 0.6132, Val Loss: 0.6897, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 250/10000, Tr Loss: 0.6438, Tr Acc: 0.5849, Val Loss: 0.6897, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 251/10000, Tr Loss: 0.6375, Tr Acc: 0.5660, Val Loss: 0.6897, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 252/10000, Tr Loss: 0.6273, Tr Acc: 0.6226, Val Loss: 0.6897, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 253/10000, Tr Loss: 0.6403, Tr Acc: 0.6226, Val Loss: 0.6896, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 254/10000, Tr Loss: 0.6403, Tr Acc: 0.6132, Val Loss: 0.6896, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 255/10000, Tr Loss: 0.6404, Tr Acc: 0.6792, Val Loss: 0.6896, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 256/10000, Tr Loss: 0.6289, Tr Acc: 0.7170, Val Loss: 0.6896, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 257/10000, Tr Loss: 0.6260, Tr Acc: 0.6509, Val Loss: 0.6896, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 258/10000, Tr Loss: 0.6274, Tr Acc: 0.6698, Val Loss: 0.6896, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 259/10000, Tr Loss: 0.6355, Tr Acc: 0.6038, Val Loss: 0.6896, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 260/10000, Tr Loss: 0.6354, Tr Acc: 0.6132, Val Loss: 0.6896, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 261/10000, Tr Loss: 0.6511, Tr Acc: 0.5755, Val Loss: 0.6897, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 262/10000, Tr Loss: 0.6455, Tr Acc: 0.5566, Val Loss: 0.6897, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 263/10000, Tr Loss: 0.6430, Tr Acc: 0.6132, Val Loss: 0.6897, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 264/10000, Tr Loss: 0.6289, Tr Acc: 0.6321, Val Loss: 0.6897, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 265/10000, Tr Loss: 0.6332, Tr Acc: 0.5943, Val Loss: 0.6897, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 266/10000, Tr Loss: 0.6277, Tr Acc: 0.6981, Val Loss: 0.6897, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 267/10000, Tr Loss: 0.6449, Tr Acc: 0.5943, Val Loss: 0.6898, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 268/10000, Tr Loss: 0.6347, Tr Acc: 0.6604, Val Loss: 0.6899, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 269/10000, Tr Loss: 0.6489, Tr Acc: 0.5189, Val Loss: 0.6900, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 270/10000, Tr Loss: 0.6378, Tr Acc: 0.6415, Val Loss: 0.6901, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 271/10000, Tr Loss: 0.6380, Tr Acc: 0.6038, Val Loss: 0.6902, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 272/10000, Tr Loss: 0.6381, Tr Acc: 0.5943, Val Loss: 0.6903, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 273/10000, Tr Loss: 0.6577, Tr Acc: 0.5189, Val Loss: 0.6904, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 274/10000, Tr Loss: 0.6509, Tr Acc: 0.5566, Val Loss: 0.6905, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 275/10000, Tr Loss: 0.6423, Tr Acc: 0.5189, Val Loss: 0.6906, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 276/10000, Tr Loss: 0.6266, Tr Acc: 0.6415, Val Loss: 0.6908, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 277/10000, Tr Loss: 0.6251, Tr Acc: 0.6509, Val Loss: 0.6910, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 278/10000, Tr Loss: 0.6441, Tr Acc: 0.6321, Val Loss: 0.6912, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 279/10000, Tr Loss: 0.6197, Tr Acc: 0.7170, Val Loss: 0.6913, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 280/10000, Tr Loss: 0.6380, Tr Acc: 0.6038, Val Loss: 0.6915, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 281/10000, Tr Loss: 0.6438, Tr Acc: 0.6038, Val Loss: 0.6916, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 282/10000, Tr Loss: 0.6448, Tr Acc: 0.5849, Val Loss: 0.6918, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 283/10000, Tr Loss: 0.6425, Tr Acc: 0.5189, Val Loss: 0.6919, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 284/10000, Tr Loss: 0.6346, Tr Acc: 0.6132, Val Loss: 0.6921, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 285/10000, Tr Loss: 0.6484, Tr Acc: 0.4811, Val Loss: 0.6922, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 286/10000, Tr Loss: 0.6442, Tr Acc: 0.6321, Val Loss: 0.6921, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 287/10000, Tr Loss: 0.6436, Tr Acc: 0.6226, Val Loss: 0.6920, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 288/10000, Tr Loss: 0.6476, Tr Acc: 0.5849, Val Loss: 0.6918, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 289/10000, Tr Loss: 0.6300, Tr Acc: 0.6132, Val Loss: 0.6916, Val Acc: 0.4565 , le : 0.000100\n",
      "Epoch 290/10000, Tr Loss: 0.6459, Tr Acc: 0.6038, Val Loss: 0.6912, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 291/10000, Tr Loss: 0.6418, Tr Acc: 0.6132, Val Loss: 0.6910, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 292/10000, Tr Loss: 0.6308, Tr Acc: 0.6226, Val Loss: 0.6907, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 293/10000, Tr Loss: 0.6362, Tr Acc: 0.5943, Val Loss: 0.6903, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 294/10000, Tr Loss: 0.6225, Tr Acc: 0.6698, Val Loss: 0.6900, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 295/10000, Tr Loss: 0.6422, Tr Acc: 0.6226, Val Loss: 0.6897, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 296/10000, Tr Loss: 0.6318, Tr Acc: 0.6415, Val Loss: 0.6895, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 297/10000, Tr Loss: 0.6079, Tr Acc: 0.7264, Val Loss: 0.6893, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 298/10000, Tr Loss: 0.6213, Tr Acc: 0.6415, Val Loss: 0.6892, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 299/10000, Tr Loss: 0.6373, Tr Acc: 0.5660, Val Loss: 0.6891, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 300/10000, Tr Loss: 0.6380, Tr Acc: 0.6132, Val Loss: 0.6891, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 301/10000, Tr Loss: 0.6484, Tr Acc: 0.5472, Val Loss: 0.6891, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 302/10000, Tr Loss: 0.6404, Tr Acc: 0.5943, Val Loss: 0.6892, Val Acc: 0.5217 , le : 0.000100\n",
      "Epoch 303/10000, Tr Loss: 0.6191, Tr Acc: 0.6792, Val Loss: 0.6893, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 304/10000, Tr Loss: 0.6398, Tr Acc: 0.5660, Val Loss: 0.6893, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 305/10000, Tr Loss: 0.6364, Tr Acc: 0.6792, Val Loss: 0.6894, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 306/10000, Tr Loss: 0.6299, Tr Acc: 0.6226, Val Loss: 0.6895, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 307/10000, Tr Loss: 0.6485, Tr Acc: 0.5000, Val Loss: 0.6896, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 308/10000, Tr Loss: 0.6403, Tr Acc: 0.5660, Val Loss: 0.6898, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 309/10000, Tr Loss: 0.6457, Tr Acc: 0.5943, Val Loss: 0.6899, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 310/10000, Tr Loss: 0.6366, Tr Acc: 0.5566, Val Loss: 0.6900, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 311/10000, Tr Loss: 0.6453, Tr Acc: 0.5660, Val Loss: 0.6901, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 312/10000, Tr Loss: 0.6363, Tr Acc: 0.5849, Val Loss: 0.6901, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 313/10000, Tr Loss: 0.6336, Tr Acc: 0.6509, Val Loss: 0.6900, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 314/10000, Tr Loss: 0.6361, Tr Acc: 0.6226, Val Loss: 0.6900, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 315/10000, Tr Loss: 0.6483, Tr Acc: 0.5472, Val Loss: 0.6899, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 316/10000, Tr Loss: 0.6303, Tr Acc: 0.5943, Val Loss: 0.6898, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 317/10000, Tr Loss: 0.6391, Tr Acc: 0.6604, Val Loss: 0.6898, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 318/10000, Tr Loss: 0.6483, Tr Acc: 0.5943, Val Loss: 0.6898, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 319/10000, Tr Loss: 0.6253, Tr Acc: 0.6509, Val Loss: 0.6899, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 320/10000, Tr Loss: 0.6340, Tr Acc: 0.6509, Val Loss: 0.6901, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 321/10000, Tr Loss: 0.6434, Tr Acc: 0.6226, Val Loss: 0.6903, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 322/10000, Tr Loss: 0.6497, Tr Acc: 0.5189, Val Loss: 0.6904, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 323/10000, Tr Loss: 0.6370, Tr Acc: 0.6509, Val Loss: 0.6906, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 324/10000, Tr Loss: 0.6379, Tr Acc: 0.6415, Val Loss: 0.6908, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 325/10000, Tr Loss: 0.6365, Tr Acc: 0.5849, Val Loss: 0.6909, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 326/10000, Tr Loss: 0.6250, Tr Acc: 0.6321, Val Loss: 0.6911, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 327/10000, Tr Loss: 0.6294, Tr Acc: 0.6321, Val Loss: 0.6912, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 328/10000, Tr Loss: 0.6407, Tr Acc: 0.5943, Val Loss: 0.6914, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 329/10000, Tr Loss: 0.6253, Tr Acc: 0.6415, Val Loss: 0.6915, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 330/10000, Tr Loss: 0.6331, Tr Acc: 0.5943, Val Loss: 0.6916, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 331/10000, Tr Loss: 0.6351, Tr Acc: 0.5566, Val Loss: 0.6916, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 332/10000, Tr Loss: 0.6163, Tr Acc: 0.6226, Val Loss: 0.6918, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 333/10000, Tr Loss: 0.6406, Tr Acc: 0.5472, Val Loss: 0.6919, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 334/10000, Tr Loss: 0.6201, Tr Acc: 0.6321, Val Loss: 0.6921, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 335/10000, Tr Loss: 0.6603, Tr Acc: 0.5377, Val Loss: 0.6922, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 336/10000, Tr Loss: 0.6341, Tr Acc: 0.6038, Val Loss: 0.6923, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 337/10000, Tr Loss: 0.6312, Tr Acc: 0.6132, Val Loss: 0.6925, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 338/10000, Tr Loss: 0.6301, Tr Acc: 0.6321, Val Loss: 0.6927, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 339/10000, Tr Loss: 0.6126, Tr Acc: 0.6887, Val Loss: 0.6928, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 340/10000, Tr Loss: 0.6217, Tr Acc: 0.5943, Val Loss: 0.6930, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 341/10000, Tr Loss: 0.6217, Tr Acc: 0.6038, Val Loss: 0.6931, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 342/10000, Tr Loss: 0.6266, Tr Acc: 0.6226, Val Loss: 0.6933, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 343/10000, Tr Loss: 0.6210, Tr Acc: 0.6132, Val Loss: 0.6934, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 344/10000, Tr Loss: 0.6416, Tr Acc: 0.5943, Val Loss: 0.6934, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 345/10000, Tr Loss: 0.6282, Tr Acc: 0.5943, Val Loss: 0.6935, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 346/10000, Tr Loss: 0.6129, Tr Acc: 0.6981, Val Loss: 0.6936, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 347/10000, Tr Loss: 0.6446, Tr Acc: 0.5472, Val Loss: 0.6935, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 348/10000, Tr Loss: 0.6338, Tr Acc: 0.5849, Val Loss: 0.6935, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 349/10000, Tr Loss: 0.6269, Tr Acc: 0.6226, Val Loss: 0.6934, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 350/10000, Tr Loss: 0.6455, Tr Acc: 0.6038, Val Loss: 0.6933, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 351/10000, Tr Loss: 0.6350, Tr Acc: 0.6132, Val Loss: 0.6931, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 352/10000, Tr Loss: 0.6195, Tr Acc: 0.6038, Val Loss: 0.6928, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 353/10000, Tr Loss: 0.6441, Tr Acc: 0.5755, Val Loss: 0.6927, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 354/10000, Tr Loss: 0.6318, Tr Acc: 0.6038, Val Loss: 0.6925, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 355/10000, Tr Loss: 0.6295, Tr Acc: 0.6226, Val Loss: 0.6923, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 356/10000, Tr Loss: 0.6345, Tr Acc: 0.6132, Val Loss: 0.6922, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 357/10000, Tr Loss: 0.6287, Tr Acc: 0.5755, Val Loss: 0.6921, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 358/10000, Tr Loss: 0.6400, Tr Acc: 0.5377, Val Loss: 0.6919, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 359/10000, Tr Loss: 0.6274, Tr Acc: 0.6226, Val Loss: 0.6918, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 360/10000, Tr Loss: 0.6277, Tr Acc: 0.6509, Val Loss: 0.6918, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 361/10000, Tr Loss: 0.6308, Tr Acc: 0.5660, Val Loss: 0.6917, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 362/10000, Tr Loss: 0.6331, Tr Acc: 0.6132, Val Loss: 0.6917, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 363/10000, Tr Loss: 0.6272, Tr Acc: 0.6321, Val Loss: 0.6917, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 364/10000, Tr Loss: 0.6324, Tr Acc: 0.6226, Val Loss: 0.6917, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 365/10000, Tr Loss: 0.6167, Tr Acc: 0.5943, Val Loss: 0.6917, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 366/10000, Tr Loss: 0.6187, Tr Acc: 0.6792, Val Loss: 0.6918, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 367/10000, Tr Loss: 0.6238, Tr Acc: 0.6698, Val Loss: 0.6919, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 368/10000, Tr Loss: 0.6272, Tr Acc: 0.6321, Val Loss: 0.6919, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 369/10000, Tr Loss: 0.6424, Tr Acc: 0.5755, Val Loss: 0.6920, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 370/10000, Tr Loss: 0.6198, Tr Acc: 0.7170, Val Loss: 0.6920, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 371/10000, Tr Loss: 0.6268, Tr Acc: 0.6509, Val Loss: 0.6920, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 372/10000, Tr Loss: 0.6286, Tr Acc: 0.5283, Val Loss: 0.6920, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 373/10000, Tr Loss: 0.6314, Tr Acc: 0.6604, Val Loss: 0.6920, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 374/10000, Tr Loss: 0.6326, Tr Acc: 0.5566, Val Loss: 0.6921, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 375/10000, Tr Loss: 0.6324, Tr Acc: 0.5472, Val Loss: 0.6921, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 376/10000, Tr Loss: 0.6176, Tr Acc: 0.6226, Val Loss: 0.6922, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 377/10000, Tr Loss: 0.6241, Tr Acc: 0.6604, Val Loss: 0.6923, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 378/10000, Tr Loss: 0.6343, Tr Acc: 0.6226, Val Loss: 0.6924, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 379/10000, Tr Loss: 0.6270, Tr Acc: 0.6132, Val Loss: 0.6924, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 380/10000, Tr Loss: 0.6280, Tr Acc: 0.6038, Val Loss: 0.6925, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 381/10000, Tr Loss: 0.6208, Tr Acc: 0.6604, Val Loss: 0.6925, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 382/10000, Tr Loss: 0.6473, Tr Acc: 0.5660, Val Loss: 0.6926, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 383/10000, Tr Loss: 0.6342, Tr Acc: 0.5566, Val Loss: 0.6928, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 384/10000, Tr Loss: 0.6307, Tr Acc: 0.5660, Val Loss: 0.6929, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 385/10000, Tr Loss: 0.6383, Tr Acc: 0.5660, Val Loss: 0.6929, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 386/10000, Tr Loss: 0.6168, Tr Acc: 0.6981, Val Loss: 0.6930, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 387/10000, Tr Loss: 0.6245, Tr Acc: 0.6038, Val Loss: 0.6931, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 388/10000, Tr Loss: 0.6196, Tr Acc: 0.6321, Val Loss: 0.6931, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 389/10000, Tr Loss: 0.6353, Tr Acc: 0.5755, Val Loss: 0.6930, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 390/10000, Tr Loss: 0.6291, Tr Acc: 0.6415, Val Loss: 0.6929, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 391/10000, Tr Loss: 0.6222, Tr Acc: 0.6038, Val Loss: 0.6928, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 392/10000, Tr Loss: 0.6326, Tr Acc: 0.6321, Val Loss: 0.6928, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 393/10000, Tr Loss: 0.6291, Tr Acc: 0.6226, Val Loss: 0.6928, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 394/10000, Tr Loss: 0.6161, Tr Acc: 0.6604, Val Loss: 0.6928, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 395/10000, Tr Loss: 0.6288, Tr Acc: 0.5849, Val Loss: 0.6927, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 396/10000, Tr Loss: 0.6251, Tr Acc: 0.6226, Val Loss: 0.6928, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 397/10000, Tr Loss: 0.6105, Tr Acc: 0.6604, Val Loss: 0.6928, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 398/10000, Tr Loss: 0.6282, Tr Acc: 0.6604, Val Loss: 0.6928, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 399/10000, Tr Loss: 0.6179, Tr Acc: 0.6604, Val Loss: 0.6929, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 400/10000, Tr Loss: 0.6241, Tr Acc: 0.5660, Val Loss: 0.6929, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 401/10000, Tr Loss: 0.6385, Tr Acc: 0.5472, Val Loss: 0.6930, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 402/10000, Tr Loss: 0.6355, Tr Acc: 0.5472, Val Loss: 0.6930, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 403/10000, Tr Loss: 0.6260, Tr Acc: 0.6321, Val Loss: 0.6931, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 404/10000, Tr Loss: 0.6268, Tr Acc: 0.6132, Val Loss: 0.6931, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 405/10000, Tr Loss: 0.6304, Tr Acc: 0.6604, Val Loss: 0.6930, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 406/10000, Tr Loss: 0.6317, Tr Acc: 0.5660, Val Loss: 0.6930, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 407/10000, Tr Loss: 0.6302, Tr Acc: 0.5943, Val Loss: 0.6930, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 408/10000, Tr Loss: 0.6106, Tr Acc: 0.6887, Val Loss: 0.6930, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 409/10000, Tr Loss: 0.6291, Tr Acc: 0.6226, Val Loss: 0.6929, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 410/10000, Tr Loss: 0.6289, Tr Acc: 0.5755, Val Loss: 0.6929, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 411/10000, Tr Loss: 0.6420, Tr Acc: 0.5660, Val Loss: 0.6929, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 412/10000, Tr Loss: 0.6183, Tr Acc: 0.6321, Val Loss: 0.6930, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 413/10000, Tr Loss: 0.6389, Tr Acc: 0.5566, Val Loss: 0.6930, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 414/10000, Tr Loss: 0.6462, Tr Acc: 0.5094, Val Loss: 0.6931, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 415/10000, Tr Loss: 0.6337, Tr Acc: 0.6415, Val Loss: 0.6932, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 416/10000, Tr Loss: 0.6315, Tr Acc: 0.6415, Val Loss: 0.6933, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 417/10000, Tr Loss: 0.6204, Tr Acc: 0.6509, Val Loss: 0.6935, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 418/10000, Tr Loss: 0.6221, Tr Acc: 0.6132, Val Loss: 0.6936, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 419/10000, Tr Loss: 0.6182, Tr Acc: 0.6509, Val Loss: 0.6938, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 420/10000, Tr Loss: 0.6273, Tr Acc: 0.6132, Val Loss: 0.6940, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 421/10000, Tr Loss: 0.6191, Tr Acc: 0.6509, Val Loss: 0.6942, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 422/10000, Tr Loss: 0.6160, Tr Acc: 0.6792, Val Loss: 0.6943, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 423/10000, Tr Loss: 0.6157, Tr Acc: 0.6698, Val Loss: 0.6945, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 424/10000, Tr Loss: 0.6187, Tr Acc: 0.6038, Val Loss: 0.6946, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 425/10000, Tr Loss: 0.6177, Tr Acc: 0.6132, Val Loss: 0.6946, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 426/10000, Tr Loss: 0.6273, Tr Acc: 0.5849, Val Loss: 0.6946, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 427/10000, Tr Loss: 0.6249, Tr Acc: 0.5755, Val Loss: 0.6947, Val Acc: 0.4348 , le : 0.000100\n",
      "Epoch 428/10000, Tr Loss: 0.6108, Tr Acc: 0.6792, Val Loss: 0.6947, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 429/10000, Tr Loss: 0.6222, Tr Acc: 0.6226, Val Loss: 0.6947, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 430/10000, Tr Loss: 0.6232, Tr Acc: 0.6132, Val Loss: 0.6947, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 431/10000, Tr Loss: 0.6291, Tr Acc: 0.5189, Val Loss: 0.6947, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 432/10000, Tr Loss: 0.6165, Tr Acc: 0.6509, Val Loss: 0.6948, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 433/10000, Tr Loss: 0.6142, Tr Acc: 0.6887, Val Loss: 0.6948, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 434/10000, Tr Loss: 0.6206, Tr Acc: 0.6038, Val Loss: 0.6948, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 435/10000, Tr Loss: 0.6178, Tr Acc: 0.6038, Val Loss: 0.6947, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 436/10000, Tr Loss: 0.6318, Tr Acc: 0.5849, Val Loss: 0.6947, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 437/10000, Tr Loss: 0.6181, Tr Acc: 0.6604, Val Loss: 0.6947, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 438/10000, Tr Loss: 0.6316, Tr Acc: 0.5660, Val Loss: 0.6946, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 439/10000, Tr Loss: 0.6180, Tr Acc: 0.5849, Val Loss: 0.6946, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 440/10000, Tr Loss: 0.6141, Tr Acc: 0.6887, Val Loss: 0.6945, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 441/10000, Tr Loss: 0.6074, Tr Acc: 0.6509, Val Loss: 0.6945, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 442/10000, Tr Loss: 0.6178, Tr Acc: 0.6604, Val Loss: 0.6946, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 443/10000, Tr Loss: 0.6099, Tr Acc: 0.6981, Val Loss: 0.6946, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 444/10000, Tr Loss: 0.6154, Tr Acc: 0.6321, Val Loss: 0.6947, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 445/10000, Tr Loss: 0.6101, Tr Acc: 0.7075, Val Loss: 0.6947, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 446/10000, Tr Loss: 0.6314, Tr Acc: 0.5660, Val Loss: 0.6948, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 447/10000, Tr Loss: 0.6121, Tr Acc: 0.6415, Val Loss: 0.6948, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 448/10000, Tr Loss: 0.6143, Tr Acc: 0.6321, Val Loss: 0.6949, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 449/10000, Tr Loss: 0.6288, Tr Acc: 0.6415, Val Loss: 0.6949, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 450/10000, Tr Loss: 0.6211, Tr Acc: 0.6132, Val Loss: 0.6949, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 451/10000, Tr Loss: 0.6143, Tr Acc: 0.6226, Val Loss: 0.6949, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 452/10000, Tr Loss: 0.6093, Tr Acc: 0.6792, Val Loss: 0.6950, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 453/10000, Tr Loss: 0.6122, Tr Acc: 0.6887, Val Loss: 0.6951, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 454/10000, Tr Loss: 0.6317, Tr Acc: 0.6132, Val Loss: 0.6952, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 455/10000, Tr Loss: 0.6240, Tr Acc: 0.6226, Val Loss: 0.6952, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 456/10000, Tr Loss: 0.6087, Tr Acc: 0.7170, Val Loss: 0.6952, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 457/10000, Tr Loss: 0.6296, Tr Acc: 0.5943, Val Loss: 0.6952, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 458/10000, Tr Loss: 0.6232, Tr Acc: 0.6226, Val Loss: 0.6952, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 459/10000, Tr Loss: 0.6175, Tr Acc: 0.6321, Val Loss: 0.6952, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 460/10000, Tr Loss: 0.6223, Tr Acc: 0.6132, Val Loss: 0.6952, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 461/10000, Tr Loss: 0.6228, Tr Acc: 0.5849, Val Loss: 0.6951, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 462/10000, Tr Loss: 0.6220, Tr Acc: 0.5283, Val Loss: 0.6950, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 463/10000, Tr Loss: 0.6176, Tr Acc: 0.6038, Val Loss: 0.6949, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 464/10000, Tr Loss: 0.6243, Tr Acc: 0.6604, Val Loss: 0.6949, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 465/10000, Tr Loss: 0.6198, Tr Acc: 0.6415, Val Loss: 0.6949, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 466/10000, Tr Loss: 0.6136, Tr Acc: 0.6226, Val Loss: 0.6949, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 467/10000, Tr Loss: 0.6163, Tr Acc: 0.6887, Val Loss: 0.6949, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 468/10000, Tr Loss: 0.6207, Tr Acc: 0.6038, Val Loss: 0.6950, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 469/10000, Tr Loss: 0.6304, Tr Acc: 0.6321, Val Loss: 0.6951, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 470/10000, Tr Loss: 0.6123, Tr Acc: 0.6604, Val Loss: 0.6952, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 471/10000, Tr Loss: 0.6163, Tr Acc: 0.6321, Val Loss: 0.6953, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 472/10000, Tr Loss: 0.6104, Tr Acc: 0.6604, Val Loss: 0.6954, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 473/10000, Tr Loss: 0.6166, Tr Acc: 0.6792, Val Loss: 0.6954, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 474/10000, Tr Loss: 0.6240, Tr Acc: 0.5849, Val Loss: 0.6955, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 475/10000, Tr Loss: 0.6081, Tr Acc: 0.6604, Val Loss: 0.6955, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 476/10000, Tr Loss: 0.6220, Tr Acc: 0.5849, Val Loss: 0.6954, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 477/10000, Tr Loss: 0.6189, Tr Acc: 0.6509, Val Loss: 0.6954, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 478/10000, Tr Loss: 0.6141, Tr Acc: 0.6509, Val Loss: 0.6955, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 479/10000, Tr Loss: 0.6240, Tr Acc: 0.5660, Val Loss: 0.6955, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 480/10000, Tr Loss: 0.6341, Tr Acc: 0.5566, Val Loss: 0.6956, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 481/10000, Tr Loss: 0.6213, Tr Acc: 0.6038, Val Loss: 0.6957, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 482/10000, Tr Loss: 0.6136, Tr Acc: 0.6604, Val Loss: 0.6958, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 483/10000, Tr Loss: 0.6120, Tr Acc: 0.6415, Val Loss: 0.6959, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 484/10000, Tr Loss: 0.6120, Tr Acc: 0.6321, Val Loss: 0.6960, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 485/10000, Tr Loss: 0.6014, Tr Acc: 0.6698, Val Loss: 0.6960, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 486/10000, Tr Loss: 0.6197, Tr Acc: 0.6415, Val Loss: 0.6962, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 487/10000, Tr Loss: 0.6063, Tr Acc: 0.6981, Val Loss: 0.6962, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 488/10000, Tr Loss: 0.6281, Tr Acc: 0.5943, Val Loss: 0.6963, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 489/10000, Tr Loss: 0.6010, Tr Acc: 0.7170, Val Loss: 0.6963, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 490/10000, Tr Loss: 0.6158, Tr Acc: 0.6132, Val Loss: 0.6963, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 491/10000, Tr Loss: 0.6262, Tr Acc: 0.5849, Val Loss: 0.6964, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 492/10000, Tr Loss: 0.6262, Tr Acc: 0.6226, Val Loss: 0.6963, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 493/10000, Tr Loss: 0.6111, Tr Acc: 0.6321, Val Loss: 0.6963, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 494/10000, Tr Loss: 0.6139, Tr Acc: 0.6792, Val Loss: 0.6963, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 495/10000, Tr Loss: 0.6077, Tr Acc: 0.6604, Val Loss: 0.6964, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 496/10000, Tr Loss: 0.6223, Tr Acc: 0.6415, Val Loss: 0.6964, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 497/10000, Tr Loss: 0.6124, Tr Acc: 0.6604, Val Loss: 0.6965, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 498/10000, Tr Loss: 0.6270, Tr Acc: 0.6038, Val Loss: 0.6966, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 499/10000, Tr Loss: 0.6151, Tr Acc: 0.5943, Val Loss: 0.6966, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 500/10000, Tr Loss: 0.6172, Tr Acc: 0.6509, Val Loss: 0.6966, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 501/10000, Tr Loss: 0.6224, Tr Acc: 0.5943, Val Loss: 0.6966, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 502/10000, Tr Loss: 0.6252, Tr Acc: 0.5189, Val Loss: 0.6965, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 503/10000, Tr Loss: 0.6143, Tr Acc: 0.6226, Val Loss: 0.6965, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 504/10000, Tr Loss: 0.6171, Tr Acc: 0.6415, Val Loss: 0.6965, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 505/10000, Tr Loss: 0.6206, Tr Acc: 0.5755, Val Loss: 0.6965, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 506/10000, Tr Loss: 0.6127, Tr Acc: 0.6604, Val Loss: 0.6964, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 507/10000, Tr Loss: 0.6180, Tr Acc: 0.6509, Val Loss: 0.6963, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 508/10000, Tr Loss: 0.6245, Tr Acc: 0.5755, Val Loss: 0.6962, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 509/10000, Tr Loss: 0.6073, Tr Acc: 0.6698, Val Loss: 0.6961, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 510/10000, Tr Loss: 0.6280, Tr Acc: 0.5660, Val Loss: 0.6961, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 511/10000, Tr Loss: 0.6011, Tr Acc: 0.6887, Val Loss: 0.6961, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 512/10000, Tr Loss: 0.6281, Tr Acc: 0.5472, Val Loss: 0.6961, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 513/10000, Tr Loss: 0.6304, Tr Acc: 0.5472, Val Loss: 0.6962, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 514/10000, Tr Loss: 0.6069, Tr Acc: 0.6509, Val Loss: 0.6963, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 515/10000, Tr Loss: 0.6138, Tr Acc: 0.5755, Val Loss: 0.6964, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 516/10000, Tr Loss: 0.6119, Tr Acc: 0.6415, Val Loss: 0.6965, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 517/10000, Tr Loss: 0.6075, Tr Acc: 0.6981, Val Loss: 0.6968, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 518/10000, Tr Loss: 0.6202, Tr Acc: 0.5755, Val Loss: 0.6969, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 519/10000, Tr Loss: 0.6108, Tr Acc: 0.6604, Val Loss: 0.6971, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 520/10000, Tr Loss: 0.6181, Tr Acc: 0.6038, Val Loss: 0.6973, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 521/10000, Tr Loss: 0.6235, Tr Acc: 0.5755, Val Loss: 0.6973, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 522/10000, Tr Loss: 0.6167, Tr Acc: 0.6226, Val Loss: 0.6974, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 523/10000, Tr Loss: 0.6162, Tr Acc: 0.6226, Val Loss: 0.6974, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 524/10000, Tr Loss: 0.6380, Tr Acc: 0.4811, Val Loss: 0.6974, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 525/10000, Tr Loss: 0.6235, Tr Acc: 0.6132, Val Loss: 0.6974, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 526/10000, Tr Loss: 0.6294, Tr Acc: 0.5566, Val Loss: 0.6973, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 527/10000, Tr Loss: 0.6198, Tr Acc: 0.6321, Val Loss: 0.6972, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 528/10000, Tr Loss: 0.6205, Tr Acc: 0.6132, Val Loss: 0.6971, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 529/10000, Tr Loss: 0.6120, Tr Acc: 0.6604, Val Loss: 0.6970, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 530/10000, Tr Loss: 0.6231, Tr Acc: 0.6132, Val Loss: 0.6970, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 531/10000, Tr Loss: 0.6133, Tr Acc: 0.5943, Val Loss: 0.6970, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 532/10000, Tr Loss: 0.6093, Tr Acc: 0.6698, Val Loss: 0.6970, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 533/10000, Tr Loss: 0.6298, Tr Acc: 0.5472, Val Loss: 0.6969, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 534/10000, Tr Loss: 0.6166, Tr Acc: 0.5849, Val Loss: 0.6970, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 535/10000, Tr Loss: 0.6113, Tr Acc: 0.6321, Val Loss: 0.6970, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 536/10000, Tr Loss: 0.6148, Tr Acc: 0.6038, Val Loss: 0.6970, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 537/10000, Tr Loss: 0.6127, Tr Acc: 0.6509, Val Loss: 0.6970, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 538/10000, Tr Loss: 0.6074, Tr Acc: 0.6321, Val Loss: 0.6969, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 539/10000, Tr Loss: 0.6207, Tr Acc: 0.6132, Val Loss: 0.6969, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 540/10000, Tr Loss: 0.6236, Tr Acc: 0.5943, Val Loss: 0.6969, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 541/10000, Tr Loss: 0.6142, Tr Acc: 0.6509, Val Loss: 0.6970, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 542/10000, Tr Loss: 0.6093, Tr Acc: 0.6038, Val Loss: 0.6970, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 543/10000, Tr Loss: 0.6205, Tr Acc: 0.6509, Val Loss: 0.6971, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 544/10000, Tr Loss: 0.6107, Tr Acc: 0.6792, Val Loss: 0.6971, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 545/10000, Tr Loss: 0.6191, Tr Acc: 0.6509, Val Loss: 0.6972, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 546/10000, Tr Loss: 0.6069, Tr Acc: 0.6509, Val Loss: 0.6972, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 547/10000, Tr Loss: 0.6150, Tr Acc: 0.5660, Val Loss: 0.6973, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 548/10000, Tr Loss: 0.6122, Tr Acc: 0.6792, Val Loss: 0.6973, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 549/10000, Tr Loss: 0.6123, Tr Acc: 0.6321, Val Loss: 0.6974, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 550/10000, Tr Loss: 0.6221, Tr Acc: 0.6226, Val Loss: 0.6975, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 551/10000, Tr Loss: 0.6089, Tr Acc: 0.6509, Val Loss: 0.6975, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 552/10000, Tr Loss: 0.6067, Tr Acc: 0.6792, Val Loss: 0.6975, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 553/10000, Tr Loss: 0.6104, Tr Acc: 0.6604, Val Loss: 0.6975, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 554/10000, Tr Loss: 0.6056, Tr Acc: 0.6509, Val Loss: 0.6975, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 555/10000, Tr Loss: 0.6256, Tr Acc: 0.6226, Val Loss: 0.6975, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 556/10000, Tr Loss: 0.6102, Tr Acc: 0.6792, Val Loss: 0.6975, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 557/10000, Tr Loss: 0.6127, Tr Acc: 0.6604, Val Loss: 0.6975, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 558/10000, Tr Loss: 0.6110, Tr Acc: 0.6038, Val Loss: 0.6976, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 559/10000, Tr Loss: 0.6093, Tr Acc: 0.6604, Val Loss: 0.6976, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 560/10000, Tr Loss: 0.6259, Tr Acc: 0.5943, Val Loss: 0.6977, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 561/10000, Tr Loss: 0.6127, Tr Acc: 0.6509, Val Loss: 0.6978, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 562/10000, Tr Loss: 0.6156, Tr Acc: 0.6132, Val Loss: 0.6979, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 563/10000, Tr Loss: 0.6194, Tr Acc: 0.5660, Val Loss: 0.6980, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 564/10000, Tr Loss: 0.6112, Tr Acc: 0.6415, Val Loss: 0.6980, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 565/10000, Tr Loss: 0.6123, Tr Acc: 0.6604, Val Loss: 0.6980, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 566/10000, Tr Loss: 0.6081, Tr Acc: 0.6509, Val Loss: 0.6979, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 567/10000, Tr Loss: 0.6170, Tr Acc: 0.6321, Val Loss: 0.6980, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 568/10000, Tr Loss: 0.6073, Tr Acc: 0.5660, Val Loss: 0.6979, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 569/10000, Tr Loss: 0.6100, Tr Acc: 0.6604, Val Loss: 0.6979, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 570/10000, Tr Loss: 0.6136, Tr Acc: 0.6415, Val Loss: 0.6979, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 571/10000, Tr Loss: 0.6058, Tr Acc: 0.6509, Val Loss: 0.6979, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 572/10000, Tr Loss: 0.6273, Tr Acc: 0.6038, Val Loss: 0.6978, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 573/10000, Tr Loss: 0.6209, Tr Acc: 0.6038, Val Loss: 0.6977, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 574/10000, Tr Loss: 0.6131, Tr Acc: 0.6321, Val Loss: 0.6977, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 575/10000, Tr Loss: 0.6054, Tr Acc: 0.6792, Val Loss: 0.6976, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 576/10000, Tr Loss: 0.6133, Tr Acc: 0.6887, Val Loss: 0.6977, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 577/10000, Tr Loss: 0.6136, Tr Acc: 0.6038, Val Loss: 0.6977, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 578/10000, Tr Loss: 0.6155, Tr Acc: 0.6415, Val Loss: 0.6978, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 579/10000, Tr Loss: 0.6193, Tr Acc: 0.5849, Val Loss: 0.6978, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 580/10000, Tr Loss: 0.6072, Tr Acc: 0.6321, Val Loss: 0.6978, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 581/10000, Tr Loss: 0.6233, Tr Acc: 0.5755, Val Loss: 0.6977, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 582/10000, Tr Loss: 0.6140, Tr Acc: 0.6415, Val Loss: 0.6977, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 583/10000, Tr Loss: 0.6317, Tr Acc: 0.5000, Val Loss: 0.6978, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 584/10000, Tr Loss: 0.6120, Tr Acc: 0.6698, Val Loss: 0.6978, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 585/10000, Tr Loss: 0.6083, Tr Acc: 0.5943, Val Loss: 0.6979, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 586/10000, Tr Loss: 0.6155, Tr Acc: 0.6226, Val Loss: 0.6980, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 587/10000, Tr Loss: 0.6098, Tr Acc: 0.6792, Val Loss: 0.6980, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 588/10000, Tr Loss: 0.6032, Tr Acc: 0.6509, Val Loss: 0.6982, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 589/10000, Tr Loss: 0.6125, Tr Acc: 0.6038, Val Loss: 0.6983, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 590/10000, Tr Loss: 0.6025, Tr Acc: 0.6415, Val Loss: 0.6985, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 591/10000, Tr Loss: 0.6215, Tr Acc: 0.5755, Val Loss: 0.6988, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 592/10000, Tr Loss: 0.6093, Tr Acc: 0.5943, Val Loss: 0.6990, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 593/10000, Tr Loss: 0.6112, Tr Acc: 0.6132, Val Loss: 0.6992, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 594/10000, Tr Loss: 0.6104, Tr Acc: 0.6604, Val Loss: 0.6994, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 595/10000, Tr Loss: 0.5913, Tr Acc: 0.7358, Val Loss: 0.6995, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 596/10000, Tr Loss: 0.6179, Tr Acc: 0.6038, Val Loss: 0.6997, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 597/10000, Tr Loss: 0.6075, Tr Acc: 0.6226, Val Loss: 0.6998, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 598/10000, Tr Loss: 0.6085, Tr Acc: 0.5849, Val Loss: 0.6999, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 599/10000, Tr Loss: 0.6132, Tr Acc: 0.6415, Val Loss: 0.7000, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 600/10000, Tr Loss: 0.6042, Tr Acc: 0.6604, Val Loss: 0.7000, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 601/10000, Tr Loss: 0.6141, Tr Acc: 0.6321, Val Loss: 0.7001, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 602/10000, Tr Loss: 0.6362, Tr Acc: 0.5755, Val Loss: 0.7001, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 603/10000, Tr Loss: 0.6018, Tr Acc: 0.6887, Val Loss: 0.7001, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 604/10000, Tr Loss: 0.6142, Tr Acc: 0.6132, Val Loss: 0.7001, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 605/10000, Tr Loss: 0.6053, Tr Acc: 0.6509, Val Loss: 0.7001, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 606/10000, Tr Loss: 0.6207, Tr Acc: 0.5849, Val Loss: 0.7000, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 607/10000, Tr Loss: 0.6259, Tr Acc: 0.5849, Val Loss: 0.6999, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 608/10000, Tr Loss: 0.6103, Tr Acc: 0.6321, Val Loss: 0.6999, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 609/10000, Tr Loss: 0.6045, Tr Acc: 0.6604, Val Loss: 0.6999, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 610/10000, Tr Loss: 0.6288, Tr Acc: 0.5943, Val Loss: 0.6999, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 611/10000, Tr Loss: 0.6229, Tr Acc: 0.5943, Val Loss: 0.7000, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 612/10000, Tr Loss: 0.6091, Tr Acc: 0.6604, Val Loss: 0.7001, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 613/10000, Tr Loss: 0.6118, Tr Acc: 0.6321, Val Loss: 0.7003, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 614/10000, Tr Loss: 0.6058, Tr Acc: 0.6698, Val Loss: 0.7005, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 615/10000, Tr Loss: 0.6091, Tr Acc: 0.5943, Val Loss: 0.7007, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 616/10000, Tr Loss: 0.6234, Tr Acc: 0.5849, Val Loss: 0.7008, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 617/10000, Tr Loss: 0.6144, Tr Acc: 0.5755, Val Loss: 0.7009, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 618/10000, Tr Loss: 0.6055, Tr Acc: 0.6604, Val Loss: 0.7010, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 619/10000, Tr Loss: 0.6137, Tr Acc: 0.6604, Val Loss: 0.7010, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 620/10000, Tr Loss: 0.6130, Tr Acc: 0.6321, Val Loss: 0.7010, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 621/10000, Tr Loss: 0.5928, Tr Acc: 0.7264, Val Loss: 0.7009, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 622/10000, Tr Loss: 0.6046, Tr Acc: 0.6698, Val Loss: 0.7009, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 623/10000, Tr Loss: 0.6114, Tr Acc: 0.6792, Val Loss: 0.7008, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 624/10000, Tr Loss: 0.6166, Tr Acc: 0.6226, Val Loss: 0.7008, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 625/10000, Tr Loss: 0.5971, Tr Acc: 0.6321, Val Loss: 0.7007, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 626/10000, Tr Loss: 0.6086, Tr Acc: 0.6604, Val Loss: 0.7006, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 627/10000, Tr Loss: 0.6160, Tr Acc: 0.5660, Val Loss: 0.7006, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 628/10000, Tr Loss: 0.6132, Tr Acc: 0.6415, Val Loss: 0.7006, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 629/10000, Tr Loss: 0.6126, Tr Acc: 0.6132, Val Loss: 0.7005, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 630/10000, Tr Loss: 0.6262, Tr Acc: 0.5566, Val Loss: 0.7005, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 631/10000, Tr Loss: 0.6196, Tr Acc: 0.6038, Val Loss: 0.7004, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 632/10000, Tr Loss: 0.6088, Tr Acc: 0.6226, Val Loss: 0.7004, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 633/10000, Tr Loss: 0.5928, Tr Acc: 0.7075, Val Loss: 0.7004, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 634/10000, Tr Loss: 0.6006, Tr Acc: 0.6604, Val Loss: 0.7005, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 635/10000, Tr Loss: 0.6090, Tr Acc: 0.6509, Val Loss: 0.7006, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 636/10000, Tr Loss: 0.6125, Tr Acc: 0.6132, Val Loss: 0.7007, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 637/10000, Tr Loss: 0.6131, Tr Acc: 0.6038, Val Loss: 0.7007, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 638/10000, Tr Loss: 0.6079, Tr Acc: 0.6415, Val Loss: 0.7007, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 639/10000, Tr Loss: 0.6143, Tr Acc: 0.5755, Val Loss: 0.7007, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 640/10000, Tr Loss: 0.6014, Tr Acc: 0.6698, Val Loss: 0.7007, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 641/10000, Tr Loss: 0.6179, Tr Acc: 0.6038, Val Loss: 0.7007, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 642/10000, Tr Loss: 0.6246, Tr Acc: 0.5377, Val Loss: 0.7007, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 643/10000, Tr Loss: 0.6184, Tr Acc: 0.5755, Val Loss: 0.7007, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 644/10000, Tr Loss: 0.6151, Tr Acc: 0.6132, Val Loss: 0.7007, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 645/10000, Tr Loss: 0.6106, Tr Acc: 0.6792, Val Loss: 0.7007, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 646/10000, Tr Loss: 0.6026, Tr Acc: 0.6321, Val Loss: 0.7008, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 647/10000, Tr Loss: 0.6125, Tr Acc: 0.5660, Val Loss: 0.7008, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 648/10000, Tr Loss: 0.6035, Tr Acc: 0.6509, Val Loss: 0.7009, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 649/10000, Tr Loss: 0.6239, Tr Acc: 0.6509, Val Loss: 0.7009, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 650/10000, Tr Loss: 0.6179, Tr Acc: 0.5660, Val Loss: 0.7010, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 651/10000, Tr Loss: 0.6163, Tr Acc: 0.5660, Val Loss: 0.7010, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 652/10000, Tr Loss: 0.6208, Tr Acc: 0.5755, Val Loss: 0.7010, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 653/10000, Tr Loss: 0.6071, Tr Acc: 0.6698, Val Loss: 0.7011, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 654/10000, Tr Loss: 0.6148, Tr Acc: 0.6226, Val Loss: 0.7013, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 655/10000, Tr Loss: 0.6034, Tr Acc: 0.6887, Val Loss: 0.7014, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 656/10000, Tr Loss: 0.6163, Tr Acc: 0.5755, Val Loss: 0.7015, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 657/10000, Tr Loss: 0.6058, Tr Acc: 0.6981, Val Loss: 0.7016, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 658/10000, Tr Loss: 0.6005, Tr Acc: 0.7170, Val Loss: 0.7018, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 659/10000, Tr Loss: 0.6153, Tr Acc: 0.5849, Val Loss: 0.7019, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 660/10000, Tr Loss: 0.6096, Tr Acc: 0.6509, Val Loss: 0.7020, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 661/10000, Tr Loss: 0.6086, Tr Acc: 0.6038, Val Loss: 0.7022, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 662/10000, Tr Loss: 0.6151, Tr Acc: 0.5849, Val Loss: 0.7025, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 663/10000, Tr Loss: 0.6085, Tr Acc: 0.6321, Val Loss: 0.7026, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 664/10000, Tr Loss: 0.6146, Tr Acc: 0.6415, Val Loss: 0.7027, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 665/10000, Tr Loss: 0.6049, Tr Acc: 0.6887, Val Loss: 0.7028, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 666/10000, Tr Loss: 0.6128, Tr Acc: 0.6132, Val Loss: 0.7028, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 667/10000, Tr Loss: 0.6183, Tr Acc: 0.6226, Val Loss: 0.7029, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 668/10000, Tr Loss: 0.6211, Tr Acc: 0.6321, Val Loss: 0.7029, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 669/10000, Tr Loss: 0.6028, Tr Acc: 0.6604, Val Loss: 0.7028, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 670/10000, Tr Loss: 0.5904, Tr Acc: 0.7358, Val Loss: 0.7028, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 671/10000, Tr Loss: 0.6051, Tr Acc: 0.6509, Val Loss: 0.7027, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 672/10000, Tr Loss: 0.6102, Tr Acc: 0.6132, Val Loss: 0.7027, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 673/10000, Tr Loss: 0.6102, Tr Acc: 0.6321, Val Loss: 0.7026, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 674/10000, Tr Loss: 0.6152, Tr Acc: 0.6792, Val Loss: 0.7026, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 675/10000, Tr Loss: 0.6083, Tr Acc: 0.6415, Val Loss: 0.7024, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 676/10000, Tr Loss: 0.6116, Tr Acc: 0.5943, Val Loss: 0.7023, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 677/10000, Tr Loss: 0.6162, Tr Acc: 0.5849, Val Loss: 0.7022, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 678/10000, Tr Loss: 0.6003, Tr Acc: 0.6604, Val Loss: 0.7021, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 679/10000, Tr Loss: 0.6068, Tr Acc: 0.6415, Val Loss: 0.7020, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 680/10000, Tr Loss: 0.6131, Tr Acc: 0.5189, Val Loss: 0.7020, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 681/10000, Tr Loss: 0.6123, Tr Acc: 0.6132, Val Loss: 0.7019, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 682/10000, Tr Loss: 0.6125, Tr Acc: 0.6415, Val Loss: 0.7020, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 683/10000, Tr Loss: 0.6263, Tr Acc: 0.5472, Val Loss: 0.7020, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 684/10000, Tr Loss: 0.6105, Tr Acc: 0.6226, Val Loss: 0.7020, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 685/10000, Tr Loss: 0.6076, Tr Acc: 0.5566, Val Loss: 0.7020, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 686/10000, Tr Loss: 0.6195, Tr Acc: 0.5849, Val Loss: 0.7020, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 687/10000, Tr Loss: 0.6153, Tr Acc: 0.6038, Val Loss: 0.7018, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 688/10000, Tr Loss: 0.6232, Tr Acc: 0.5755, Val Loss: 0.7017, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 689/10000, Tr Loss: 0.6156, Tr Acc: 0.6038, Val Loss: 0.7016, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 690/10000, Tr Loss: 0.6172, Tr Acc: 0.5566, Val Loss: 0.7015, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 691/10000, Tr Loss: 0.6217, Tr Acc: 0.5566, Val Loss: 0.7014, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 692/10000, Tr Loss: 0.6131, Tr Acc: 0.6415, Val Loss: 0.7013, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 693/10000, Tr Loss: 0.5949, Tr Acc: 0.6792, Val Loss: 0.7012, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 694/10000, Tr Loss: 0.6174, Tr Acc: 0.5849, Val Loss: 0.7012, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 695/10000, Tr Loss: 0.5989, Tr Acc: 0.6509, Val Loss: 0.7013, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 696/10000, Tr Loss: 0.6128, Tr Acc: 0.6226, Val Loss: 0.7013, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 697/10000, Tr Loss: 0.6103, Tr Acc: 0.6038, Val Loss: 0.7014, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 698/10000, Tr Loss: 0.6136, Tr Acc: 0.5755, Val Loss: 0.7015, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 699/10000, Tr Loss: 0.6040, Tr Acc: 0.6321, Val Loss: 0.7016, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 700/10000, Tr Loss: 0.6140, Tr Acc: 0.6132, Val Loss: 0.7017, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 701/10000, Tr Loss: 0.6159, Tr Acc: 0.6038, Val Loss: 0.7017, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 702/10000, Tr Loss: 0.6051, Tr Acc: 0.6321, Val Loss: 0.7018, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 703/10000, Tr Loss: 0.6176, Tr Acc: 0.5566, Val Loss: 0.7019, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 704/10000, Tr Loss: 0.6204, Tr Acc: 0.5472, Val Loss: 0.7019, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 705/10000, Tr Loss: 0.6116, Tr Acc: 0.5755, Val Loss: 0.7020, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 706/10000, Tr Loss: 0.6113, Tr Acc: 0.6321, Val Loss: 0.7022, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 707/10000, Tr Loss: 0.6149, Tr Acc: 0.5755, Val Loss: 0.7023, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 708/10000, Tr Loss: 0.6228, Tr Acc: 0.5377, Val Loss: 0.7024, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 709/10000, Tr Loss: 0.6052, Tr Acc: 0.6226, Val Loss: 0.7025, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 710/10000, Tr Loss: 0.6070, Tr Acc: 0.6038, Val Loss: 0.7025, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 711/10000, Tr Loss: 0.5970, Tr Acc: 0.6321, Val Loss: 0.7026, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 712/10000, Tr Loss: 0.6076, Tr Acc: 0.6509, Val Loss: 0.7026, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 713/10000, Tr Loss: 0.6313, Tr Acc: 0.5566, Val Loss: 0.7026, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 714/10000, Tr Loss: 0.6172, Tr Acc: 0.6132, Val Loss: 0.7026, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 715/10000, Tr Loss: 0.5998, Tr Acc: 0.6887, Val Loss: 0.7027, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 716/10000, Tr Loss: 0.6072, Tr Acc: 0.6792, Val Loss: 0.7027, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 717/10000, Tr Loss: 0.6096, Tr Acc: 0.5943, Val Loss: 0.7028, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 718/10000, Tr Loss: 0.5974, Tr Acc: 0.6887, Val Loss: 0.7028, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 719/10000, Tr Loss: 0.6068, Tr Acc: 0.6038, Val Loss: 0.7029, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 720/10000, Tr Loss: 0.6038, Tr Acc: 0.6321, Val Loss: 0.7030, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 721/10000, Tr Loss: 0.6102, Tr Acc: 0.5472, Val Loss: 0.7031, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 722/10000, Tr Loss: 0.6007, Tr Acc: 0.6132, Val Loss: 0.7032, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 723/10000, Tr Loss: 0.6064, Tr Acc: 0.6132, Val Loss: 0.7033, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 724/10000, Tr Loss: 0.6027, Tr Acc: 0.6415, Val Loss: 0.7033, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 725/10000, Tr Loss: 0.5945, Tr Acc: 0.6604, Val Loss: 0.7033, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 726/10000, Tr Loss: 0.6147, Tr Acc: 0.6226, Val Loss: 0.7032, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 727/10000, Tr Loss: 0.6024, Tr Acc: 0.6698, Val Loss: 0.7030, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 728/10000, Tr Loss: 0.6106, Tr Acc: 0.6509, Val Loss: 0.7029, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 729/10000, Tr Loss: 0.6033, Tr Acc: 0.6509, Val Loss: 0.7027, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 730/10000, Tr Loss: 0.6066, Tr Acc: 0.6415, Val Loss: 0.7026, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 731/10000, Tr Loss: 0.6058, Tr Acc: 0.6604, Val Loss: 0.7024, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 732/10000, Tr Loss: 0.6065, Tr Acc: 0.6132, Val Loss: 0.7023, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 733/10000, Tr Loss: 0.6138, Tr Acc: 0.6321, Val Loss: 0.7022, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 734/10000, Tr Loss: 0.6107, Tr Acc: 0.6321, Val Loss: 0.7021, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 735/10000, Tr Loss: 0.6210, Tr Acc: 0.5660, Val Loss: 0.7020, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 736/10000, Tr Loss: 0.6088, Tr Acc: 0.6321, Val Loss: 0.7019, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 737/10000, Tr Loss: 0.6117, Tr Acc: 0.6226, Val Loss: 0.7018, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 738/10000, Tr Loss: 0.6178, Tr Acc: 0.5755, Val Loss: 0.7016, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 739/10000, Tr Loss: 0.6100, Tr Acc: 0.5755, Val Loss: 0.7015, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 740/10000, Tr Loss: 0.6168, Tr Acc: 0.5755, Val Loss: 0.7013, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 741/10000, Tr Loss: 0.5975, Tr Acc: 0.6792, Val Loss: 0.7013, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 742/10000, Tr Loss: 0.6121, Tr Acc: 0.5755, Val Loss: 0.7011, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 743/10000, Tr Loss: 0.6144, Tr Acc: 0.5849, Val Loss: 0.7011, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 744/10000, Tr Loss: 0.6126, Tr Acc: 0.6132, Val Loss: 0.7010, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 745/10000, Tr Loss: 0.6143, Tr Acc: 0.6038, Val Loss: 0.7010, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 746/10000, Tr Loss: 0.6040, Tr Acc: 0.6981, Val Loss: 0.7010, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 747/10000, Tr Loss: 0.6059, Tr Acc: 0.6226, Val Loss: 0.7011, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 748/10000, Tr Loss: 0.6074, Tr Acc: 0.6321, Val Loss: 0.7012, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 749/10000, Tr Loss: 0.5956, Tr Acc: 0.5943, Val Loss: 0.7013, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 750/10000, Tr Loss: 0.6126, Tr Acc: 0.6226, Val Loss: 0.7014, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 751/10000, Tr Loss: 0.6151, Tr Acc: 0.5849, Val Loss: 0.7015, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 752/10000, Tr Loss: 0.5999, Tr Acc: 0.6604, Val Loss: 0.7016, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 753/10000, Tr Loss: 0.6016, Tr Acc: 0.5943, Val Loss: 0.7017, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 754/10000, Tr Loss: 0.5949, Tr Acc: 0.7075, Val Loss: 0.7019, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 755/10000, Tr Loss: 0.6144, Tr Acc: 0.5566, Val Loss: 0.7021, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 756/10000, Tr Loss: 0.6045, Tr Acc: 0.6698, Val Loss: 0.7023, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 757/10000, Tr Loss: 0.6118, Tr Acc: 0.6226, Val Loss: 0.7025, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 758/10000, Tr Loss: 0.6276, Tr Acc: 0.5094, Val Loss: 0.7027, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 759/10000, Tr Loss: 0.6077, Tr Acc: 0.6038, Val Loss: 0.7028, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 760/10000, Tr Loss: 0.6033, Tr Acc: 0.6698, Val Loss: 0.7029, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 761/10000, Tr Loss: 0.6037, Tr Acc: 0.6604, Val Loss: 0.7030, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 762/10000, Tr Loss: 0.6155, Tr Acc: 0.5849, Val Loss: 0.7030, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 763/10000, Tr Loss: 0.5919, Tr Acc: 0.6604, Val Loss: 0.7032, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 764/10000, Tr Loss: 0.6029, Tr Acc: 0.6226, Val Loss: 0.7032, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 765/10000, Tr Loss: 0.6201, Tr Acc: 0.5943, Val Loss: 0.7032, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 766/10000, Tr Loss: 0.5996, Tr Acc: 0.6226, Val Loss: 0.7032, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 767/10000, Tr Loss: 0.6140, Tr Acc: 0.5755, Val Loss: 0.7032, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 768/10000, Tr Loss: 0.5999, Tr Acc: 0.6415, Val Loss: 0.7032, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 769/10000, Tr Loss: 0.6123, Tr Acc: 0.5943, Val Loss: 0.7031, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 770/10000, Tr Loss: 0.6151, Tr Acc: 0.6132, Val Loss: 0.7030, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 771/10000, Tr Loss: 0.6091, Tr Acc: 0.6038, Val Loss: 0.7030, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 772/10000, Tr Loss: 0.6062, Tr Acc: 0.6415, Val Loss: 0.7029, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 773/10000, Tr Loss: 0.6029, Tr Acc: 0.6321, Val Loss: 0.7028, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 774/10000, Tr Loss: 0.6094, Tr Acc: 0.6321, Val Loss: 0.7028, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 775/10000, Tr Loss: 0.6078, Tr Acc: 0.6038, Val Loss: 0.7028, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 776/10000, Tr Loss: 0.6145, Tr Acc: 0.6038, Val Loss: 0.7028, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 777/10000, Tr Loss: 0.6056, Tr Acc: 0.5849, Val Loss: 0.7028, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 778/10000, Tr Loss: 0.6027, Tr Acc: 0.6226, Val Loss: 0.7028, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 779/10000, Tr Loss: 0.6087, Tr Acc: 0.5849, Val Loss: 0.7028, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 780/10000, Tr Loss: 0.6167, Tr Acc: 0.5943, Val Loss: 0.7028, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 781/10000, Tr Loss: 0.6119, Tr Acc: 0.5943, Val Loss: 0.7029, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 782/10000, Tr Loss: 0.5992, Tr Acc: 0.6792, Val Loss: 0.7029, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 783/10000, Tr Loss: 0.5940, Tr Acc: 0.6509, Val Loss: 0.7030, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 784/10000, Tr Loss: 0.6096, Tr Acc: 0.6415, Val Loss: 0.7031, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 785/10000, Tr Loss: 0.6044, Tr Acc: 0.6509, Val Loss: 0.7033, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 786/10000, Tr Loss: 0.6206, Tr Acc: 0.5755, Val Loss: 0.7034, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 787/10000, Tr Loss: 0.6048, Tr Acc: 0.6604, Val Loss: 0.7035, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 788/10000, Tr Loss: 0.6130, Tr Acc: 0.6038, Val Loss: 0.7036, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 789/10000, Tr Loss: 0.6229, Tr Acc: 0.6038, Val Loss: 0.7036, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 790/10000, Tr Loss: 0.6063, Tr Acc: 0.6415, Val Loss: 0.7037, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 791/10000, Tr Loss: 0.6073, Tr Acc: 0.6415, Val Loss: 0.7038, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 792/10000, Tr Loss: 0.6020, Tr Acc: 0.6038, Val Loss: 0.7039, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 793/10000, Tr Loss: 0.6057, Tr Acc: 0.6132, Val Loss: 0.7040, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 794/10000, Tr Loss: 0.6081, Tr Acc: 0.6226, Val Loss: 0.7040, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 795/10000, Tr Loss: 0.6070, Tr Acc: 0.6226, Val Loss: 0.7039, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 796/10000, Tr Loss: 0.6069, Tr Acc: 0.6792, Val Loss: 0.7039, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 797/10000, Tr Loss: 0.5975, Tr Acc: 0.6321, Val Loss: 0.7040, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 798/10000, Tr Loss: 0.6084, Tr Acc: 0.6415, Val Loss: 0.7040, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 799/10000, Tr Loss: 0.6040, Tr Acc: 0.6226, Val Loss: 0.7040, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 800/10000, Tr Loss: 0.6184, Tr Acc: 0.5755, Val Loss: 0.7040, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 801/10000, Tr Loss: 0.5960, Tr Acc: 0.6698, Val Loss: 0.7040, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 802/10000, Tr Loss: 0.6003, Tr Acc: 0.6792, Val Loss: 0.7039, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 803/10000, Tr Loss: 0.6000, Tr Acc: 0.6604, Val Loss: 0.7039, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 804/10000, Tr Loss: 0.6065, Tr Acc: 0.7170, Val Loss: 0.7039, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 805/10000, Tr Loss: 0.6153, Tr Acc: 0.5660, Val Loss: 0.7039, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 806/10000, Tr Loss: 0.6035, Tr Acc: 0.6415, Val Loss: 0.7039, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 807/10000, Tr Loss: 0.6154, Tr Acc: 0.6132, Val Loss: 0.7038, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 808/10000, Tr Loss: 0.6026, Tr Acc: 0.6321, Val Loss: 0.7038, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 809/10000, Tr Loss: 0.6085, Tr Acc: 0.6415, Val Loss: 0.7037, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 810/10000, Tr Loss: 0.6060, Tr Acc: 0.6415, Val Loss: 0.7036, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 811/10000, Tr Loss: 0.6248, Tr Acc: 0.5000, Val Loss: 0.7036, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 812/10000, Tr Loss: 0.6122, Tr Acc: 0.5755, Val Loss: 0.7035, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 813/10000, Tr Loss: 0.6183, Tr Acc: 0.5660, Val Loss: 0.7034, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 814/10000, Tr Loss: 0.6130, Tr Acc: 0.5377, Val Loss: 0.7034, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 815/10000, Tr Loss: 0.6067, Tr Acc: 0.6415, Val Loss: 0.7034, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 816/10000, Tr Loss: 0.6095, Tr Acc: 0.6038, Val Loss: 0.7035, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 817/10000, Tr Loss: 0.6087, Tr Acc: 0.7170, Val Loss: 0.7036, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 818/10000, Tr Loss: 0.6170, Tr Acc: 0.6132, Val Loss: 0.7037, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 819/10000, Tr Loss: 0.6016, Tr Acc: 0.6415, Val Loss: 0.7039, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 820/10000, Tr Loss: 0.6195, Tr Acc: 0.5755, Val Loss: 0.7040, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 821/10000, Tr Loss: 0.6113, Tr Acc: 0.6321, Val Loss: 0.7041, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 822/10000, Tr Loss: 0.5964, Tr Acc: 0.6792, Val Loss: 0.7042, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 823/10000, Tr Loss: 0.6111, Tr Acc: 0.6226, Val Loss: 0.7041, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 824/10000, Tr Loss: 0.6016, Tr Acc: 0.6981, Val Loss: 0.7042, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 825/10000, Tr Loss: 0.6111, Tr Acc: 0.6226, Val Loss: 0.7042, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 826/10000, Tr Loss: 0.6094, Tr Acc: 0.6132, Val Loss: 0.7043, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 827/10000, Tr Loss: 0.6136, Tr Acc: 0.6038, Val Loss: 0.7044, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 828/10000, Tr Loss: 0.5942, Tr Acc: 0.6604, Val Loss: 0.7045, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 829/10000, Tr Loss: 0.6160, Tr Acc: 0.6038, Val Loss: 0.7046, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 830/10000, Tr Loss: 0.6033, Tr Acc: 0.6132, Val Loss: 0.7048, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 831/10000, Tr Loss: 0.5999, Tr Acc: 0.6604, Val Loss: 0.7050, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 832/10000, Tr Loss: 0.6041, Tr Acc: 0.6132, Val Loss: 0.7051, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 833/10000, Tr Loss: 0.6155, Tr Acc: 0.5660, Val Loss: 0.7053, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 834/10000, Tr Loss: 0.6030, Tr Acc: 0.6887, Val Loss: 0.7054, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 835/10000, Tr Loss: 0.6065, Tr Acc: 0.6226, Val Loss: 0.7054, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 836/10000, Tr Loss: 0.6154, Tr Acc: 0.5660, Val Loss: 0.7055, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 837/10000, Tr Loss: 0.6100, Tr Acc: 0.5566, Val Loss: 0.7056, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 838/10000, Tr Loss: 0.6055, Tr Acc: 0.6226, Val Loss: 0.7056, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 839/10000, Tr Loss: 0.6181, Tr Acc: 0.5660, Val Loss: 0.7056, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 840/10000, Tr Loss: 0.6009, Tr Acc: 0.6321, Val Loss: 0.7057, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 841/10000, Tr Loss: 0.6051, Tr Acc: 0.6792, Val Loss: 0.7057, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 842/10000, Tr Loss: 0.6076, Tr Acc: 0.6226, Val Loss: 0.7058, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 843/10000, Tr Loss: 0.5983, Tr Acc: 0.6132, Val Loss: 0.7058, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 844/10000, Tr Loss: 0.6096, Tr Acc: 0.6038, Val Loss: 0.7058, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 845/10000, Tr Loss: 0.6077, Tr Acc: 0.6604, Val Loss: 0.7058, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 846/10000, Tr Loss: 0.6049, Tr Acc: 0.5943, Val Loss: 0.7058, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 847/10000, Tr Loss: 0.6084, Tr Acc: 0.6132, Val Loss: 0.7058, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 848/10000, Tr Loss: 0.6033, Tr Acc: 0.6698, Val Loss: 0.7058, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 849/10000, Tr Loss: 0.6078, Tr Acc: 0.6038, Val Loss: 0.7057, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 850/10000, Tr Loss: 0.5834, Tr Acc: 0.7736, Val Loss: 0.7057, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 851/10000, Tr Loss: 0.6052, Tr Acc: 0.6132, Val Loss: 0.7057, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 852/10000, Tr Loss: 0.6109, Tr Acc: 0.6415, Val Loss: 0.7056, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 853/10000, Tr Loss: 0.6031, Tr Acc: 0.6604, Val Loss: 0.7056, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 854/10000, Tr Loss: 0.6094, Tr Acc: 0.5943, Val Loss: 0.7055, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 855/10000, Tr Loss: 0.6101, Tr Acc: 0.5849, Val Loss: 0.7055, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 856/10000, Tr Loss: 0.6034, Tr Acc: 0.6415, Val Loss: 0.7054, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 857/10000, Tr Loss: 0.6033, Tr Acc: 0.5849, Val Loss: 0.7053, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 858/10000, Tr Loss: 0.6069, Tr Acc: 0.6132, Val Loss: 0.7052, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 859/10000, Tr Loss: 0.6120, Tr Acc: 0.5943, Val Loss: 0.7050, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 860/10000, Tr Loss: 0.5962, Tr Acc: 0.6604, Val Loss: 0.7048, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 861/10000, Tr Loss: 0.6136, Tr Acc: 0.5755, Val Loss: 0.7048, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 862/10000, Tr Loss: 0.6042, Tr Acc: 0.6132, Val Loss: 0.7047, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 863/10000, Tr Loss: 0.6026, Tr Acc: 0.6226, Val Loss: 0.7046, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 864/10000, Tr Loss: 0.6113, Tr Acc: 0.6509, Val Loss: 0.7045, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 865/10000, Tr Loss: 0.5974, Tr Acc: 0.6321, Val Loss: 0.7044, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 866/10000, Tr Loss: 0.5990, Tr Acc: 0.6415, Val Loss: 0.7043, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 867/10000, Tr Loss: 0.6147, Tr Acc: 0.5849, Val Loss: 0.7041, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 868/10000, Tr Loss: 0.6057, Tr Acc: 0.6132, Val Loss: 0.7040, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 869/10000, Tr Loss: 0.5967, Tr Acc: 0.6415, Val Loss: 0.7039, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 870/10000, Tr Loss: 0.5854, Tr Acc: 0.7453, Val Loss: 0.7039, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 871/10000, Tr Loss: 0.6101, Tr Acc: 0.6509, Val Loss: 0.7038, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 872/10000, Tr Loss: 0.6007, Tr Acc: 0.6698, Val Loss: 0.7038, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 873/10000, Tr Loss: 0.6035, Tr Acc: 0.5849, Val Loss: 0.7039, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 874/10000, Tr Loss: 0.6143, Tr Acc: 0.5755, Val Loss: 0.7040, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 875/10000, Tr Loss: 0.6079, Tr Acc: 0.5755, Val Loss: 0.7041, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 876/10000, Tr Loss: 0.6282, Tr Acc: 0.5189, Val Loss: 0.7042, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 877/10000, Tr Loss: 0.6050, Tr Acc: 0.6321, Val Loss: 0.7044, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 878/10000, Tr Loss: 0.6049, Tr Acc: 0.6509, Val Loss: 0.7046, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 879/10000, Tr Loss: 0.6103, Tr Acc: 0.5472, Val Loss: 0.7047, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 880/10000, Tr Loss: 0.6074, Tr Acc: 0.6038, Val Loss: 0.7049, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 881/10000, Tr Loss: 0.6010, Tr Acc: 0.6604, Val Loss: 0.7049, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 882/10000, Tr Loss: 0.5997, Tr Acc: 0.6604, Val Loss: 0.7050, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 883/10000, Tr Loss: 0.5953, Tr Acc: 0.6887, Val Loss: 0.7051, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 884/10000, Tr Loss: 0.6156, Tr Acc: 0.6038, Val Loss: 0.7051, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 885/10000, Tr Loss: 0.5958, Tr Acc: 0.6604, Val Loss: 0.7051, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 886/10000, Tr Loss: 0.6055, Tr Acc: 0.6132, Val Loss: 0.7051, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 887/10000, Tr Loss: 0.5957, Tr Acc: 0.6887, Val Loss: 0.7051, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 888/10000, Tr Loss: 0.6146, Tr Acc: 0.5755, Val Loss: 0.7050, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 889/10000, Tr Loss: 0.6023, Tr Acc: 0.6415, Val Loss: 0.7050, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 890/10000, Tr Loss: 0.6165, Tr Acc: 0.5377, Val Loss: 0.7050, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 891/10000, Tr Loss: 0.6071, Tr Acc: 0.6415, Val Loss: 0.7049, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 892/10000, Tr Loss: 0.5984, Tr Acc: 0.6604, Val Loss: 0.7050, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 893/10000, Tr Loss: 0.6228, Tr Acc: 0.5377, Val Loss: 0.7051, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 894/10000, Tr Loss: 0.6096, Tr Acc: 0.6132, Val Loss: 0.7051, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 895/10000, Tr Loss: 0.6112, Tr Acc: 0.6132, Val Loss: 0.7053, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 896/10000, Tr Loss: 0.6107, Tr Acc: 0.6509, Val Loss: 0.7054, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 897/10000, Tr Loss: 0.6033, Tr Acc: 0.6226, Val Loss: 0.7055, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 898/10000, Tr Loss: 0.6004, Tr Acc: 0.6132, Val Loss: 0.7056, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 899/10000, Tr Loss: 0.6054, Tr Acc: 0.5849, Val Loss: 0.7057, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 900/10000, Tr Loss: 0.5994, Tr Acc: 0.6792, Val Loss: 0.7057, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 901/10000, Tr Loss: 0.6065, Tr Acc: 0.6604, Val Loss: 0.7058, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 902/10000, Tr Loss: 0.6009, Tr Acc: 0.6038, Val Loss: 0.7058, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 903/10000, Tr Loss: 0.5974, Tr Acc: 0.6604, Val Loss: 0.7059, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 904/10000, Tr Loss: 0.6048, Tr Acc: 0.6698, Val Loss: 0.7060, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 905/10000, Tr Loss: 0.6184, Tr Acc: 0.5755, Val Loss: 0.7061, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 906/10000, Tr Loss: 0.6063, Tr Acc: 0.6038, Val Loss: 0.7062, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 907/10000, Tr Loss: 0.6018, Tr Acc: 0.6415, Val Loss: 0.7062, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 908/10000, Tr Loss: 0.6213, Tr Acc: 0.5755, Val Loss: 0.7062, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 909/10000, Tr Loss: 0.5974, Tr Acc: 0.5943, Val Loss: 0.7062, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 910/10000, Tr Loss: 0.6064, Tr Acc: 0.6604, Val Loss: 0.7061, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 911/10000, Tr Loss: 0.6084, Tr Acc: 0.5943, Val Loss: 0.7061, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 912/10000, Tr Loss: 0.5993, Tr Acc: 0.6509, Val Loss: 0.7060, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 913/10000, Tr Loss: 0.6168, Tr Acc: 0.5660, Val Loss: 0.7060, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 914/10000, Tr Loss: 0.6010, Tr Acc: 0.6415, Val Loss: 0.7060, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 915/10000, Tr Loss: 0.5948, Tr Acc: 0.6321, Val Loss: 0.7059, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 916/10000, Tr Loss: 0.5954, Tr Acc: 0.7264, Val Loss: 0.7059, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 917/10000, Tr Loss: 0.6055, Tr Acc: 0.6132, Val Loss: 0.7058, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 918/10000, Tr Loss: 0.6119, Tr Acc: 0.5472, Val Loss: 0.7058, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 919/10000, Tr Loss: 0.6142, Tr Acc: 0.5943, Val Loss: 0.7057, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 920/10000, Tr Loss: 0.5947, Tr Acc: 0.7170, Val Loss: 0.7056, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 921/10000, Tr Loss: 0.6110, Tr Acc: 0.5472, Val Loss: 0.7055, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 922/10000, Tr Loss: 0.6173, Tr Acc: 0.5943, Val Loss: 0.7054, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 923/10000, Tr Loss: 0.6103, Tr Acc: 0.6038, Val Loss: 0.7053, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 924/10000, Tr Loss: 0.6059, Tr Acc: 0.6321, Val Loss: 0.7052, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 925/10000, Tr Loss: 0.6159, Tr Acc: 0.5377, Val Loss: 0.7052, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 926/10000, Tr Loss: 0.6073, Tr Acc: 0.6038, Val Loss: 0.7051, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 927/10000, Tr Loss: 0.6241, Tr Acc: 0.5849, Val Loss: 0.7050, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 928/10000, Tr Loss: 0.6154, Tr Acc: 0.5755, Val Loss: 0.7050, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 929/10000, Tr Loss: 0.6109, Tr Acc: 0.6038, Val Loss: 0.7050, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 930/10000, Tr Loss: 0.6036, Tr Acc: 0.6509, Val Loss: 0.7050, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 931/10000, Tr Loss: 0.6084, Tr Acc: 0.6038, Val Loss: 0.7050, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 932/10000, Tr Loss: 0.6069, Tr Acc: 0.6226, Val Loss: 0.7050, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 933/10000, Tr Loss: 0.6100, Tr Acc: 0.6604, Val Loss: 0.7050, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 934/10000, Tr Loss: 0.6159, Tr Acc: 0.5943, Val Loss: 0.7051, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 935/10000, Tr Loss: 0.5953, Tr Acc: 0.7264, Val Loss: 0.7051, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 936/10000, Tr Loss: 0.6087, Tr Acc: 0.5943, Val Loss: 0.7052, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 937/10000, Tr Loss: 0.6183, Tr Acc: 0.5943, Val Loss: 0.7052, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 938/10000, Tr Loss: 0.6271, Tr Acc: 0.5377, Val Loss: 0.7053, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 939/10000, Tr Loss: 0.6046, Tr Acc: 0.6509, Val Loss: 0.7053, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 940/10000, Tr Loss: 0.6147, Tr Acc: 0.6132, Val Loss: 0.7054, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 941/10000, Tr Loss: 0.6025, Tr Acc: 0.6415, Val Loss: 0.7055, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 942/10000, Tr Loss: 0.6083, Tr Acc: 0.5660, Val Loss: 0.7056, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 943/10000, Tr Loss: 0.6238, Tr Acc: 0.5283, Val Loss: 0.7057, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 944/10000, Tr Loss: 0.6010, Tr Acc: 0.6226, Val Loss: 0.7059, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 945/10000, Tr Loss: 0.5886, Tr Acc: 0.6792, Val Loss: 0.7059, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 946/10000, Tr Loss: 0.5971, Tr Acc: 0.6604, Val Loss: 0.7060, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 947/10000, Tr Loss: 0.6132, Tr Acc: 0.6038, Val Loss: 0.7061, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 948/10000, Tr Loss: 0.6137, Tr Acc: 0.5849, Val Loss: 0.7062, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 949/10000, Tr Loss: 0.6110, Tr Acc: 0.6415, Val Loss: 0.7063, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 950/10000, Tr Loss: 0.5884, Tr Acc: 0.6792, Val Loss: 0.7064, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 951/10000, Tr Loss: 0.6069, Tr Acc: 0.6415, Val Loss: 0.7065, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 952/10000, Tr Loss: 0.6021, Tr Acc: 0.6509, Val Loss: 0.7066, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 953/10000, Tr Loss: 0.6164, Tr Acc: 0.5660, Val Loss: 0.7068, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 954/10000, Tr Loss: 0.6170, Tr Acc: 0.5660, Val Loss: 0.7069, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 955/10000, Tr Loss: 0.6162, Tr Acc: 0.5943, Val Loss: 0.7070, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 956/10000, Tr Loss: 0.6095, Tr Acc: 0.6604, Val Loss: 0.7071, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 957/10000, Tr Loss: 0.6129, Tr Acc: 0.5943, Val Loss: 0.7072, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 958/10000, Tr Loss: 0.6205, Tr Acc: 0.5849, Val Loss: 0.7072, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 959/10000, Tr Loss: 0.6063, Tr Acc: 0.6038, Val Loss: 0.7073, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 960/10000, Tr Loss: 0.6046, Tr Acc: 0.6415, Val Loss: 0.7073, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 961/10000, Tr Loss: 0.6078, Tr Acc: 0.6321, Val Loss: 0.7072, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 962/10000, Tr Loss: 0.6051, Tr Acc: 0.6321, Val Loss: 0.7072, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 963/10000, Tr Loss: 0.6071, Tr Acc: 0.6415, Val Loss: 0.7072, Val Acc: 0.5000 , le : 0.000100\n",
      "Epoch 964/10000, Tr Loss: 0.6040, Tr Acc: 0.6132, Val Loss: 0.7073, Val Acc: 0.5000 , le : 0.000100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for network in network_list:\n",
    "\n",
    "\n",
    "    for subject_id in subjects:\n",
    "        for percent in percent_list:\n",
    "            run = neptune.init_run(\n",
    "    project=\"AitBrainLab/Synthetic\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhMTMyMzg0My02NzlhLTQ3N2ItYTdmMS0yNTcwNDBmM2QwM2QifQ==\",\n",
    ")\n",
    "\n",
    "            #torch.manual_seed(3407)\n",
    "            eeg = EEG_fif(path, base_url, subjects, runs)\n",
    "            raw=eeg.data_to_raw()\n",
    "            raw=raw.filter( 8,35, method='fir', verbose=20)\n",
    "            epochs=eeg.epochs_visu(raw,tmin=0,tmax=7,baseline=(0,2))\n",
    "            X_train, y = eeg.get_X_y(epochs)\n",
    "            y_train=y-1\n",
    "\n",
    "            sfreq = 250\n",
    "            print(sfreq)\n",
    "\n",
    "\n",
    "            X_syntheic = np.load(\"Synthetic_data_online/X_subject_[20]_72_['left', 'right']_v2.npy\")\n",
    "            y_syntheic = np.load(\"Synthetic_data_online/y_subject_[20]_72_['left', 'right']_v2.npy\")\n",
    "            if len(types_list) == 2:\n",
    "                for task in types_list :\n",
    "                    if task == \"left\":\n",
    "                        X1 = X_syntheic[np.random.randint(0,72,percent)]\n",
    "                        y1 = y_syntheic[np.random.randint(0,72,percent)]\n",
    "                    if task == \"right\":\n",
    "                        X2 = X_syntheic[np.random.randint(72,144,percent)]\n",
    "                        y2 = y_syntheic[np.random.randint(72,144,percent)]\n",
    "                X_syntheic = np.concatenate((X1,X2),axis=0)\n",
    "                y_syntheic = np.concatenate((y1,y2),axis=0)\n",
    "            X_train = np.concatenate((X_train,X_syntheic),axis = 0)\n",
    "            y_train = np.concatenate((y_train, y_syntheic),axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_train,y_train, test_size=0.3,stratify=y_train)\n",
    "\n",
    "            print('train size',X_train.shape, y_train.shape)\n",
    "            print('test size',X_test.shape, y_test.shape)\n",
    "\n",
    "            batch_size = X_train.shape[2]\n",
    "\n",
    "            train_loader = create_dataloader(X_train, y_train, batch_size=batch_size)\n",
    "            test_loader = create_dataloader(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "            n_classes=2\n",
    "            n_chans = 8\n",
    "\n",
    "            #network_list = ['ATCNet', 'ShallowFBCSPNet', 'Deep4Net', 'EEGNetv4']\n",
    "            if network == 'ATCNet' :\n",
    "                model = ATCNet(\n",
    "                                n_chans,\n",
    "                                n_classes\n",
    "                                )\n",
    "            if network == \"ShallowFBCSPNet\":\n",
    "                model = ShallowFBCSPNet(\n",
    "                n_chans,\n",
    "                n_classes,\n",
    "                input_window_samples=1751,\n",
    "                final_conv_length=\"auto\",\n",
    "                )\n",
    "            if network == \"EEGITNet\":\n",
    "                model = EEGITNet(\n",
    "                    n_classes,\n",
    "                    n_chans,\n",
    "                    input_window_samples=1751,\n",
    "                    )\n",
    "            if network == \"Deep4Net\":\n",
    "                model = Deep4Net(\n",
    "                    n_chans,\n",
    "                    n_classes,\n",
    "                    input_window_samples=1751,\n",
    "                    final_conv_length='auto',\n",
    "                    n_filters_time=25,\n",
    "                    n_filters_spat=25,\n",
    "                    filter_time_length=10,\n",
    "                    pool_time_length=3,\n",
    "                    pool_time_stride=3,\n",
    "                    n_filters_2=50,\n",
    "                    filter_length_2=10,\n",
    "                    n_filters_3=100,\n",
    "                    filter_length_3=10,\n",
    "                    n_filters_4=200,\n",
    "                    filter_length_4=10,\n",
    "                    first_pool_mode=\"max\",\n",
    "                    later_pool_mode=\"max\",\n",
    "                    drop_prob=0.5,\n",
    "                    #double_time_convs=False,\n",
    "                    split_first_layer=True,\n",
    "                    batch_norm=True,\n",
    "                    batch_norm_alpha=0.1,\n",
    "                    stride_before_pool=False\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "            params = {\"Subject number\":subject_id,\n",
    "                      \"learning_rate\": lr ,\n",
    "                      \"optimizer\": \"AdamW\" ,\n",
    "                      \"Network\": network,\n",
    "                      \"Datasets\":\"BNCI2014001+Synthetic_diff_S20_size_{0}.\".format(percent),\n",
    "                      \"sfreq\":250,\n",
    "                      \"Class number\":2,\n",
    "                      \"Channel number\": 8,\n",
    "                      \"samples point\" : X_train.shape[2]\n",
    "\n",
    "                      }\n",
    "            run[\"parameters\"] = params\n",
    "            net = model.cuda(0)\n",
    "            optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "            criterion = nn.CrossEntropyLoss()#nn.BCELoss()#\n",
    "            train_loss,valid_loss,train_accuracy,valid_accuracy,part_weights =train(\n",
    "                                                                        model = net,\n",
    "                                                                        gpu_num = 0,\n",
    "                                                                        train_loader = train_loader,\n",
    "                                                                        test_loader = test_loader,\n",
    "                                                                        optimizer = optimizer  ,\n",
    "                                                                        criterion = criterion,\n",
    "                                                                        num_epochs=n_epochs,\n",
    "                                                                        save_weights= True,\n",
    "                                                                        lr=lr\n",
    "                                                                             )\n",
    "            model.load_state_dict(torch.load(part_weights))\n",
    "            eval(model = net,\n",
    "                gpu_num = 0,\n",
    "                valid_loader= test_loader,\n",
    "                 labels=labels,\n",
    "                 )\n",
    "\n",
    "\n",
    "            run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5437aee-2405-48db-9a8e-55b502d84345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nutapolt/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from braindecode.preprocessing import (\n",
    "    exponential_moving_standardize, \n",
    "    preprocess, \n",
    "    Preprocessor)\n",
    "from numpy import multiply\n",
    "from braindecode.datasets import MOABBDataset\n",
    "from braindecode.preprocessing import create_windows_from_events\n",
    "import torch\n",
    "from braindecode.util import set_random_seeds\n",
    "\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from braindecode.visualization import plot_confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import os\n",
    "import sys\n",
    "from mne.datasets import eegbci\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from mne.datasets import eegbci\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import wandb\n",
    "import os\n",
    "import neptune.new.integrations.sklearn as npt_utils\n",
    "import neptune.new as neptune\n",
    "\n",
    "os.environ['http_proxy'] = \"http://192.41.170.23:3128\"\n",
    "os.environ['https_proxy'] = \"http://192.41.170.23:3128\"\n",
    "\n",
    "from braindecode.models import ShallowFBCSPNet, Deep4Net ,EEGNetv4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def create_dataloader(X, y, batch_size):\n",
    "    X_tensor = torch.tensor(X).float()\n",
    "    y_tensor = torch.tensor(y).long()\n",
    "    dataset_tensor = TensorDataset(X_tensor, y_tensor)\n",
    "    dl = torch.utils.data.DataLoader(dataset_tensor, batch_size=batch_size, shuffle=True\n",
    "                                     )\n",
    "    return dl\n",
    "\n",
    "def early_stopping(train_loss, validation_loss, min_delta, tolerance):\n",
    "    counter = 0\n",
    "    if (validation_loss - train_loss) > min_delta:\n",
    "        counter +=1\n",
    "        if counter >= tolerance:\n",
    "          return True\n",
    "\n",
    "def extrack_dataset(dataset):\n",
    "    for x, y, window_ind in dataset:\n",
    "        x_shape = x.shape\n",
    "        y_shape = len(dataset.get_metadata().target)\n",
    "        break\n",
    "    X = np.zeros((y_shape,x_shape[0],x_shape[1]))\n",
    "    y_=[]\n",
    "    i=0\n",
    "    for x, y, window_ind in dataset:\n",
    "        X[i]=x\n",
    "        y_.append(y)\n",
    "        i+=1\n",
    "    #X2 = X[:, 7:8, :]\n",
    "    #X3= X[:, 11:12, :]\n",
    "    #(288, 22, 1125)\n",
    "    #X = np.concatenate((X2,X3), axis=1)\n",
    "    print(X.shape)\n",
    "    X = X.reshape(y_shape,X.shape[2],X.shape[1])\n",
    "    #X = X.reshape(y_shape,X.shape[2],X.shape[1])\n",
    "    return X,np.array(y_).T\n",
    "\n",
    "\n",
    "def extrack_dataset_2class(dataset):\n",
    "    for x, y, window_ind in dataset:\n",
    "        x_shape = x.shape\n",
    "        y_shape = len(dataset.get_metadata().target)\n",
    "        break\n",
    "    X = np.zeros((y_shape,x_shape[0],x_shape[1]))\n",
    "    y_=[]\n",
    "    i=0\n",
    "    for x, y, window_ind in dataset:\n",
    "        X[i]=x\n",
    "        y_.append(y)\n",
    "        i+=1\n",
    "    y = np.array(y_).T\n",
    "    X_ = np.zeros((int(y_shape/2) ,x_shape[0],x_shape[1])) #for 2 class\n",
    "    y_for_2class =[]\n",
    "    j=0\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == 0 or y[i] == 1:\n",
    "            y_for_2class.append(y[i])\n",
    "            X_[j]=X[i]\n",
    "            j +=1\n",
    "    y_for_2class = np.array(y_for_2class).T\n",
    "    X_ = X_.reshape(y_for_2class.shape[0],X_.shape[2],X_.shape[1])\n",
    "    return X_,y_for_2class\n",
    "\n",
    "def extrack_dataset_2class_cut(dataset):\n",
    "    for x, y, window_ind in dataset:\n",
    "        x_shape = x.shape\n",
    "        y_shape = len(dataset.get_metadata().target)\n",
    "        break\n",
    "    X = np.zeros((y_shape,x_shape[0],x_shape[1]))\n",
    "    y_=[]\n",
    "    i=0\n",
    "    for x, y, window_ind in dataset:\n",
    "        X[i]=x\n",
    "        y_.append(y)\n",
    "        i+=1\n",
    "    y = np.array(y_).T\n",
    "    X_ = np.zeros((int(y_shape/2) ,x_shape[0],x_shape[1])) #for 2 class\n",
    "    y_for_2class =[]\n",
    "    j=0\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == 0 or y[i] == 1:\n",
    "            y_for_2class.append(y[i])\n",
    "            X_[j]=X[i]\n",
    "            j +=1\n",
    "    y_for_2class = np.array(y_for_2class).T\n",
    "    X_ = X_.reshape(y_for_2class.shape[0],X_.shape[2],X_.shape[1])\n",
    "    #100, 1, 576, 22\n",
    "    X_ = X_[:,64:320,:]\n",
    "    print(X_.shape)\n",
    "    return X_,y_for_2class\n",
    "\n",
    "\n",
    "def train(model,gpu_num,train_loader,test_loader,\n",
    "          weights_name=False,\n",
    "          optimizer = None,\n",
    "          criterion = None,\n",
    "          num_epochs=500,\n",
    "          vail_loader= None,\n",
    "          save_weights = False,\n",
    "          neptune = True,\n",
    "          lr = None\n",
    "         ):\n",
    "\n",
    "    # Train the model\n",
    "\n",
    "\n",
    "    train_loss = []\n",
    "    valid_loss = [10,11]\n",
    "    train_accuracy = []\n",
    "    valid_accuracy = []\n",
    "\n",
    "    old_loss = 100\n",
    "    old_acc = 0\n",
    "    valid_loss_vail = []\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        iter_loss = 0.0\n",
    "        correct = 0\n",
    "        iterations = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for i, (items, classes) in enumerate(train_loader):\n",
    "            items = Variable(items)\n",
    "            classes = classes.type(torch.LongTensor)\n",
    "            classes = Variable(classes)\n",
    "\n",
    "            if cuda.is_available():\n",
    "                items = items.cuda(gpu_num)\n",
    "                classes = classes.cuda(gpu_num)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(items)\n",
    "            #avg_pedic = torch.mean(outputs, 1, True).float()\n",
    "            #avg_pedic = avg_pedic.reshape(classes.shape[0])\n",
    "            #print(avg_pedic.shape)\n",
    "            #print(classes.shape)\n",
    "            loss = criterion(outputs, classes)\n",
    "\n",
    "            iter_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            metrics = {\"train/train_loss\": loss}\n",
    "\n",
    "            #print(loss)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == classes.data).sum()\n",
    "            iterations += 1\n",
    "\n",
    "        train_loss.append(iter_loss/iterations)\n",
    "\n",
    "\n",
    "        train_accuracy.append(( correct.float() / len(train_loader.dataset)))\n",
    "        train_metrics = {\"train/train_loss\": iter_loss/iterations,\n",
    "                       \"train/train_accuracy\": (100 * correct.float() / len(train_loader.dataset))}\n",
    "\n",
    "\n",
    "\n",
    "        loss = 0.0\n",
    "        correct = 0\n",
    "        iterations = 0\n",
    "        model.eval()\n",
    "\n",
    "        for i, (items, classes) in enumerate(test_loader):\n",
    "            classes = classes.type(torch.LongTensor)\n",
    "            items = Variable(items)\n",
    "            classes = Variable(classes)\n",
    "\n",
    "            if cuda.is_available():\n",
    "                items = items.cuda(gpu_num)\n",
    "                classes = classes.cuda(gpu_num)\n",
    "\n",
    "\n",
    "            outputs = model(items)\n",
    "            #avg_pedic = torch.mean(outputs, 1, True).float()\n",
    "            #avg_pedic = avg_pedic.reshape(classes.shape[0])\n",
    "\n",
    "            loss += criterion(outputs, classes).item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            correct += (predicted == classes.data).sum()\n",
    "            #print(\"correct : {}\".format(classes.data))\n",
    "            #print(\"predicted : {}\".format(predicted))\n",
    "            iterations += 1\n",
    "\n",
    "        valid_loss.append(loss/iterations)\n",
    "        correct_scalar = np.array([correct.clone().cpu()])[0]\n",
    "        valid_accuracy.append(correct_scalar / len(test_loader.dataset) )\n",
    "\n",
    "        test_metrics = {\"Test/Test_loss\": loss/iterations,\n",
    "                       \"Test/Test_accuracy\": correct_scalar / len(test_loader.dataset) }\n",
    "        if save_weights is True:\n",
    "            if epoch+1 > 2 and valid_loss[-1] < old_loss and old_acc <= valid_accuracy[-1] :\n",
    "                    newpath = r'./{}'.format(weights_name)\n",
    "                    if not os.path.exists(newpath):\n",
    "                        os.makedirs(newpath)\n",
    "                    torch.save(model.state_dict(),'./{}/{:.4f}_{}_{:.4f}_{:.4f}'.format(weights_name,valid_loss[-1],weights_name,valid_loss[-1],valid_accuracy[-1]))\n",
    "                    part_weights = './{}/{:.4f}_{}_{:.4f}_{:.4f}'.format(weights_name,valid_loss[-1],weights_name,valid_loss[-1],valid_accuracy[-1])\n",
    "                    old_loss = valid_loss[-1]\n",
    "                    old_acc = valid_accuracy[-1]\n",
    "\n",
    "        print ('Epoch %d/%d, Tr Loss: %.4f, Tr Acc: %.4f, Val Loss: %.4f, Val Acc: %.4f , le : %f'\n",
    "                       %(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], valid_loss[-1], valid_accuracy[-1],lr))\n",
    "        if early_stopping(train_loss[-1], valid_loss[-1], min_delta=10, tolerance = 20):\n",
    "            print(\"We are at epoch:\", epoch+1)\n",
    "            run[f\"epoch/valid_accuracy\"].append(valid_accuracy[-1])\n",
    "            run[f\"epoch/train_accuracy\"].append(train_accuracy[-1])\n",
    "            run[f\"epoch/train_loss\"].append(train_loss[-1])\n",
    "            run[f\"epoch/valid_loss\"].append(valid_loss[-1])\n",
    "\n",
    "            break\n",
    "        if neptune is True:\n",
    "            run[f\"epoch/valid_accuracy\"].append(valid_accuracy[-1])\n",
    "            run[f\"epoch/train_accuracy\"].append(train_accuracy[-1])\n",
    "            run[f\"epoch/train_loss\"].append(train_loss[-1])\n",
    "            run[f\"epoch/valid_loss\"].append(valid_loss[-1])\n",
    "\n",
    "\n",
    "    return train_loss,valid_loss,train_accuracy,valid_accuracy,part_weights\n",
    "def eval(model,\n",
    "         gpu_num,\n",
    "          vail_loader= None,\n",
    "         labels=None,\n",
    "         ):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    correct=0\n",
    "    for i, (items, classes) in enumerate(vail_loader):\n",
    "        classes = classes.type(torch.LongTensor)\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        if cuda.is_available():\n",
    "            items = items.cuda(gpu_num)\n",
    "            classes = classes.cuda(gpu_num)\n",
    "\n",
    "        outputs = model(items)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.append(predicted.clone().cpu().numpy())\n",
    "        y_true.append(classes.data.clone().cpu().numpy())\n",
    "        correct += (predicted == classes.data).sum()\n",
    "    correct_scalar = np.array([correct.clone().cpu()])[0]\n",
    "    valid_accuracy.append(correct_scalar / len(vail_loader.dataset) )\n",
    "\n",
    "    confusion_mat = confusion_matrix(np.array(y_true).T,np.array(y_pred).T )\n",
    "    run[f\"epoch/eval_ACC\"].append(valid_accuracy[-1])\n",
    "    run[\"confusion matrices subject_id : {0}\".format(subject_id)].upload(plot_confusion_matrix(confusion_mat, class_names=labels,rotate_row_labels=0,rotate_col_labels=90,with_f1_score=True))\n",
    "    print(confusion_mat)\n",
    "    return y_pred,y_true,correct_scalar,valid_accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = 1125\n",
    "\n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, 22), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "\n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "\n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "\n",
    "        # FC Layer\n",
    "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "        # I have 120 timepoints.\n",
    "        self.fc1 = nn.Linear(560, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = self.pooling2(x)\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = self.pooling3(x)\n",
    "\n",
    "        # FC Layer\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc9d1e3-3e9a-43e5-9db8-5cd28e492c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = list(range(1,10))\n",
    "\n",
    "low_cut_hz = 8.  # low cut frequency for filtering\n",
    "high_cut_hz = 35.  # high cut frequency for filtering\n",
    "resample = 128\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "# Factor to convert from V to uV\n",
    "factor = 1e6\n",
    "preprocessors = [\n",
    "                    Preprocessor('pick_types', eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
    "                    Preprocessor(lambda data: multiply(data, factor)),  # Convert from V to uV\n",
    "                    Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n",
    "                    #Preprocessor('resample', sfreq=resample),\n",
    "                    Preprocessor(exponential_moving_standardize,  # Exponential movin standardization\n",
    "                                factor_new=factor_new,\n",
    "                                 init_block_size=init_block_size)\n",
    "                    ]\n",
    "n_epochs = 5000\n",
    "lr = 0.001\n",
    "\n",
    "weight_decay = 0.5 * 0.001\n",
    "task_list = ['left', 'right', 'foot', 'tongue']\n",
    "#task_list = ['tongue']\n",
    "network_list = ['ATCNet','EEGITNet','ShallowFBCSPNet','Deep4Net']\n",
    "\n",
    "#network_list = ['EEGITNet']\n",
    "percent_list = [18,36,54,72]\n",
    "#percent_list = [54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for subject_id in subjects:\n",
    "    for percent in percent_list:\n",
    "\n",
    "        run = neptune.init_run(\n",
    "        project=\"AitBrainLab/Synthetic\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhMTMyMzg0My02NzlhLTQ3N2ItYTdmMS0yNTcwNDBmM2QwM2QifQ==\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "        dataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids= [subject_id])\n",
    "        preprocess(dataset, preprocessors)\n",
    "        trial_start_offset_seconds = -0.5\n",
    "        # Extract sampling frequency, check that they are same in all datasets\n",
    "        sfreq = dataset.datasets[0].raw.info['sfreq']\n",
    "        print(sfreq)\n",
    "        assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
    "        # Calculate the trial start offset in samples.\n",
    "        trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
    "\n",
    "        # Create windows using braindecode function for this. It needs parameters to define how\n",
    "        # trials should be used.\n",
    "        windows_dataset = create_windows_from_events(\n",
    "            dataset,\n",
    "            trial_start_offset_samples=trial_start_offset_samples,\n",
    "            trial_stop_offset_samples=0,\n",
    "            preload=True,\n",
    "        )\n",
    "        splitted = windows_dataset.split('session')\n",
    "        train_set = splitted['session_T']\n",
    "        valid_set = splitted['session_E']\n",
    "\n",
    "        input_window_samples = train_set[0][0].shape[1]\n",
    "        X_train,y_train = extrack_dataset(train_set)\n",
    "        X_train = X_train[:, np.newaxis,:,:]\n",
    "\n",
    "        X_valid,y_valid = extrack_dataset(valid_set)\n",
    "        X_syntheic = np.load(\"Synthetic_data_shift/X_subject_{0}.npy\".format(subject_id)) #np.load(\"Synthetic_data/X_subject_3_20_[0, 1].npy\")#\n",
    "        y_syntheic = np.load(\"Synthetic_data_shift/y_subject_{0}.npy\".format(subject_id))#np.load(\"Synthetic_data/y_subject_3_20_[0, 1].npy\")#t))#np.load(\"Synthetic_data/y_subject_3_20_[0, 1].npy\")#\n",
    "        #['left', 'right', 'foot', 'tongue']\n",
    "        if len(task_list) == 4:\n",
    "            for task in task_list :\n",
    "                if task == \"left\":\n",
    "                    #X = X.reshape(y_shape,X.shape[2],X.shape[1])\n",
    "                    y1 = y_syntheic[np.random.randint(0,72,percent)]\n",
    "                    X1 = X_syntheic[np.random.randint(0,72,percent)]\n",
    "                    X1=X1.reshape(y1.shape[0],X1.shape[2],X1.shape[1])\n",
    "                if task == \"right\":\n",
    "                    y2 = y_syntheic[np.random.randint(72,144,percent)]\n",
    "                    X2 = X_syntheic[np.random.randint(72,144,percent)]\n",
    "                    X2=X2.reshape(y2.shape[0],X2.shape[2],X2.shape[1])\n",
    "                if task == \"foot\":\n",
    "                    y3 = y_syntheic[np.random.randint(144,216,percent)]\n",
    "                    X3 = X_syntheic[np.random.randint(144,216,percent)]\n",
    "                    X3=X3.reshape(y3.shape[0],X3.shape[2],X3.shape[1])\n",
    "                if task == \"tongue\":\n",
    "                    y4 = y_syntheic[np.random.randint(216,288,percent)]\n",
    "                    X4 = X_syntheic[np.random.randint(216,288,percent)]\n",
    "                    X4=X4.reshape(y4.shape[0],X4.shape[2],X4.shape[1])\n",
    "            X_syntheic = np.concatenate((X1,X2,X3,X4),axis=0)\n",
    "            y_syntheic = np.concatenate((y1,y2,y3,y4),axis=0)\n",
    "            X_syntheic = X_syntheic[:, np.newaxis,:,:]\n",
    "        if len(task_list)==1:\n",
    "            for task in task_list :\n",
    "                if task == \"left\":\n",
    "                    X_syntheic = X_syntheic[np.random.randint(0,72,percent)]\n",
    "                    y_syntheic = y_syntheic[np.random.randint(0,72,percent)]\n",
    "                if task == \"right\":\n",
    "                    X_syntheic = X_syntheic[np.random.randint(72,144,percent)]\n",
    "                    y_syntheic = y_syntheic[np.random.randint(72,144,percent)]\n",
    "                if task == \"foot\":\n",
    "                    X_syntheic = X_syntheic[np.random.randint(144,216,percent)]\n",
    "                    y_syntheic = y_syntheic[np.random.randint(144,216,percent)]\n",
    "                if task == \"tongue\":\n",
    "                    X_syntheic = X_syntheic[np.random.randint(216,288,percent)]\n",
    "                    y_syntheic = y_syntheic[np.random.randint(216,288,percent)]\n",
    "            X_syntheic=X_syntheic.reshape(y_syntheic.shape[0],X_syntheic.shape[2],X_syntheic.shape[1])\n",
    "            X_syntheic = X_syntheic[:, np.newaxis,:,:]\n",
    "            X_train = np.concatenate((X_train,X_syntheic),axis = 0)\n",
    "            y_train = np.concatenate((y_train, y_syntheic),axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        X_valid = X_valid[:, np.newaxis,:,:]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_train,y_train, test_size=0.3,stratify=y_train)\n",
    "        label_dict = valid_set.datasets[0].windows.event_id.items()\n",
    "        labels =list(dict(sorted(list(label_dict), key=lambda kv: kv[1])).keys()) #['left_hand', 'right_hand']#list(dict(sorted(list(label_dict), key=lambda kv: kv[1])).keys())\n",
    "        print('train size',X_train.shape, y_train.shape)\n",
    "        print('test size',X_test.shape, y_test.shape)\n",
    "\n",
    "        batch_size = X_train.shape[2]\n",
    "\n",
    "        train_loader = create_dataloader(X_train, y_train, batch_size=batch_size)\n",
    "        test_loader = create_dataloader(X_test, y_test, batch_size=batch_size)\n",
    "        valid_loader = create_dataloader(X_valid, y_valid, batch_size=batch_size)\n",
    "        n_classes=4\n",
    "        n_chans = X_train.shape[1]\n",
    "\n",
    "\n",
    "        input_window_samples = train_set[0][0].shape[1]\n",
    "        model = EEGNet()\n",
    "                #EEGNetv4(\n",
    "                 #   n_chans,\n",
    "                  #  n_classes,\n",
    "                   # final_conv_length='auto',\n",
    "                   # input_window_samples=input_window_samples,\n",
    "                   # )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        params = {\"Subject number\":subject_id,\n",
    "                      \"learning_rate\": lr ,\n",
    "                      \"optimizer\": \"AdamW\" ,\n",
    "                      \"Network\": \"EEGNetv4\",\n",
    "                      \"Datasets\":\"BNCI2014001+Synthetic_size_{0}.\".format(percent),\n",
    "                      \"sfreq\":dataset.datasets[0].raw.info['sfreq'],\n",
    "                      \"Class number\":n_classes,\n",
    "                      \"Channel number\": train_set[0][0].shape[0],\n",
    "                      \"samples point\" : X_train.shape[2]\n",
    "\n",
    "                      }\n",
    "        run[\"parameters\"] = params\n",
    "\n",
    "        net = model.cuda(0)\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()#nn.BCELoss()#\n",
    "        train_loss,valid_loss,train_accuracy,valid_accuracy,part_weights =train(\n",
    "                                                                    model = net,\n",
    "                                                                    gpu_num = 0,\n",
    "                                                                    train_loader = train_loader,\n",
    "                                                                    test_loader = test_loader,\n",
    "                                                                    optimizer = optimizer  ,\n",
    "                                                                    criterion = criterion,\n",
    "                                                                    num_epochs=n_epochs,\n",
    "                                                                    save_weights= True,\n",
    "                                                                    lr=lr\n",
    "                                                                         )\n",
    "        model.load_state_dict(torch.load(part_weights))\n",
    "        eval(model = net,\n",
    "            gpu_num = 0,\n",
    "              vail_loader= valid_loader,\n",
    "             labels=labels,\n",
    "             )\n",
    "\n",
    "\n",
    "        run.stop()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35 13 14 10]\n",
      " [ 2 54 13  3]\n",
      " [15 15 31 11]\n",
      " [27  5  4 36]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nutapolt/.local/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "([array([0, 2, 3, 1, 1, 3, 0, 1, 3, 1, 2, 2, 2, 3, 2, 0, 1, 0, 3, 1, 0, 0,\n         3, 0, 1, 3, 1, 2, 1, 3, 1, 1, 3, 0, 2, 0, 1, 1, 2, 2, 2, 1, 0, 1,\n         3, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 3, 1, 1, 0, 1, 1, 3, 1, 0, 2,\n         0, 1, 0, 3, 2, 2, 0, 0, 0, 0, 2, 0, 1, 2, 1, 2, 0, 1, 2, 0, 3, 0,\n         3, 2, 0, 2, 0, 2, 0, 3, 2, 1, 1, 2, 2, 2, 0, 0, 0, 0, 0, 2, 0, 1,\n         1, 1, 2, 0, 3, 3, 2, 3, 1, 2, 2, 0, 0, 0, 1, 1, 3, 3, 0, 2, 3, 1,\n         1, 3, 0, 0, 1, 2, 0, 1, 0, 3, 2, 3, 3, 0, 3, 2, 0, 3, 3, 0, 0, 2,\n         1, 1, 3, 1, 1, 2, 1, 1, 1, 2, 2, 0, 0, 1, 1, 3, 3, 3, 3, 1, 3, 3,\n         0, 2, 2, 3, 1, 3, 1, 3, 3, 1, 0, 2, 3, 3, 2, 0, 1, 1, 0, 3, 2, 1,\n         0, 1, 3, 0, 0, 0, 1, 0, 0, 1, 2, 2, 1, 1, 0, 3, 1, 0, 2, 2, 0, 2,\n         0, 3, 1, 2, 2, 3, 2, 3, 1, 1, 2, 1, 2, 0, 3, 0, 3, 2, 1, 1, 2, 0,\n         0, 1, 1, 1, 3, 0, 3, 1, 2, 3, 1, 2, 3, 1, 1, 1, 2, 0, 0, 2, 2, 0,\n         2, 0, 3, 1, 0, 2, 1, 3, 1, 3, 0, 3, 0, 1, 0, 3, 0, 0, 2, 1, 3, 0,\n         1, 1])],\n [array([2, 1, 3, 1, 1, 3, 0, 1, 3, 1, 0, 1, 1, 3, 2, 2, 0, 3, 3, 1, 3, 1,\n         0, 3, 1, 1, 1, 0, 1, 0, 2, 1, 3, 3, 2, 2, 2, 0, 1, 0, 1, 1, 3, 1,\n         2, 0, 1, 1, 0, 2, 1, 2, 3, 0, 0, 1, 3, 1, 0, 3, 1, 1, 3, 3, 2, 0,\n         3, 2, 3, 3, 3, 2, 3, 0, 0, 2, 3, 0, 3, 2, 1, 2, 3, 2, 3, 0, 3, 0,\n         2, 2, 0, 1, 0, 2, 0, 2, 2, 1, 1, 0, 2, 2, 0, 3, 3, 0, 0, 2, 1, 1,\n         2, 1, 2, 3, 3, 3, 2, 2, 3, 1, 2, 0, 3, 2, 1, 1, 3, 3, 3, 2, 3, 2,\n         1, 3, 2, 2, 1, 1, 3, 1, 2, 0, 2, 2, 3, 3, 2, 2, 0, 3, 2, 0, 3, 1,\n         1, 1, 3, 1, 0, 0, 1, 0, 0, 2, 2, 0, 3, 1, 2, 0, 3, 2, 3, 1, 3, 0,\n         3, 1, 2, 3, 1, 0, 1, 3, 1, 2, 3, 2, 0, 0, 2, 0, 1, 0, 2, 0, 0, 1,\n         0, 2, 3, 3, 0, 0, 1, 0, 2, 2, 2, 2, 1, 0, 0, 3, 1, 0, 2, 2, 0, 2,\n         3, 3, 1, 3, 1, 1, 0, 3, 1, 1, 2, 1, 2, 0, 3, 3, 0, 1, 2, 1, 0, 0,\n         0, 2, 1, 1, 2, 0, 3, 0, 2, 3, 2, 2, 3, 1, 2, 1, 0, 0, 0, 1, 0, 2,\n         0, 0, 3, 1, 2, 0, 1, 2, 1, 2, 0, 3, 3, 1, 2, 3, 3, 0, 0, 3, 3, 0,\n         0, 3])],\n 156,\n [0.26436781609195403,\n  0.3563218390804598,\n  0.3333333333333333,\n  0.3333333333333333,\n  0.2988505747126437,\n  0.3218390804597701,\n  0.3448275862068966,\n  0.3563218390804598,\n  0.3333333333333333,\n  0.3563218390804598,\n  0.3563218390804598,\n  0.28735632183908044,\n  0.42528735632183906,\n  0.4827586206896552,\n  0.28735632183908044,\n  0.3448275862068966,\n  0.4367816091954023,\n  0.42528735632183906,\n  0.3448275862068966,\n  0.40229885057471265,\n  0.41379310344827586,\n  0.367816091954023,\n  0.3793103448275862,\n  0.3333333333333333,\n  0.3448275862068966,\n  0.367816091954023,\n  0.41379310344827586,\n  0.3333333333333333,\n  0.3563218390804598,\n  0.3563218390804598,\n  0.367816091954023,\n  0.42528735632183906,\n  0.367816091954023,\n  0.3793103448275862,\n  0.367816091954023,\n  0.3218390804597701,\n  0.39080459770114945,\n  0.40229885057471265,\n  0.3793103448275862,\n  0.39080459770114945,\n  0.3793103448275862,\n  0.42528735632183906,\n  0.41379310344827586,\n  0.41379310344827586,\n  0.42528735632183906,\n  0.5057471264367817,\n  0.45977011494252873,\n  0.4367816091954023,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.4482758620689655,\n  0.47126436781609193,\n  0.47126436781609193,\n  0.4367816091954023,\n  0.5057471264367817,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.5287356321839081,\n  0.41379310344827586,\n  0.5057471264367817,\n  0.45977011494252873,\n  0.4482758620689655,\n  0.42528735632183906,\n  0.42528735632183906,\n  0.4942528735632184,\n  0.47126436781609193,\n  0.4482758620689655,\n  0.47126436781609193,\n  0.5057471264367817,\n  0.4367816091954023,\n  0.47126436781609193,\n  0.4827586206896552,\n  0.4942528735632184,\n  0.5172413793103449,\n  0.47126436781609193,\n  0.5287356321839081,\n  0.5172413793103449,\n  0.4827586206896552,\n  0.4827586206896552,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.5287356321839081,\n  0.4482758620689655,\n  0.4942528735632184,\n  0.5057471264367817,\n  0.4482758620689655,\n  0.4827586206896552,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.5172413793103449,\n  0.5287356321839081,\n  0.5172413793103449,\n  0.5057471264367817,\n  0.4827586206896552,\n  0.4942528735632184,\n  0.47126436781609193,\n  0.45977011494252873,\n  0.47126436781609193,\n  0.4942528735632184,\n  0.5057471264367817,\n  0.4482758620689655,\n  0.42528735632183906,\n  0.5172413793103449,\n  0.5057471264367817,\n  0.5517241379310345,\n  0.47126436781609193,\n  0.45977011494252873,\n  0.47126436781609193,\n  0.5057471264367817,\n  0.47126436781609193,\n  0.5057471264367817,\n  0.4482758620689655,\n  0.5402298850574713,\n  0.5287356321839081,\n  0.4827586206896552,\n  0.5057471264367817,\n  0.5402298850574713,\n  0.45977011494252873,\n  0.4942528735632184,\n  0.5057471264367817,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.5057471264367817,\n  0.4942528735632184,\n  0.4827586206896552,\n  0.4827586206896552,\n  0.45977011494252873,\n  0.4827586206896552,\n  0.5287356321839081,\n  0.4942528735632184,\n  0.4827586206896552,\n  0.47126436781609193,\n  0.47126436781609193,\n  0.45977011494252873,\n  0.4942528735632184,\n  0.5172413793103449,\n  0.4942528735632184,\n  0.4827586206896552,\n  0.4942528735632184,\n  0.5402298850574713,\n  0.47126436781609193,\n  0.47126436781609193,\n  0.5402298850574713,\n  0.4827586206896552,\n  0.5287356321839081,\n  0.4367816091954023,\n  0.5057471264367817,\n  0.5287356321839081,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.42528735632183906,\n  0.4827586206896552,\n  0.5287356321839081,\n  0.47126436781609193,\n  0.45977011494252873,\n  0.5977011494252874,\n  0.42528735632183906,\n  0.45977011494252873,\n  0.5172413793103449,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.47126436781609193,\n  0.45977011494252873,\n  0.5517241379310345,\n  0.4482758620689655,\n  0.4827586206896552,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.47126436781609193,\n  0.5057471264367817,\n  0.5057471264367817,\n  0.47126436781609193,\n  0.4942528735632184,\n  0.3793103448275862,\n  0.45977011494252873,\n  0.4942528735632184,\n  0.45977011494252873,\n  0.47126436781609193,\n  0.5287356321839081,\n  0.4367816091954023,\n  0.42528735632183906,\n  0.5057471264367817,\n  0.4942528735632184,\n  0.4827586206896552,\n  0.47126436781609193,\n  0.47126436781609193,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.47126436781609193,\n  0.45977011494252873,\n  0.4942528735632184,\n  0.47126436781609193,\n  0.4942528735632184,\n  0.5057471264367817,\n  0.5057471264367817,\n  0.45977011494252873,\n  0.41379310344827586,\n  0.5402298850574713,\n  0.45977011494252873,\n  0.4827586206896552,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.5172413793103449,\n  0.4827586206896552,\n  0.47126436781609193,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.4827586206896552,\n  0.5172413793103449,\n  0.4827586206896552,\n  0.45977011494252873,\n  0.42528735632183906,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.4482758620689655,\n  0.4827586206896552,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.47126436781609193,\n  0.5057471264367817,\n  0.4827586206896552,\n  0.4827586206896552,\n  0.5057471264367817,\n  0.5517241379310345,\n  0.5057471264367817,\n  0.4367816091954023,\n  0.5172413793103449,\n  0.45977011494252873,\n  0.4482758620689655,\n  0.4827586206896552,\n  0.4367816091954023,\n  0.45977011494252873,\n  0.42528735632183906,\n  0.4942528735632184,\n  0.41379310344827586,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.4482758620689655,\n  0.5402298850574713,\n  0.5057471264367817,\n  0.4942528735632184,\n  0.3793103448275862,\n  0.4482758620689655,\n  0.5402298850574713,\n  0.47126436781609193,\n  0.41379310344827586,\n  0.5287356321839081,\n  0.4367816091954023,\n  0.45977011494252873,\n  0.5057471264367817,\n  0.45977011494252873,\n  0.4367816091954023,\n  0.4827586206896552,\n  0.5517241379310345,\n  0.47126436781609193,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.5057471264367817,\n  0.45977011494252873,\n  0.4942528735632184,\n  0.4367816091954023,\n  0.5057471264367817,\n  0.4942528735632184,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.4827586206896552,\n  0.4827586206896552,\n  0.47126436781609193,\n  0.45977011494252873,\n  0.41379310344827586,\n  0.41379310344827586,\n  0.4827586206896552,\n  0.47126436781609193,\n  0.40229885057471265,\n  0.4942528735632184,\n  0.5057471264367817,\n  0.45977011494252873,\n  0.4367816091954023,\n  0.4482758620689655,\n  0.4942528735632184,\n  0.45977011494252873,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.5057471264367817,\n  0.45977011494252873,\n  0.5172413793103449,\n  0.4827586206896552,\n  0.47126436781609193,\n  0.45977011494252873,\n  0.4827586206896552,\n  0.4827586206896552,\n  0.39080459770114945,\n  0.4482758620689655,\n  0.47126436781609193,\n  0.4942528735632184,\n  0.4367816091954023,\n  0.42528735632183906,\n  0.4367816091954023,\n  0.4827586206896552,\n  0.45977011494252873,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.5172413793103449,\n  0.4367816091954023,\n  0.40229885057471265,\n  0.4827586206896552,\n  0.41379310344827586,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.4482758620689655,\n  0.4367816091954023,\n  0.47126436781609193,\n  0.4827586206896552,\n  0.45977011494252873,\n  0.5057471264367817,\n  0.4367816091954023,\n  0.4827586206896552,\n  0.47126436781609193,\n  0.3448275862068966,\n  0.5057471264367817,\n  0.4367816091954023,\n  0.5172413793103449,\n  0.4942528735632184,\n  0.4367816091954023,\n  0.5287356321839081,\n  0.5172413793103449,\n  0.45977011494252873,\n  0.4942528735632184,\n  0.4367816091954023,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.4367816091954023,\n  0.5517241379310345,\n  0.41379310344827586,\n  0.4482758620689655,\n  0.4942528735632184,\n  0.47126436781609193,\n  0.42528735632183906,\n  0.47126436781609193,\n  0.4482758620689655,\n  0.5057471264367817,\n  0.45977011494252873,\n  0.47126436781609193,\n  0.5287356321839081,\n  0.45977011494252873,\n  0.5287356321839081,\n  0.4827586206896552,\n  0.4827586206896552,\n  0.40229885057471265,\n  0.4367816091954023,\n  0.45977011494252873,\n  0.42528735632183906,\n  0.5057471264367817,\n  0.47126436781609193,\n  0.5057471264367817,\n  0.4942528735632184,\n  0.4482758620689655,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.47126436781609193,\n  0.4367816091954023,\n  0.47126436781609193,\n  0.47126436781609193,\n  0.4942528735632184,\n  0.45977011494252873,\n  0.4482758620689655,\n  0.4482758620689655,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.47126436781609193,\n  0.4827586206896552,\n  0.4367816091954023,\n  0.45977011494252873,\n  0.4942528735632184,\n  0.41379310344827586,\n  0.42528735632183906,\n  0.4942528735632184,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.42528735632183906,\n  0.4827586206896552,\n  0.45977011494252873,\n  0.5172413793103449,\n  0.47126436781609193,\n  0.45977011494252873,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.4482758620689655,\n  0.4827586206896552,\n  0.5057471264367817,\n  0.47126436781609193,\n  0.45977011494252873,\n  0.4942528735632184,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.4367816091954023,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.42528735632183906,\n  0.4827586206896552,\n  0.47126436781609193,\n  0.4827586206896552,\n  0.5057471264367817,\n  0.4827586206896552,\n  0.47126436781609193,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.4367816091954023,\n  0.4942528735632184,\n  0.47126436781609193,\n  0.47126436781609193,\n  0.4367816091954023,\n  0.47126436781609193,\n  0.4827586206896552,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.5057471264367817,\n  0.41379310344827586,\n  0.45977011494252873,\n  0.4367816091954023,\n  0.5057471264367817,\n  0.4482758620689655,\n  0.41379310344827586,\n  0.41379310344827586,\n  0.47126436781609193,\n  0.4942528735632184,\n  0.45977011494252873,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.5287356321839081,\n  0.45977011494252873,\n  0.5057471264367817,\n  0.41379310344827586,\n  0.4942528735632184,\n  0.4827586206896552,\n  0.4827586206896552,\n  0.4367816091954023,\n  0.4482758620689655,\n  0.40229885057471265,\n  0.4367816091954023,\n  0.47126436781609193,\n  0.45977011494252873,\n  0.47126436781609193,\n  0.42528735632183906,\n  0.4482758620689655,\n  0.4367816091954023,\n  0.5057471264367817,\n  0.42528735632183906,\n  0.45977011494252873,\n  0.4942528735632184,\n  0.4827586206896552,\n  0.47126436781609193,\n  0.4367816091954023,\n  0.5287356321839081,\n  0.42528735632183906,\n  0.4367816091954023,\n  0.4482758620689655,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.42528735632183906,\n  0.45977011494252873,\n  0.5402298850574713,\n  0.4827586206896552,\n  0.5172413793103449,\n  0.5172413793103449,\n  0.42528735632183906,\n  0.5287356321839081,\n  0.47126436781609193,\n  0.4367816091954023,\n  0.5057471264367817,\n  0.4367816091954023,\n  0.5287356321839081,\n  0.39080459770114945,\n  0.47126436781609193,\n  0.4367816091954023,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.4827586206896552,\n  0.5287356321839081,\n  0.4367816091954023,\n  0.4942528735632184,\n  0.5172413793103449,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.5632183908045977,\n  0.5057471264367817,\n  0.4482758620689655,\n  0.4482758620689655,\n  0.4367816091954023,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.40229885057471265,\n  0.40229885057471265,\n  0.4482758620689655,\n  0.4482758620689655,\n  0.5287356321839081,\n  0.4482758620689655,\n  0.5057471264367817,\n  0.42528735632183906,\n  0.4367816091954023,\n  0.47126436781609193,\n  0.47126436781609193,\n  0.5287356321839081,\n  0.4942528735632184,\n  0.4367816091954023,\n  0.5287356321839081,\n  0.42528735632183906,\n  0.4482758620689655,\n  0.5057471264367817,\n  0.4482758620689655,\n  0.5287356321839081,\n  0.45977011494252873,\n  0.4942528735632184,\n  0.5057471264367817,\n  0.47126436781609193,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.42528735632183906,\n  0.41379310344827586,\n  0.4942528735632184,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.5057471264367817,\n  0.4827586206896552,\n  0.42528735632183906,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.4827586206896552,\n  0.45977011494252873,\n  0.47126436781609193,\n  0.39080459770114945,\n  0.45977011494252873,\n  0.4367816091954023,\n  0.4482758620689655,\n  0.41379310344827586,\n  0.42528735632183906,\n  0.47126436781609193,\n  0.4827586206896552,\n  0.4367816091954023,\n  0.42528735632183906,\n  0.5172413793103449,\n  0.40229885057471265,\n  0.4367816091954023,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.4942528735632184,\n  0.4367816091954023,\n  0.4942528735632184,\n  0.5287356321839081,\n  0.42528735632183906,\n  0.47126436781609193,\n  0.42528735632183906,\n  0.42528735632183906,\n  0.4827586206896552,\n  0.41379310344827586,\n  0.4367816091954023,\n  0.47126436781609193,\n  0.4482758620689655,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.5172413793103449,\n  0.4482758620689655,\n  0.4942528735632184,\n  0.45977011494252873,\n  0.4482758620689655,\n  0.5057471264367817,\n  0.47126436781609193,\n  0.45977011494252873,\n  0.47126436781609193,\n  0.40229885057471265,\n  0.47126436781609193,\n  0.47126436781609193,\n  0.4367816091954023,\n  0.47126436781609193,\n  0.42528735632183906,\n  0.40229885057471265,\n  0.4827586206896552,\n  0.4827586206896552,\n  0.4827586206896552,\n  0.4942528735632184,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.5057471264367817,\n  0.4367816091954023,\n  0.47126436781609193,\n  0.42528735632183906,\n  0.45977011494252873,\n  0.4482758620689655,\n  0.5517241379310345,\n  0.47126436781609193,\n  0.4827586206896552,\n  0.4942528735632184,\n  0.4367816091954023,\n  0.4482758620689655,\n  0.5172413793103449,\n  0.45977011494252873,\n  0.4367816091954023,\n  0.4482758620689655,\n  0.4367816091954023,\n  0.42528735632183906,\n  0.45977011494252873,\n  0.4367816091954023,\n  0.4827586206896552,\n  0.4367816091954023,\n  0.4482758620689655,\n  0.4482758620689655,\n  0.42528735632183906,\n  0.45977011494252873,\n  0.3793103448275862,\n  0.4827586206896552,\n  0.5172413793103449,\n  0.45977011494252873,\n  0.39080459770114945,\n  0.4942528735632184,\n  0.47126436781609193,\n  0.4482758620689655,\n  0.42528735632183906,\n  0.5287356321839081,\n  0.5172413793103449,\n  0.4942528735632184,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.5172413793103449,\n  0.42528735632183906,\n  0.41379310344827586,\n  0.5172413793103449,\n  0.4482758620689655,\n  0.4942528735632184,\n  0.45977011494252873,\n  0.4827586206896552,\n  0.41379310344827586,\n  0.4482758620689655,\n  0.47126436781609193,\n  0.4827586206896552,\n  0.4367816091954023,\n  0.4367816091954023,\n  0.47126436781609193,\n  0.4482758620689655,\n  0.4482758620689655,\n  0.5057471264367817,\n  0.4942528735632184,\n  0.42528735632183906,\n  0.45977011494252873,\n  0.4942528735632184,\n  0.45977011494252873,\n  0.42528735632183906,\n  0.5172413793103449,\n  0.45977011494252873,\n  0.4367816091954023,\n  0.4482758620689655,\n  0.40229885057471265,\n  0.45977011494252873,\n  0.40229885057471265,\n  0.4827586206896552,\n  0.47126436781609193,\n  0.4367816091954023,\n  0.4942528735632184,\n  0.4367816091954023,\n  0.4827586206896552,\n  0.41379310344827586,\n  0.4367816091954023,\n  0.4482758620689655,\n  0.3793103448275862,\n  0.4942528735632184,\n  0.4827586206896552,\n  0.4942528735632184,\n  0.4367816091954023,\n  0.4827586206896552,\n  0.4367816091954023,\n  0.4942528735632184,\n  0.47126436781609193,\n  0.5057471264367817,\n  0.4367816091954023,\n  0.5287356321839081,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.42528735632183906,\n  0.41379310344827586,\n  0.41379310344827586,\n  0.4827586206896552,\n  0.47126436781609193,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.4482758620689655,\n  0.5287356321839081,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.47126436781609193,\n  0.4942528735632184,\n  0.5057471264367817,\n  0.4482758620689655,\n  0.40229885057471265,\n  0.45977011494252873,\n  0.4942528735632184,\n  0.47126436781609193,\n  0.45977011494252873,\n  0.4367816091954023,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.41379310344827586,\n  0.4367816091954023,\n  0.42528735632183906,\n  0.4367816091954023,\n  0.4482758620689655,\n  0.42528735632183906,\n  0.47126436781609193,\n  0.4827586206896552,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.4942528735632184,\n  0.39080459770114945,\n  0.45977011494252873,\n  0.4482758620689655,\n  0.5057471264367817,\n  0.5057471264367817,\n  0.5057471264367817,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.4827586206896552,\n  0.47126436781609193,\n  0.47126436781609193,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.39080459770114945,\n  0.4482758620689655,\n  0.5172413793103449,\n  0.4942528735632184,\n  0.4942528735632184,\n  0.5057471264367817,\n  0.41379310344827586,\n  0.4827586206896552,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.4367816091954023,\n  0.47126436781609193,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.4827586206896552,\n  0.4367816091954023,\n  0.4367816091954023,\n  0.5172413793103449,\n  0.4827586206896552,\n  0.4942528735632184,\n  0.39080459770114945,\n  0.42528735632183906,\n  0.41379310344827586,\n  0.47126436781609193,\n  0.47126436781609193,\n  0.39080459770114945,\n  0.4367816091954023,\n  0.4482758620689655,\n  0.4942528735632184,\n  0.4482758620689655,\n  0.4827586206896552,\n  0.45977011494252873,\n  0.42528735632183906,\n  0.42528735632183906,\n  0.42528735632183906,\n  0.4482758620689655,\n  0.47126436781609193,\n  0.4942528735632184,\n  0.39080459770114945,\n  0.4942528735632184,\n  0.5057471264367817,\n  0.47126436781609193,\n  0.45977011494252873,\n  0.42528735632183906,\n  0.41379310344827586,\n  0.4827586206896552,\n  0.41379310344827586,\n  0.4827586206896552,\n  0.3793103448275862,\n  0.42528735632183906,\n  0.41379310344827586,\n  0.4482758620689655,\n  0.4827586206896552,\n  0.45977011494252873,\n  0.42528735632183906,\n  0.4367816091954023,\n  0.5057471264367817,\n  0.4482758620689655,\n  0.4367816091954023,\n  0.45977011494252873,\n  0.5402298850574713,\n  0.41379310344827586,\n  0.45977011494252873,\n  0.4482758620689655,\n  0.4367816091954023,\n  0.45977011494252873,\n  0.4482758620689655,\n  0.4942528735632184,\n  0.4482758620689655,\n  0.41379310344827586,\n  0.4482758620689655,\n  0.5402298850574713,\n  0.4827586206896552,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.4367816091954023,\n  0.45977011494252873,\n  0.42528735632183906,\n  0.4942528735632184,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.4827586206896552,\n  0.4367816091954023,\n  0.4942528735632184,\n  0.4367816091954023,\n  0.45977011494252873,\n  0.47126436781609193,\n  0.47126436781609193,\n  0.4482758620689655,\n  0.47126436781609193,\n  0.4827586206896552,\n  0.47126436781609193,\n  0.5057471264367817,\n  0.42528735632183906,\n  0.47126436781609193,\n  0.4482758620689655,\n  0.4942528735632184,\n  0.4482758620689655,\n  0.4367816091954023,\n  0.4367816091954023,\n  0.4827586206896552,\n  0.47126436781609193,\n  0.4367816091954023,\n  0.4827586206896552,\n  0.4367816091954023,\n  0.40229885057471265,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.4942528735632184,\n  0.45977011494252873,\n  0.47126436781609193,\n  0.5517241379310345,\n  0.42528735632183906,\n  0.45977011494252873,\n  0.4482758620689655,\n  0.4942528735632184,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.4942528735632184,\n  0.45977011494252873,\n  0.39080459770114945,\n  0.42528735632183906,\n  0.4482758620689655,\n  0.4367816091954023,\n  0.5287356321839081,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.3563218390804598,\n  0.47126436781609193,\n  0.47126436781609193,\n  0.41379310344827586,\n  0.4482758620689655,\n  0.5287356321839081,\n  0.4942528735632184,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.5287356321839081,\n  0.47126436781609193,\n  0.4482758620689655,\n  0.4367816091954023,\n  0.4942528735632184,\n  0.3793103448275862,\n  0.4482758620689655,\n  0.4827586206896552,\n  0.4367816091954023,\n  0.5057471264367817,\n  0.4482758620689655,\n  0.4367816091954023,\n  0.4827586206896552,\n  0.4942528735632184,\n  0.42528735632183906,\n  0.4827586206896552,\n  0.47126436781609193,\n  0.42528735632183906,\n  0.47126436781609193,\n  0.4482758620689655,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.5057471264367817,\n  0.45977011494252873,\n  0.4827586206896552,\n  0.4367816091954023,\n  0.4827586206896552,\n  0.39080459770114945,\n  0.4367816091954023,\n  0.40229885057471265,\n  0.42528735632183906,\n  0.45977011494252873,\n  0.47126436781609193,\n  0.4827586206896552,\n  0.45977011494252873,\n  0.4942528735632184,\n  0.41379310344827586,\n  0.42528735632183906,\n  0.4942528735632184,\n  0.4827586206896552,\n  0.42528735632183906,\n  0.5057471264367817,\n  0.47126436781609193,\n  0.47126436781609193,\n  0.5057471264367817,\n  0.4827586206896552,\n  0.45977011494252873,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.4367816091954023,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.4827586206896552,\n  0.47126436781609193,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.45977011494252873,\n  0.4827586206896552,\n  0.45977011494252873,\n  0.47126436781609193,\n  0.42528735632183906,\n  0.5287356321839081,\n  0.4942528735632184,\n  0.41379310344827586,\n  0.42528735632183906,\n  0.45977011494252873,\n  0.4367816091954023,\n  0.47126436781609193,\n  0.4482758620689655,\n  0.4942528735632184,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.4482758620689655,\n  0.4482758620689655,\n  0.4827586206896552,\n  0.5172413793103449,\n  0.41379310344827586,\n  0.45977011494252873,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.5172413793103449,\n  0.4827586206896552,\n  0.4482758620689655,\n  0.4367816091954023,\n  0.4942528735632184,\n  0.41379310344827586,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.41379310344827586,\n  0.47126436781609193,\n  0.4942528735632184,\n  0.47126436781609193,\n  0.4367816091954023,\n  0.4367816091954023,\n  0.5172413793103449,\n  0.45977011494252873,\n  0.42528735632183906,\n  0.4827586206896552,\n  0.4367816091954023,\n  0.4482758620689655,\n  0.4482758620689655,\n  0.45977011494252873,\n  0.4367816091954023,\n  0.47126436781609193,\n  0.39080459770114945,\n  0.4367816091954023,\n  0.4827586206896552,\n  0.4827586206896552,\n  0.45977011494252873,\n  0.4827586206896552,\n  0.4367816091954023,\n  0.47126436781609193,\n  0.5287356321839081,\n  0.41379310344827586,\n  0.4482758620689655,\n  0.4942528735632184,\n  0.41379310344827586,\n  0.47126436781609193,\n  0.4827586206896552,\n  ...])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

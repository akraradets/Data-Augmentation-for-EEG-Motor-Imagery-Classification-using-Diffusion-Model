{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nutapolt/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from braindecode.preprocessing import (\n",
    "    exponential_moving_standardize,\n",
    "    preprocess,\n",
    "    Preprocessor)\n",
    "from numpy import multiply\n",
    "from braindecode.datasets import MOABBDataset\n",
    "from braindecode.preprocessing import create_windows_from_events\n",
    "import torch\n",
    "from braindecode.util import set_random_seeds\n",
    "\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from braindecode.visualization import plot_confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import os\n",
    "import sys\n",
    "from mne.datasets import eegbci\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from mne.datasets import eegbci\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import wandb\n",
    "import os\n",
    "import neptune.new.integrations.sklearn as npt_utils\n",
    "import neptune.new as neptune\n",
    "\n",
    "\n",
    "\n",
    "os.environ['http_proxy'] = \"http://192.41.170.23:3128\"\n",
    "os.environ['https_proxy'] = \"http://192.41.170.23:3128\"\n",
    "\n",
    "from braindecode.models import ShallowFBCSPNet, Deep4Net ,EEGNetv4,HybridNet,EEGInceptionMI,EEGITNet,ATCNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def create_dataloader(X, y, batch_size):\n",
    "    X_tensor = torch.tensor(X).float()\n",
    "    y_tensor = torch.tensor(y).long()\n",
    "    dataset_tensor = TensorDataset(X_tensor, y_tensor)\n",
    "    dl = torch.utils.data.DataLoader(dataset_tensor, batch_size=batch_size, shuffle=True)\n",
    "    return dl\n",
    "\n",
    "def early_stopping(train_loss, validation_loss, min_delta, tolerance):\n",
    "    counter = 0\n",
    "    if (validation_loss - train_loss) > min_delta:\n",
    "        counter +=1\n",
    "        if counter >= tolerance:\n",
    "          return True\n",
    "\n",
    "def extrack_dataset(dataset):\n",
    "    for x, y, window_ind in dataset:\n",
    "        x_shape = x.shape\n",
    "        y_shape = len(dataset.get_metadata().target)\n",
    "        break\n",
    "    X = np.zeros((y_shape,x_shape[0],x_shape[1]))\n",
    "    y_=[]\n",
    "    i=0\n",
    "    for x, y, window_ind in dataset:\n",
    "        X[i]=x\n",
    "        y_.append(y)\n",
    "        i+=1\n",
    "    #X2 = X[:, 7:8, :]\n",
    "    #X3= X[:, 11:12, :]\n",
    "    #(288, 22, 1125)\n",
    "    #X = np.concatenate((X2,X3), axis=1)\n",
    "    print(X.shape)\n",
    "\n",
    "    #X = X.reshape(y_shape,X.shape[2],X.shape[1])\n",
    "    return X,np.array(y_).T\n",
    "\n",
    "\n",
    "def extrack_dataset_2class(dataset):\n",
    "    for x, y, window_ind in dataset:\n",
    "        x_shape = x.shape\n",
    "        y_shape = len(dataset.get_metadata().target)\n",
    "        break\n",
    "    X = np.zeros((y_shape,x_shape[0],x_shape[1]))\n",
    "    y_=[]\n",
    "    i=0\n",
    "    for x, y, window_ind in dataset:\n",
    "        X[i]=x\n",
    "        y_.append(y)\n",
    "        i+=1\n",
    "    y = np.array(y_).T\n",
    "    X_ = np.zeros((int(y_shape/2) ,x_shape[0],x_shape[1])) #for 2 class\n",
    "    y_for_2class =[]\n",
    "    j=0\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == 0 or y[i] == 1:\n",
    "            y_for_2class.append(y[i])\n",
    "            X_[j]=X[i]\n",
    "            j +=1\n",
    "    y_for_2class = np.array(y_for_2class).T\n",
    "    X_ = X_.reshape(y_for_2class.shape[0],X_.shape[2],X_.shape[1])\n",
    "    return X_,y_for_2class\n",
    "\n",
    "def extrack_dataset_2class_cut(dataset):\n",
    "    for x, y, window_ind in dataset:\n",
    "        x_shape = x.shape\n",
    "        y_shape = len(dataset.get_metadata().target)\n",
    "        break\n",
    "    X = np.zeros((y_shape,x_shape[0],x_shape[1]))\n",
    "    y_=[]\n",
    "    i=0\n",
    "    for x, y, window_ind in dataset:\n",
    "        X[i]=x\n",
    "        y_.append(y)\n",
    "        i+=1\n",
    "    y = np.array(y_).T\n",
    "    X_ = np.zeros((int(y_shape/2) ,x_shape[0],x_shape[1])) #for 2 class\n",
    "    y_for_2class =[]\n",
    "    j=0\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == 0 or y[i] == 1:\n",
    "            y_for_2class.append(y[i])\n",
    "            X_[j]=X[i]\n",
    "            j +=1\n",
    "    y_for_2class = np.array(y_for_2class).T\n",
    "    X_ = X_.reshape(y_for_2class.shape[0],X_.shape[2],X_.shape[1])\n",
    "    #100, 1, 576, 22\n",
    "    X_ = X_[:,64:320,:]\n",
    "    print(X_.shape)\n",
    "    return X_,y_for_2class\n",
    "\n",
    "\n",
    "def train(model,gpu_num,train_loader,test_loader,\n",
    "          weights_name=False,\n",
    "          optimizer = None,\n",
    "          criterion = None,\n",
    "          num_epochs=500,\n",
    "          vail_loader= None,\n",
    "          save_weights = False,\n",
    "          neptune = True,\n",
    "          lr = None\n",
    "         ):\n",
    "\n",
    "    # Train the model\n",
    "\n",
    "\n",
    "    train_loss = []\n",
    "    valid_loss = [10,11]\n",
    "    train_accuracy = []\n",
    "    valid_accuracy = []\n",
    "\n",
    "    old_loss = 100\n",
    "    old_acc = 0\n",
    "    valid_loss_vail = []\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        iter_loss = 0.0\n",
    "        correct = 0\n",
    "        iterations = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for i, (items, classes) in enumerate(train_loader):\n",
    "            items = Variable(items)\n",
    "            classes = classes.type(torch.LongTensor)\n",
    "            classes = Variable(classes)\n",
    "\n",
    "            if cuda.is_available():\n",
    "                items = items.cuda(gpu_num)\n",
    "                classes = classes.cuda(gpu_num)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(items)\n",
    "            #avg_pedic = torch.mean(outputs, 1, True).float()\n",
    "            #avg_pedic = avg_pedic.reshape(classes.shape[0])\n",
    "            #print(avg_pedic.shape)\n",
    "            #print(classes.shape)\n",
    "            loss = criterion(outputs, classes)\n",
    "\n",
    "            iter_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            metrics = {\"train/train_loss\": loss}\n",
    "\n",
    "            #print(loss)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == classes.data).sum()\n",
    "            iterations += 1\n",
    "\n",
    "        train_loss.append(iter_loss/iterations)\n",
    "\n",
    "\n",
    "        train_accuracy.append(( correct.float() / len(train_loader.dataset)))\n",
    "        train_metrics = {\"train/train_loss\": iter_loss/iterations,\n",
    "                       \"train/train_accuracy\": (100 * correct.float() / len(train_loader.dataset))}\n",
    "\n",
    "\n",
    "\n",
    "        loss = 0.0\n",
    "        correct = 0\n",
    "        iterations = 0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for i, (items, classes) in enumerate(test_loader):\n",
    "            classes = classes.type(torch.LongTensor)\n",
    "            items = Variable(items)\n",
    "            classes = Variable(classes)\n",
    "\n",
    "            if cuda.is_available():\n",
    "                items = items.cuda(gpu_num)\n",
    "                classes = classes.cuda(gpu_num)\n",
    "\n",
    "\n",
    "            outputs = model(items)\n",
    "            #avg_pedic = torch.mean(outputs, 1, True).float()\n",
    "            #avg_pedic = avg_pedic.reshape(classes.shape[0])\n",
    "\n",
    "            loss += criterion(outputs, classes).item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            correct += (predicted == classes.data).sum()\n",
    "            #print(\"correct : {}\".format(classes.data))\n",
    "            #print(\"predicted : {}\".format(predicted))\n",
    "            iterations += 1\n",
    "\n",
    "        valid_loss.append(loss/iterations)\n",
    "        correct_scalar = np.array([correct.clone().cpu()])[0]\n",
    "        valid_accuracy.append(correct_scalar / len(test_loader.dataset) )\n",
    "\n",
    "        test_metrics = {\"Test/Test_loss\": loss/iterations,\n",
    "                       \"Test/Test_accuracy\": correct_scalar / len(test_loader.dataset) }\n",
    "\n",
    "        if save_weights is True:\n",
    "            if epoch+1 > 2 and valid_loss[-1] < old_loss and old_acc <= valid_accuracy[-1] :\n",
    "                    newpath = r'./{}'.format(weights_name)\n",
    "                    if not os.path.exists(newpath):\n",
    "                        os.makedirs(newpath)\n",
    "                    torch.save(model.state_dict(),'./{}/{:.4f}_{}_{:.4f}_{:.4f}'.format(weights_name,valid_loss[-1],weights_name,valid_loss[-1],valid_accuracy[-1]))\n",
    "                    part_weights = './{}/{:.4f}_{}_{:.4f}_{:.4f}'.format(weights_name,valid_loss[-1],weights_name,valid_loss[-1],valid_accuracy[-1])\n",
    "                    old_loss = valid_loss[-1]\n",
    "                    old_acc = valid_accuracy[-1]\n",
    "\n",
    "        print ('Epoch %d/%d, Tr Loss: %.4f, Tr Acc: %.4f, Val Loss: %.4f, Val Acc: %.4f , le : %f'\n",
    "                       %(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], valid_loss[-1], valid_accuracy[-1],lr))\n",
    "        if early_stopping(train_loss[-1], valid_loss[-1], min_delta=10, tolerance = 20):\n",
    "            print(\"We are at epoch:\", epoch+1)\n",
    "            run[f\"epoch/valid_accuracy\"].append(valid_accuracy[-1])\n",
    "            run[f\"epoch/train_accuracy\"].append(train_accuracy[-1])\n",
    "            run[f\"epoch/train_loss\"].append(train_loss[-1])\n",
    "            run[f\"epoch/valid_loss\"].append(valid_loss[-1])\n",
    "\n",
    "            break\n",
    "        if neptune is True:\n",
    "            run[f\"epoch/valid_accuracy\"].append(valid_accuracy[-1])\n",
    "            run[f\"epoch/train_accuracy\"].append(train_accuracy[-1])\n",
    "            run[f\"epoch/train_loss\"].append(train_loss[-1])\n",
    "            run[f\"epoch/valid_loss\"].append(valid_loss[-1])\n",
    "        if epoch+1 == 1:\n",
    "            stop_loss = valid_loss[-1]\n",
    "\n",
    "        if  (epoch+1)//500 == 0 and epoch+1 > 1400 :\n",
    "            if valid_loss[-1] > valid_loss[-500]:\n",
    "                print(\"Stop\")\n",
    "                break\n",
    "\n",
    "    return train_loss,valid_loss,train_accuracy,valid_accuracy,part_weights\n",
    "def eval(model,\n",
    "         gpu_num,\n",
    "          valid_loader= None,\n",
    "         labels=None,\n",
    "\n",
    "         ):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    correct=0\n",
    "    evaluate_accuracy= []\n",
    "    for i, (items, classes) in enumerate(valid_loader):\n",
    "        classes = classes.type(torch.LongTensor)\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        if cuda.is_available():\n",
    "            items = items.cuda(gpu_num)\n",
    "            classes = classes.cuda(gpu_num)\n",
    "\n",
    "        outputs = model(items)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.append(predicted.clone().cpu().numpy())\n",
    "        y_true.append(classes.data.clone().cpu().numpy())\n",
    "        correct += (predicted == classes.data).sum()\n",
    "    correct_scalar = np.array([correct.clone().cpu()])[0]\n",
    "    evaluate_accuracy.append(correct_scalar / classes.shape[0] )\n",
    "\n",
    "    confusion_mat = confusion_matrix(np.array(y_true).T,np.array(y_pred).T )\n",
    "    run[f\"epoch/eval_ACC\"].append(evaluate_accuracy[0])\n",
    "    run[\"confusion matrices subject_id : {0}\".format(subject_id)].upload(plot_confusion_matrix(confusion_mat, class_names=labels,rotate_row_labels=0,rotate_col_labels=90,with_f1_score=True))\n",
    "    plot_confusion_matrix(confusion_mat, class_names=labels,rotate_row_labels=0,rotate_col_labels=90,with_f1_score=True).savefig('confusion_matrix/confusion matrices subject_id : {2}_{0}_of_{1}.png'.format(subject_id,params['Datasets'],network))\n",
    "    return y_pred,y_true,correct_scalar,evaluate_accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nutapolt/.local/lib/python3.8/site-packages/braindecode/preprocessing/preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
      "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
     ]
    }
   ],
   "source": [
    "subjects = list(range(1,10))\n",
    "\n",
    "low_cut_hz = 8.  # low cut frequency for filtering\n",
    "high_cut_hz = 35.  # high cut frequency for filtering\n",
    "resample = 128\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "# Factor to convert from V to uV\n",
    "factor = 1e6\n",
    "preprocessors = [\n",
    "                    Preprocessor('pick_types', eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
    "                    Preprocessor(lambda data: multiply(data, factor)),  # Convert from V to uV\n",
    "                    Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n",
    "                    #Preprocessor('resample', sfreq=resample),\n",
    "                    Preprocessor(exponential_moving_standardize,  # Exponential movin standardization\n",
    "                                factor_new=factor_new,\n",
    "                                 init_block_size=init_block_size)\n",
    "                    ]\n",
    "n_epochs = 5000\n",
    "lr = 0.001\n",
    "\n",
    "weight_decay = 0.5 * 0.0001\n",
    "task_list = ['left', 'right', 'foot', 'tongue']\n",
    "#task_list = ['foot']\n",
    "network_list = ['ATCNet','EEGITNet','ShallowFBCSPNet','Deep4Net']\n",
    "\n",
    "#network_list = ['Deep4Net']\n",
    "#percent_list = [72]\n",
    "percent_list = [18,36,54,72]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/AitBrainLab/Synthetic/e/SYNTHET-579\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "250.0\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "(288, 22, 1125)\n",
      "(288, 22, 1125)\n",
      "train size (201, 22, 1125) (201,)\n",
      "test size (87, 22, 1125) (87,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nutapolt/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:453: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ../aten/src/ATen/native/Convolution.cpp:882.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000, Tr Loss: 1.4802, Tr Acc: 0.2488, Val Loss: 1.3895, Val Acc: 0.2529 , le : 0.001000\n",
      "Epoch 2/5000, Tr Loss: 1.4788, Tr Acc: 0.2438, Val Loss: 1.3925, Val Acc: 0.2529 , le : 0.001000\n",
      "Epoch 3/5000, Tr Loss: 1.4266, Tr Acc: 0.2836, Val Loss: 1.3889, Val Acc: 0.2529 , le : 0.001000\n",
      "Epoch 4/5000, Tr Loss: 1.3922, Tr Acc: 0.3184, Val Loss: 1.3821, Val Acc: 0.2529 , le : 0.001000\n",
      "Epoch 5/5000, Tr Loss: 1.4043, Tr Acc: 0.3035, Val Loss: 1.3748, Val Acc: 0.2529 , le : 0.001000\n",
      "Epoch 6/5000, Tr Loss: 1.4281, Tr Acc: 0.2836, Val Loss: 1.3673, Val Acc: 0.3333 , le : 0.001000\n",
      "Epoch 7/5000, Tr Loss: 1.3763, Tr Acc: 0.2985, Val Loss: 1.3606, Val Acc: 0.4253 , le : 0.001000\n",
      "Epoch 8/5000, Tr Loss: 1.3684, Tr Acc: 0.3333, Val Loss: 1.3555, Val Acc: 0.4598 , le : 0.001000\n",
      "Epoch 9/5000, Tr Loss: 1.3539, Tr Acc: 0.3284, Val Loss: 1.3490, Val Acc: 0.4598 , le : 0.001000\n",
      "Epoch 10/5000, Tr Loss: 1.3790, Tr Acc: 0.3483, Val Loss: 1.3413, Val Acc: 0.4138 , le : 0.001000\n",
      "Epoch 11/5000, Tr Loss: 1.3405, Tr Acc: 0.3035, Val Loss: 1.3318, Val Acc: 0.4023 , le : 0.001000\n",
      "Epoch 12/5000, Tr Loss: 1.3332, Tr Acc: 0.3532, Val Loss: 1.3195, Val Acc: 0.4023 , le : 0.001000\n",
      "Epoch 13/5000, Tr Loss: 1.3070, Tr Acc: 0.3632, Val Loss: 1.3057, Val Acc: 0.4023 , le : 0.001000\n",
      "Epoch 14/5000, Tr Loss: 1.3391, Tr Acc: 0.3383, Val Loss: 1.2893, Val Acc: 0.4138 , le : 0.001000\n",
      "Epoch 15/5000, Tr Loss: 1.3313, Tr Acc: 0.3134, Val Loss: 1.2680, Val Acc: 0.4253 , le : 0.001000\n",
      "Epoch 16/5000, Tr Loss: 1.2863, Tr Acc: 0.4080, Val Loss: 1.2424, Val Acc: 0.4598 , le : 0.001000\n",
      "Epoch 17/5000, Tr Loss: 1.2701, Tr Acc: 0.3881, Val Loss: 1.2139, Val Acc: 0.4828 , le : 0.001000\n",
      "Epoch 18/5000, Tr Loss: 1.2299, Tr Acc: 0.4577, Val Loss: 1.1791, Val Acc: 0.4828 , le : 0.001000\n",
      "Epoch 19/5000, Tr Loss: 1.2429, Tr Acc: 0.4328, Val Loss: 1.1429, Val Acc: 0.5172 , le : 0.001000\n",
      "Epoch 20/5000, Tr Loss: 1.1839, Tr Acc: 0.4527, Val Loss: 1.1043, Val Acc: 0.5287 , le : 0.001000\n",
      "Epoch 21/5000, Tr Loss: 1.2313, Tr Acc: 0.4179, Val Loss: 1.0665, Val Acc: 0.5402 , le : 0.001000\n",
      "Epoch 22/5000, Tr Loss: 1.1502, Tr Acc: 0.4776, Val Loss: 1.0320, Val Acc: 0.5517 , le : 0.001000\n",
      "Epoch 23/5000, Tr Loss: 1.1589, Tr Acc: 0.4527, Val Loss: 0.9970, Val Acc: 0.5517 , le : 0.001000\n",
      "Epoch 24/5000, Tr Loss: 1.1079, Tr Acc: 0.5075, Val Loss: 0.9649, Val Acc: 0.5287 , le : 0.001000\n",
      "Epoch 25/5000, Tr Loss: 1.1281, Tr Acc: 0.4776, Val Loss: 0.9377, Val Acc: 0.5172 , le : 0.001000\n",
      "Epoch 26/5000, Tr Loss: 1.1162, Tr Acc: 0.5124, Val Loss: 0.9133, Val Acc: 0.5057 , le : 0.001000\n",
      "Epoch 27/5000, Tr Loss: 1.0841, Tr Acc: 0.5174, Val Loss: 0.8922, Val Acc: 0.5172 , le : 0.001000\n",
      "Epoch 28/5000, Tr Loss: 1.1108, Tr Acc: 0.4577, Val Loss: 0.8739, Val Acc: 0.5172 , le : 0.001000\n",
      "Epoch 29/5000, Tr Loss: 1.0610, Tr Acc: 0.5323, Val Loss: 0.8572, Val Acc: 0.5287 , le : 0.001000\n",
      "Epoch 30/5000, Tr Loss: 1.0751, Tr Acc: 0.5075, Val Loss: 0.8418, Val Acc: 0.5287 , le : 0.001000\n",
      "Epoch 31/5000, Tr Loss: 1.0709, Tr Acc: 0.5224, Val Loss: 0.8300, Val Acc: 0.5287 , le : 0.001000\n",
      "Epoch 32/5000, Tr Loss: 1.0118, Tr Acc: 0.5373, Val Loss: 0.8187, Val Acc: 0.5402 , le : 0.001000\n",
      "Epoch 33/5000, Tr Loss: 1.0059, Tr Acc: 0.5871, Val Loss: 0.8095, Val Acc: 0.5517 , le : 0.001000\n",
      "Epoch 34/5000, Tr Loss: 1.0002, Tr Acc: 0.5572, Val Loss: 0.8020, Val Acc: 0.5517 , le : 0.001000\n",
      "Epoch 35/5000, Tr Loss: 1.0126, Tr Acc: 0.5274, Val Loss: 0.7960, Val Acc: 0.5517 , le : 0.001000\n",
      "Epoch 36/5000, Tr Loss: 1.0118, Tr Acc: 0.4925, Val Loss: 0.7908, Val Acc: 0.5402 , le : 0.001000\n",
      "Epoch 37/5000, Tr Loss: 0.9702, Tr Acc: 0.5572, Val Loss: 0.7874, Val Acc: 0.5402 , le : 0.001000\n",
      "Epoch 38/5000, Tr Loss: 0.9737, Tr Acc: 0.5323, Val Loss: 0.7850, Val Acc: 0.5287 , le : 0.001000\n",
      "Epoch 39/5000, Tr Loss: 0.9477, Tr Acc: 0.5871, Val Loss: 0.7820, Val Acc: 0.5172 , le : 0.001000\n",
      "Epoch 40/5000, Tr Loss: 0.9513, Tr Acc: 0.5970, Val Loss: 0.7804, Val Acc: 0.5172 , le : 0.001000\n",
      "Epoch 41/5000, Tr Loss: 0.9379, Tr Acc: 0.5871, Val Loss: 0.7795, Val Acc: 0.5172 , le : 0.001000\n",
      "Epoch 42/5000, Tr Loss: 0.9641, Tr Acc: 0.5224, Val Loss: 0.7780, Val Acc: 0.5172 , le : 0.001000\n",
      "Epoch 43/5000, Tr Loss: 0.9085, Tr Acc: 0.5821, Val Loss: 0.7777, Val Acc: 0.5172 , le : 0.001000\n",
      "Epoch 44/5000, Tr Loss: 0.9058, Tr Acc: 0.6169, Val Loss: 0.7777, Val Acc: 0.5172 , le : 0.001000\n",
      "Epoch 45/5000, Tr Loss: 0.8956, Tr Acc: 0.6219, Val Loss: 0.7784, Val Acc: 0.5287 , le : 0.001000\n",
      "Epoch 46/5000, Tr Loss: 0.9109, Tr Acc: 0.6020, Val Loss: 0.7789, Val Acc: 0.5402 , le : 0.001000\n",
      "Epoch 47/5000, Tr Loss: 0.8976, Tr Acc: 0.5920, Val Loss: 0.7785, Val Acc: 0.5402 , le : 0.001000\n",
      "Epoch 48/5000, Tr Loss: 0.8913, Tr Acc: 0.6418, Val Loss: 0.7779, Val Acc: 0.5402 , le : 0.001000\n",
      "Epoch 49/5000, Tr Loss: 0.8700, Tr Acc: 0.6219, Val Loss: 0.7772, Val Acc: 0.5402 , le : 0.001000\n",
      "Epoch 50/5000, Tr Loss: 0.8568, Tr Acc: 0.6318, Val Loss: 0.7763, Val Acc: 0.5517 , le : 0.001000\n",
      "Epoch 51/5000, Tr Loss: 0.8427, Tr Acc: 0.6219, Val Loss: 0.7742, Val Acc: 0.5517 , le : 0.001000\n",
      "Epoch 52/5000, Tr Loss: 0.8822, Tr Acc: 0.6020, Val Loss: 0.7741, Val Acc: 0.5632 , le : 0.001000\n",
      "Epoch 53/5000, Tr Loss: 0.8977, Tr Acc: 0.5622, Val Loss: 0.7715, Val Acc: 0.5632 , le : 0.001000\n",
      "Epoch 54/5000, Tr Loss: 0.8078, Tr Acc: 0.7015, Val Loss: 0.7678, Val Acc: 0.5632 , le : 0.001000\n",
      "Epoch 55/5000, Tr Loss: 0.8514, Tr Acc: 0.6269, Val Loss: 0.7665, Val Acc: 0.5632 , le : 0.001000\n",
      "Epoch 56/5000, Tr Loss: 0.8252, Tr Acc: 0.6617, Val Loss: 0.7652, Val Acc: 0.5747 , le : 0.001000\n",
      "Epoch 57/5000, Tr Loss: 0.8453, Tr Acc: 0.6318, Val Loss: 0.7644, Val Acc: 0.5747 , le : 0.001000\n",
      "Epoch 58/5000, Tr Loss: 0.8046, Tr Acc: 0.6766, Val Loss: 0.7628, Val Acc: 0.5977 , le : 0.001000\n",
      "Epoch 59/5000, Tr Loss: 0.8400, Tr Acc: 0.6517, Val Loss: 0.7622, Val Acc: 0.5977 , le : 0.001000\n",
      "Epoch 60/5000, Tr Loss: 0.8365, Tr Acc: 0.6667, Val Loss: 0.7613, Val Acc: 0.5862 , le : 0.001000\n",
      "Epoch 61/5000, Tr Loss: 0.8178, Tr Acc: 0.6517, Val Loss: 0.7611, Val Acc: 0.5747 , le : 0.001000\n",
      "Epoch 62/5000, Tr Loss: 0.8466, Tr Acc: 0.6468, Val Loss: 0.7608, Val Acc: 0.5747 , le : 0.001000\n",
      "Epoch 63/5000, Tr Loss: 0.7434, Tr Acc: 0.7363, Val Loss: 0.7623, Val Acc: 0.5747 , le : 0.001000\n",
      "Epoch 64/5000, Tr Loss: 0.8076, Tr Acc: 0.6368, Val Loss: 0.7628, Val Acc: 0.5977 , le : 0.001000\n",
      "Epoch 65/5000, Tr Loss: 0.8247, Tr Acc: 0.6318, Val Loss: 0.7633, Val Acc: 0.5977 , le : 0.001000\n",
      "Epoch 66/5000, Tr Loss: 0.7598, Tr Acc: 0.6816, Val Loss: 0.7637, Val Acc: 0.5977 , le : 0.001000\n",
      "Epoch 67/5000, Tr Loss: 0.7868, Tr Acc: 0.6517, Val Loss: 0.7605, Val Acc: 0.5977 , le : 0.001000\n",
      "Epoch 68/5000, Tr Loss: 0.7554, Tr Acc: 0.7413, Val Loss: 0.7564, Val Acc: 0.5977 , le : 0.001000\n",
      "Epoch 69/5000, Tr Loss: 0.7626, Tr Acc: 0.6816, Val Loss: 0.7519, Val Acc: 0.6092 , le : 0.001000\n",
      "Epoch 70/5000, Tr Loss: 0.6951, Tr Acc: 0.7612, Val Loss: 0.7460, Val Acc: 0.6207 , le : 0.001000\n",
      "Epoch 71/5000, Tr Loss: 0.7416, Tr Acc: 0.6766, Val Loss: 0.7415, Val Acc: 0.6092 , le : 0.001000\n",
      "Epoch 72/5000, Tr Loss: 0.7316, Tr Acc: 0.6915, Val Loss: 0.7382, Val Acc: 0.6207 , le : 0.001000\n",
      "Epoch 73/5000, Tr Loss: 0.7231, Tr Acc: 0.7065, Val Loss: 0.7342, Val Acc: 0.6207 , le : 0.001000\n",
      "Epoch 74/5000, Tr Loss: 0.7417, Tr Acc: 0.6716, Val Loss: 0.7316, Val Acc: 0.6207 , le : 0.001000\n",
      "Epoch 75/5000, Tr Loss: 0.6862, Tr Acc: 0.7363, Val Loss: 0.7290, Val Acc: 0.6207 , le : 0.001000\n",
      "Epoch 76/5000, Tr Loss: 0.7129, Tr Acc: 0.6965, Val Loss: 0.7243, Val Acc: 0.6207 , le : 0.001000\n",
      "Epoch 77/5000, Tr Loss: 0.6836, Tr Acc: 0.7711, Val Loss: 0.7210, Val Acc: 0.6322 , le : 0.001000\n",
      "Epoch 78/5000, Tr Loss: 0.6973, Tr Acc: 0.6866, Val Loss: 0.7213, Val Acc: 0.6437 , le : 0.001000\n",
      "Epoch 79/5000, Tr Loss: 0.7015, Tr Acc: 0.7065, Val Loss: 0.7196, Val Acc: 0.6437 , le : 0.001000\n",
      "Epoch 80/5000, Tr Loss: 0.6982, Tr Acc: 0.7164, Val Loss: 0.7168, Val Acc: 0.6552 , le : 0.001000\n",
      "Epoch 81/5000, Tr Loss: 0.6789, Tr Acc: 0.7363, Val Loss: 0.7162, Val Acc: 0.6667 , le : 0.001000\n",
      "Epoch 82/5000, Tr Loss: 0.6878, Tr Acc: 0.6866, Val Loss: 0.7159, Val Acc: 0.6552 , le : 0.001000\n",
      "Epoch 83/5000, Tr Loss: 0.6208, Tr Acc: 0.7562, Val Loss: 0.7168, Val Acc: 0.6552 , le : 0.001000\n",
      "Epoch 84/5000, Tr Loss: 0.6189, Tr Acc: 0.7562, Val Loss: 0.7176, Val Acc: 0.6552 , le : 0.001000\n",
      "Epoch 85/5000, Tr Loss: 0.6396, Tr Acc: 0.7463, Val Loss: 0.7173, Val Acc: 0.6552 , le : 0.001000\n",
      "Epoch 86/5000, Tr Loss: 0.6666, Tr Acc: 0.7264, Val Loss: 0.7166, Val Acc: 0.6667 , le : 0.001000\n",
      "Epoch 87/5000, Tr Loss: 0.6680, Tr Acc: 0.7512, Val Loss: 0.7185, Val Acc: 0.6782 , le : 0.001000\n",
      "Epoch 88/5000, Tr Loss: 0.6182, Tr Acc: 0.7761, Val Loss: 0.7190, Val Acc: 0.6667 , le : 0.001000\n",
      "Epoch 89/5000, Tr Loss: 0.6129, Tr Acc: 0.7711, Val Loss: 0.7181, Val Acc: 0.6552 , le : 0.001000\n",
      "Epoch 90/5000, Tr Loss: 0.6273, Tr Acc: 0.7662, Val Loss: 0.7190, Val Acc: 0.6667 , le : 0.001000\n",
      "Epoch 91/5000, Tr Loss: 0.5972, Tr Acc: 0.7711, Val Loss: 0.7185, Val Acc: 0.6782 , le : 0.001000\n",
      "Epoch 92/5000, Tr Loss: 0.5894, Tr Acc: 0.7910, Val Loss: 0.7176, Val Acc: 0.6782 , le : 0.001000\n",
      "Epoch 93/5000, Tr Loss: 0.6345, Tr Acc: 0.7612, Val Loss: 0.7150, Val Acc: 0.6782 , le : 0.001000\n",
      "Epoch 94/5000, Tr Loss: 0.5883, Tr Acc: 0.7711, Val Loss: 0.7124, Val Acc: 0.6782 , le : 0.001000\n",
      "Epoch 95/5000, Tr Loss: 0.5670, Tr Acc: 0.7910, Val Loss: 0.7115, Val Acc: 0.6552 , le : 0.001000\n",
      "Epoch 96/5000, Tr Loss: 0.5608, Tr Acc: 0.7861, Val Loss: 0.7091, Val Acc: 0.6667 , le : 0.001000\n",
      "Epoch 97/5000, Tr Loss: 0.5611, Tr Acc: 0.8010, Val Loss: 0.7057, Val Acc: 0.6782 , le : 0.001000\n",
      "Epoch 98/5000, Tr Loss: 0.5588, Tr Acc: 0.8159, Val Loss: 0.7005, Val Acc: 0.6667 , le : 0.001000\n",
      "Epoch 99/5000, Tr Loss: 0.5549, Tr Acc: 0.7910, Val Loss: 0.6950, Val Acc: 0.6897 , le : 0.001000\n",
      "Epoch 100/5000, Tr Loss: 0.5388, Tr Acc: 0.8109, Val Loss: 0.6927, Val Acc: 0.7011 , le : 0.001000\n",
      "Epoch 101/5000, Tr Loss: 0.5394, Tr Acc: 0.8209, Val Loss: 0.6916, Val Acc: 0.6897 , le : 0.001000\n",
      "Epoch 102/5000, Tr Loss: 0.5224, Tr Acc: 0.8607, Val Loss: 0.6880, Val Acc: 0.6897 , le : 0.001000\n",
      "Epoch 103/5000, Tr Loss: 0.4825, Tr Acc: 0.8657, Val Loss: 0.6863, Val Acc: 0.6897 , le : 0.001000\n",
      "Epoch 104/5000, Tr Loss: 0.4570, Tr Acc: 0.8557, Val Loss: 0.6855, Val Acc: 0.6897 , le : 0.001000\n",
      "Epoch 105/5000, Tr Loss: 0.5087, Tr Acc: 0.8159, Val Loss: 0.6826, Val Acc: 0.7011 , le : 0.001000\n",
      "Epoch 106/5000, Tr Loss: 0.4786, Tr Acc: 0.8358, Val Loss: 0.6795, Val Acc: 0.6897 , le : 0.001000\n",
      "Epoch 107/5000, Tr Loss: 0.4825, Tr Acc: 0.8358, Val Loss: 0.6771, Val Acc: 0.7011 , le : 0.001000\n",
      "Epoch 108/5000, Tr Loss: 0.4640, Tr Acc: 0.8507, Val Loss: 0.6715, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 109/5000, Tr Loss: 0.4686, Tr Acc: 0.8458, Val Loss: 0.6675, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 110/5000, Tr Loss: 0.4613, Tr Acc: 0.8557, Val Loss: 0.6630, Val Acc: 0.7011 , le : 0.001000\n",
      "Epoch 111/5000, Tr Loss: 0.4568, Tr Acc: 0.8259, Val Loss: 0.6608, Val Acc: 0.7011 , le : 0.001000\n",
      "Epoch 112/5000, Tr Loss: 0.4799, Tr Acc: 0.8159, Val Loss: 0.6586, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 113/5000, Tr Loss: 0.4456, Tr Acc: 0.8856, Val Loss: 0.6590, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 114/5000, Tr Loss: 0.4182, Tr Acc: 0.8557, Val Loss: 0.6597, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 115/5000, Tr Loss: 0.4213, Tr Acc: 0.8607, Val Loss: 0.6612, Val Acc: 0.7011 , le : 0.001000\n",
      "Epoch 116/5000, Tr Loss: 0.3841, Tr Acc: 0.9055, Val Loss: 0.6622, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 117/5000, Tr Loss: 0.3926, Tr Acc: 0.8856, Val Loss: 0.6650, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 118/5000, Tr Loss: 0.3796, Tr Acc: 0.8905, Val Loss: 0.6679, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 119/5000, Tr Loss: 0.3748, Tr Acc: 0.9005, Val Loss: 0.6681, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 120/5000, Tr Loss: 0.3784, Tr Acc: 0.8955, Val Loss: 0.6645, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 121/5000, Tr Loss: 0.4074, Tr Acc: 0.8806, Val Loss: 0.6628, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 122/5000, Tr Loss: 0.4213, Tr Acc: 0.8756, Val Loss: 0.6616, Val Acc: 0.7011 , le : 0.001000\n",
      "Epoch 123/5000, Tr Loss: 0.3538, Tr Acc: 0.9005, Val Loss: 0.6597, Val Acc: 0.7011 , le : 0.001000\n",
      "Epoch 124/5000, Tr Loss: 0.3561, Tr Acc: 0.9104, Val Loss: 0.6602, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 125/5000, Tr Loss: 0.3447, Tr Acc: 0.8905, Val Loss: 0.6620, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 126/5000, Tr Loss: 0.3623, Tr Acc: 0.8856, Val Loss: 0.6676, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 127/5000, Tr Loss: 0.3360, Tr Acc: 0.9303, Val Loss: 0.6722, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 128/5000, Tr Loss: 0.3062, Tr Acc: 0.9154, Val Loss: 0.6754, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 129/5000, Tr Loss: 0.2937, Tr Acc: 0.9254, Val Loss: 0.6762, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 130/5000, Tr Loss: 0.3304, Tr Acc: 0.8955, Val Loss: 0.6755, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 131/5000, Tr Loss: 0.3432, Tr Acc: 0.8955, Val Loss: 0.6727, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 132/5000, Tr Loss: 0.3515, Tr Acc: 0.9104, Val Loss: 0.6663, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 133/5000, Tr Loss: 0.3302, Tr Acc: 0.9055, Val Loss: 0.6632, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 134/5000, Tr Loss: 0.2794, Tr Acc: 0.9353, Val Loss: 0.6609, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 135/5000, Tr Loss: 0.2512, Tr Acc: 0.9602, Val Loss: 0.6581, Val Acc: 0.7011 , le : 0.001000\n",
      "Epoch 136/5000, Tr Loss: 0.2799, Tr Acc: 0.9453, Val Loss: 0.6565, Val Acc: 0.7011 , le : 0.001000\n",
      "Epoch 137/5000, Tr Loss: 0.3391, Tr Acc: 0.8905, Val Loss: 0.6546, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 138/5000, Tr Loss: 0.3177, Tr Acc: 0.9204, Val Loss: 0.6541, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 139/5000, Tr Loss: 0.2777, Tr Acc: 0.9353, Val Loss: 0.6546, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 140/5000, Tr Loss: 0.2437, Tr Acc: 0.9701, Val Loss: 0.6585, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 141/5000, Tr Loss: 0.2687, Tr Acc: 0.9303, Val Loss: 0.6572, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 142/5000, Tr Loss: 0.2556, Tr Acc: 0.9453, Val Loss: 0.6574, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 143/5000, Tr Loss: 0.2598, Tr Acc: 0.9353, Val Loss: 0.6590, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 144/5000, Tr Loss: 0.2679, Tr Acc: 0.9403, Val Loss: 0.6608, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 145/5000, Tr Loss: 0.2628, Tr Acc: 0.9303, Val Loss: 0.6626, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 146/5000, Tr Loss: 0.2691, Tr Acc: 0.9353, Val Loss: 0.6660, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 147/5000, Tr Loss: 0.2446, Tr Acc: 0.9353, Val Loss: 0.6697, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 148/5000, Tr Loss: 0.2342, Tr Acc: 0.9552, Val Loss: 0.6729, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 149/5000, Tr Loss: 0.2331, Tr Acc: 0.9602, Val Loss: 0.6764, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 150/5000, Tr Loss: 0.2475, Tr Acc: 0.9254, Val Loss: 0.6763, Val Acc: 0.7011 , le : 0.001000\n",
      "Epoch 151/5000, Tr Loss: 0.2140, Tr Acc: 0.9502, Val Loss: 0.6747, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 152/5000, Tr Loss: 0.2100, Tr Acc: 0.9502, Val Loss: 0.6719, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 153/5000, Tr Loss: 0.2226, Tr Acc: 0.9602, Val Loss: 0.6705, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 154/5000, Tr Loss: 0.2450, Tr Acc: 0.9353, Val Loss: 0.6689, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 155/5000, Tr Loss: 0.2144, Tr Acc: 0.9602, Val Loss: 0.6678, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 156/5000, Tr Loss: 0.2125, Tr Acc: 0.9453, Val Loss: 0.6672, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 157/5000, Tr Loss: 0.2167, Tr Acc: 0.9552, Val Loss: 0.6709, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 158/5000, Tr Loss: 0.1875, Tr Acc: 0.9701, Val Loss: 0.6765, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 159/5000, Tr Loss: 0.1871, Tr Acc: 0.9751, Val Loss: 0.6830, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 160/5000, Tr Loss: 0.1868, Tr Acc: 0.9801, Val Loss: 0.6874, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 161/5000, Tr Loss: 0.1716, Tr Acc: 0.9801, Val Loss: 0.6866, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 162/5000, Tr Loss: 0.1930, Tr Acc: 0.9502, Val Loss: 0.6842, Val Acc: 0.7011 , le : 0.001000\n",
      "Epoch 163/5000, Tr Loss: 0.2055, Tr Acc: 0.9552, Val Loss: 0.6829, Val Acc: 0.7011 , le : 0.001000\n",
      "Epoch 164/5000, Tr Loss: 0.1611, Tr Acc: 0.9701, Val Loss: 0.6805, Val Acc: 0.7126 , le : 0.001000\n",
      "Epoch 165/5000, Tr Loss: 0.1695, Tr Acc: 0.9701, Val Loss: 0.6748, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 166/5000, Tr Loss: 0.1425, Tr Acc: 0.9801, Val Loss: 0.6705, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 167/5000, Tr Loss: 0.1625, Tr Acc: 0.9751, Val Loss: 0.6689, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 168/5000, Tr Loss: 0.1624, Tr Acc: 0.9701, Val Loss: 0.6682, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 169/5000, Tr Loss: 0.1680, Tr Acc: 0.9552, Val Loss: 0.6668, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 170/5000, Tr Loss: 0.1797, Tr Acc: 0.9652, Val Loss: 0.6639, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 171/5000, Tr Loss: 0.1563, Tr Acc: 0.9652, Val Loss: 0.6620, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 172/5000, Tr Loss: 0.1332, Tr Acc: 0.9851, Val Loss: 0.6626, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 173/5000, Tr Loss: 0.1647, Tr Acc: 0.9602, Val Loss: 0.6648, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 174/5000, Tr Loss: 0.1655, Tr Acc: 0.9552, Val Loss: 0.6670, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 175/5000, Tr Loss: 0.1577, Tr Acc: 0.9851, Val Loss: 0.6704, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 176/5000, Tr Loss: 0.1456, Tr Acc: 0.9801, Val Loss: 0.6743, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 177/5000, Tr Loss: 0.1262, Tr Acc: 0.9851, Val Loss: 0.6791, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 178/5000, Tr Loss: 0.1440, Tr Acc: 0.9801, Val Loss: 0.6847, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 179/5000, Tr Loss: 0.1166, Tr Acc: 0.9851, Val Loss: 0.6890, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 180/5000, Tr Loss: 0.1383, Tr Acc: 0.9652, Val Loss: 0.6926, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 181/5000, Tr Loss: 0.1387, Tr Acc: 0.9751, Val Loss: 0.6970, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 182/5000, Tr Loss: 0.1283, Tr Acc: 0.9751, Val Loss: 0.6994, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 183/5000, Tr Loss: 0.1131, Tr Acc: 0.9801, Val Loss: 0.6998, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 184/5000, Tr Loss: 0.1344, Tr Acc: 0.9751, Val Loss: 0.7005, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 185/5000, Tr Loss: 0.1306, Tr Acc: 0.9751, Val Loss: 0.7016, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 186/5000, Tr Loss: 0.1375, Tr Acc: 0.9751, Val Loss: 0.7034, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 187/5000, Tr Loss: 0.1157, Tr Acc: 0.9751, Val Loss: 0.7078, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 188/5000, Tr Loss: 0.1117, Tr Acc: 0.9900, Val Loss: 0.7112, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 189/5000, Tr Loss: 0.1158, Tr Acc: 0.9900, Val Loss: 0.7149, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 190/5000, Tr Loss: 0.0979, Tr Acc: 0.9900, Val Loss: 0.7181, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 191/5000, Tr Loss: 0.0939, Tr Acc: 0.9900, Val Loss: 0.7202, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 192/5000, Tr Loss: 0.1131, Tr Acc: 0.9900, Val Loss: 0.7240, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 193/5000, Tr Loss: 0.0954, Tr Acc: 0.9900, Val Loss: 0.7285, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 194/5000, Tr Loss: 0.0900, Tr Acc: 0.9950, Val Loss: 0.7327, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 195/5000, Tr Loss: 0.0996, Tr Acc: 0.9900, Val Loss: 0.7371, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 196/5000, Tr Loss: 0.1197, Tr Acc: 0.9851, Val Loss: 0.7365, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 197/5000, Tr Loss: 0.1487, Tr Acc: 0.9652, Val Loss: 0.7342, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 198/5000, Tr Loss: 0.1131, Tr Acc: 0.9851, Val Loss: 0.7318, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 199/5000, Tr Loss: 0.0995, Tr Acc: 0.9900, Val Loss: 0.7293, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 200/5000, Tr Loss: 0.0999, Tr Acc: 0.9801, Val Loss: 0.7260, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 201/5000, Tr Loss: 0.0774, Tr Acc: 0.9950, Val Loss: 0.7238, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 202/5000, Tr Loss: 0.0934, Tr Acc: 0.9851, Val Loss: 0.7228, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 203/5000, Tr Loss: 0.0842, Tr Acc: 0.9801, Val Loss: 0.7222, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 204/5000, Tr Loss: 0.0916, Tr Acc: 0.9950, Val Loss: 0.7236, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 205/5000, Tr Loss: 0.0863, Tr Acc: 0.9900, Val Loss: 0.7254, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 206/5000, Tr Loss: 0.0722, Tr Acc: 0.9851, Val Loss: 0.7296, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 207/5000, Tr Loss: 0.0996, Tr Acc: 0.9801, Val Loss: 0.7326, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 208/5000, Tr Loss: 0.0807, Tr Acc: 1.0000, Val Loss: 0.7360, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 209/5000, Tr Loss: 0.0879, Tr Acc: 0.9801, Val Loss: 0.7411, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 210/5000, Tr Loss: 0.0952, Tr Acc: 0.9851, Val Loss: 0.7432, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 211/5000, Tr Loss: 0.0966, Tr Acc: 0.9751, Val Loss: 0.7441, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 212/5000, Tr Loss: 0.0742, Tr Acc: 0.9950, Val Loss: 0.7437, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 213/5000, Tr Loss: 0.0781, Tr Acc: 0.9950, Val Loss: 0.7436, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 214/5000, Tr Loss: 0.0814, Tr Acc: 0.9851, Val Loss: 0.7419, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 215/5000, Tr Loss: 0.0912, Tr Acc: 0.9851, Val Loss: 0.7381, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 216/5000, Tr Loss: 0.0928, Tr Acc: 0.9851, Val Loss: 0.7333, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 217/5000, Tr Loss: 0.0586, Tr Acc: 1.0000, Val Loss: 0.7304, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 218/5000, Tr Loss: 0.0830, Tr Acc: 0.9950, Val Loss: 0.7288, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 219/5000, Tr Loss: 0.0666, Tr Acc: 0.9900, Val Loss: 0.7293, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 220/5000, Tr Loss: 0.0697, Tr Acc: 0.9950, Val Loss: 0.7300, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 221/5000, Tr Loss: 0.0557, Tr Acc: 1.0000, Val Loss: 0.7318, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 222/5000, Tr Loss: 0.0561, Tr Acc: 1.0000, Val Loss: 0.7332, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 223/5000, Tr Loss: 0.0684, Tr Acc: 0.9851, Val Loss: 0.7365, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 224/5000, Tr Loss: 0.0607, Tr Acc: 1.0000, Val Loss: 0.7390, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 225/5000, Tr Loss: 0.0630, Tr Acc: 0.9950, Val Loss: 0.7424, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 226/5000, Tr Loss: 0.0542, Tr Acc: 1.0000, Val Loss: 0.7455, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 227/5000, Tr Loss: 0.0734, Tr Acc: 0.9900, Val Loss: 0.7469, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 228/5000, Tr Loss: 0.0701, Tr Acc: 0.9900, Val Loss: 0.7491, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 229/5000, Tr Loss: 0.0600, Tr Acc: 1.0000, Val Loss: 0.7503, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 230/5000, Tr Loss: 0.0669, Tr Acc: 0.9900, Val Loss: 0.7514, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 231/5000, Tr Loss: 0.0596, Tr Acc: 0.9900, Val Loss: 0.7537, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 232/5000, Tr Loss: 0.0473, Tr Acc: 1.0000, Val Loss: 0.7572, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 233/5000, Tr Loss: 0.0765, Tr Acc: 0.9851, Val Loss: 0.7527, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 234/5000, Tr Loss: 0.0553, Tr Acc: 1.0000, Val Loss: 0.7488, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 235/5000, Tr Loss: 0.0626, Tr Acc: 0.9851, Val Loss: 0.7461, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 236/5000, Tr Loss: 0.0534, Tr Acc: 0.9950, Val Loss: 0.7449, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 237/5000, Tr Loss: 0.0631, Tr Acc: 0.9900, Val Loss: 0.7439, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 238/5000, Tr Loss: 0.0592, Tr Acc: 1.0000, Val Loss: 0.7443, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 239/5000, Tr Loss: 0.0444, Tr Acc: 0.9950, Val Loss: 0.7496, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 240/5000, Tr Loss: 0.0609, Tr Acc: 0.9950, Val Loss: 0.7545, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 241/5000, Tr Loss: 0.0590, Tr Acc: 0.9900, Val Loss: 0.7580, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 242/5000, Tr Loss: 0.0670, Tr Acc: 0.9900, Val Loss: 0.7616, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 243/5000, Tr Loss: 0.0580, Tr Acc: 0.9950, Val Loss: 0.7637, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 244/5000, Tr Loss: 0.0648, Tr Acc: 0.9950, Val Loss: 0.7661, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 245/5000, Tr Loss: 0.0434, Tr Acc: 1.0000, Val Loss: 0.7684, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 246/5000, Tr Loss: 0.0500, Tr Acc: 0.9900, Val Loss: 0.7685, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 247/5000, Tr Loss: 0.0726, Tr Acc: 0.9900, Val Loss: 0.7685, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 248/5000, Tr Loss: 0.0653, Tr Acc: 0.9900, Val Loss: 0.7696, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 249/5000, Tr Loss: 0.0582, Tr Acc: 0.9900, Val Loss: 0.7724, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 250/5000, Tr Loss: 0.0434, Tr Acc: 0.9950, Val Loss: 0.7764, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 251/5000, Tr Loss: 0.0469, Tr Acc: 0.9950, Val Loss: 0.7800, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 252/5000, Tr Loss: 0.0436, Tr Acc: 1.0000, Val Loss: 0.7844, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 253/5000, Tr Loss: 0.0473, Tr Acc: 0.9950, Val Loss: 0.7892, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 254/5000, Tr Loss: 0.0433, Tr Acc: 1.0000, Val Loss: 0.7921, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 255/5000, Tr Loss: 0.0693, Tr Acc: 0.9900, Val Loss: 0.7872, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 256/5000, Tr Loss: 0.0420, Tr Acc: 0.9950, Val Loss: 0.7806, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 257/5000, Tr Loss: 0.0438, Tr Acc: 1.0000, Val Loss: 0.7746, Val Acc: 0.7241 , le : 0.001000\n",
      "Epoch 258/5000, Tr Loss: 0.0491, Tr Acc: 0.9950, Val Loss: 0.7704, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 259/5000, Tr Loss: 0.0407, Tr Acc: 1.0000, Val Loss: 0.7686, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 260/5000, Tr Loss: 0.0366, Tr Acc: 1.0000, Val Loss: 0.7685, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 261/5000, Tr Loss: 0.0514, Tr Acc: 0.9900, Val Loss: 0.7673, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 262/5000, Tr Loss: 0.0488, Tr Acc: 0.9900, Val Loss: 0.7655, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 263/5000, Tr Loss: 0.0508, Tr Acc: 0.9851, Val Loss: 0.7629, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 264/5000, Tr Loss: 0.0523, Tr Acc: 0.9900, Val Loss: 0.7625, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 265/5000, Tr Loss: 0.0429, Tr Acc: 1.0000, Val Loss: 0.7631, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 266/5000, Tr Loss: 0.0474, Tr Acc: 0.9950, Val Loss: 0.7661, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 267/5000, Tr Loss: 0.0411, Tr Acc: 0.9950, Val Loss: 0.7718, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 268/5000, Tr Loss: 0.0338, Tr Acc: 1.0000, Val Loss: 0.7793, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 269/5000, Tr Loss: 0.0512, Tr Acc: 0.9950, Val Loss: 0.7854, Val Acc: 0.7816 , le : 0.001000\n",
      "Epoch 270/5000, Tr Loss: 0.0655, Tr Acc: 0.9900, Val Loss: 0.7889, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 271/5000, Tr Loss: 0.0354, Tr Acc: 0.9950, Val Loss: 0.7941, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 272/5000, Tr Loss: 0.0448, Tr Acc: 0.9950, Val Loss: 0.7993, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 273/5000, Tr Loss: 0.0346, Tr Acc: 0.9950, Val Loss: 0.8047, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 274/5000, Tr Loss: 0.0377, Tr Acc: 0.9900, Val Loss: 0.8065, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 275/5000, Tr Loss: 0.0574, Tr Acc: 0.9900, Val Loss: 0.8071, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 276/5000, Tr Loss: 0.0351, Tr Acc: 1.0000, Val Loss: 0.8095, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 277/5000, Tr Loss: 0.0473, Tr Acc: 0.9900, Val Loss: 0.8109, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 278/5000, Tr Loss: 0.0360, Tr Acc: 0.9950, Val Loss: 0.8135, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 279/5000, Tr Loss: 0.0351, Tr Acc: 0.9950, Val Loss: 0.8139, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 280/5000, Tr Loss: 0.0326, Tr Acc: 1.0000, Val Loss: 0.8148, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 281/5000, Tr Loss: 0.0283, Tr Acc: 1.0000, Val Loss: 0.8148, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 282/5000, Tr Loss: 0.0322, Tr Acc: 1.0000, Val Loss: 0.8152, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 283/5000, Tr Loss: 0.0326, Tr Acc: 1.0000, Val Loss: 0.8151, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 284/5000, Tr Loss: 0.0297, Tr Acc: 1.0000, Val Loss: 0.8143, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 285/5000, Tr Loss: 0.0426, Tr Acc: 0.9900, Val Loss: 0.8154, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 286/5000, Tr Loss: 0.0284, Tr Acc: 0.9950, Val Loss: 0.8155, Val Acc: 0.7356 , le : 0.001000\n",
      "Epoch 287/5000, Tr Loss: 0.0351, Tr Acc: 0.9950, Val Loss: 0.8148, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 288/5000, Tr Loss: 0.0308, Tr Acc: 1.0000, Val Loss: 0.8121, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 289/5000, Tr Loss: 0.0336, Tr Acc: 1.0000, Val Loss: 0.8113, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 290/5000, Tr Loss: 0.0255, Tr Acc: 1.0000, Val Loss: 0.8136, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 291/5000, Tr Loss: 0.0389, Tr Acc: 0.9950, Val Loss: 0.8198, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 292/5000, Tr Loss: 0.0394, Tr Acc: 0.9900, Val Loss: 0.8244, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 293/5000, Tr Loss: 0.0345, Tr Acc: 0.9950, Val Loss: 0.8319, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 294/5000, Tr Loss: 0.0323, Tr Acc: 0.9950, Val Loss: 0.8378, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 295/5000, Tr Loss: 0.0240, Tr Acc: 1.0000, Val Loss: 0.8412, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 296/5000, Tr Loss: 0.0278, Tr Acc: 1.0000, Val Loss: 0.8433, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 297/5000, Tr Loss: 0.0293, Tr Acc: 1.0000, Val Loss: 0.8456, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 298/5000, Tr Loss: 0.0345, Tr Acc: 1.0000, Val Loss: 0.8454, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 299/5000, Tr Loss: 0.0247, Tr Acc: 1.0000, Val Loss: 0.8428, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 300/5000, Tr Loss: 0.0328, Tr Acc: 1.0000, Val Loss: 0.8409, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 301/5000, Tr Loss: 0.0324, Tr Acc: 1.0000, Val Loss: 0.8385, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 302/5000, Tr Loss: 0.0300, Tr Acc: 1.0000, Val Loss: 0.8358, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 303/5000, Tr Loss: 0.0340, Tr Acc: 1.0000, Val Loss: 0.8336, Val Acc: 0.7586 , le : 0.001000\n",
      "Epoch 304/5000, Tr Loss: 0.0238, Tr Acc: 1.0000, Val Loss: 0.8324, Val Acc: 0.7701 , le : 0.001000\n",
      "Epoch 305/5000, Tr Loss: 0.0282, Tr Acc: 0.9950, Val Loss: 0.8325, Val Acc: 0.7471 , le : 0.001000\n",
      "Epoch 306/5000, Tr Loss: 0.0182, Tr Acc: 1.0000, Val Loss: 0.8333, Val Acc: 0.7471 , le : 0.001000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for network in network_list:\n",
    "\n",
    "\n",
    "    for subject_id in subjects:\n",
    "        for percent in percent_list:\n",
    "            run = neptune.init_run(\n",
    "    project=\"AitBrainLab/Synthetic\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhMTMyMzg0My02NzlhLTQ3N2ItYTdmMS0yNTcwNDBmM2QwM2QifQ==\",\n",
    ")\n",
    "\n",
    "            #torch.manual_seed(3407)\n",
    "            dataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids= [subject_id])\n",
    "            preprocess(dataset, preprocessors)\n",
    "            trial_start_offset_seconds = -0.5\n",
    "            # Extract sampling frequency, check that they are same in all datasets\n",
    "            sfreq = dataset.datasets[0].raw.info['sfreq']\n",
    "            print(sfreq)\n",
    "            assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
    "            # Calculate the trial start offset in samples.\n",
    "            trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
    "\n",
    "            # Create windows using braindecode function for this. It needs parameters to define how\n",
    "            # trials should be used.\n",
    "            windows_dataset = create_windows_from_events(\n",
    "                dataset,\n",
    "                trial_start_offset_samples=trial_start_offset_samples,\n",
    "                trial_stop_offset_samples=0,\n",
    "                preload=True,\n",
    "            )\n",
    "            splitted = windows_dataset.split('session')\n",
    "            train_set = splitted['session_T']\n",
    "            valid_set = splitted['session_E']\n",
    "\n",
    "            input_window_samples = train_set[0][0].shape[1]\n",
    "            X_train,y_train = extrack_dataset(train_set)\n",
    "            #X_train = X_train[:, np.newaxis,:,:]\n",
    "\n",
    "            X_valid,y_valid = extrack_dataset(valid_set)\n",
    "            #X_valid = X_valid[:, np.newaxis,:,:]\n",
    "\n",
    "            X_syntheic = np.load(\"Synthetic_data_shift/X_subject_{0}.npy\".format(subject_id)) #np.load(\"Synthetic_data/X_subject_3_20_[0, 1].npy\")#\n",
    "            y_syntheic = np.load(\"Synthetic_data_shift/y_subject_{0}.npy\".format(subject_id))#np.load(\"Synthetic_data/y_subject_3_20_[0, 1].npy\")#\n",
    "            #['left', 'right', 'foot', 'tongue']\n",
    "            if len(task_list) == 4:\n",
    "                for task in task_list :\n",
    "                    if task == \"left\":\n",
    "                        X1 = X_syntheic[np.random.randint(0,72,percent)]\n",
    "                        y1 = y_syntheic[np.random.randint(0,72,percent)]\n",
    "                    if task == \"right\":\n",
    "                        X2 = X_syntheic[np.random.randint(72,144,percent)]\n",
    "                        y2 = y_syntheic[np.random.randint(72,144,percent)]\n",
    "                    if task == \"foot\":\n",
    "                        X3 = X_syntheic[np.random.randint(144,216,percent)]\n",
    "                        y3 = y_syntheic[np.random.randint(144,216,percent)]\n",
    "                    if task == \"tongue\":\n",
    "                        X4 = X_syntheic[np.random.randint(216,288,percent)]\n",
    "                        y4 = y_syntheic[np.random.randint(216,288,percent)]\n",
    "                X_syntheic = np.concatenate((X1,X2,X3,X4),axis=0)\n",
    "                y_syntheic = np.concatenate((y1,y2,y3,y4),axis=0)\n",
    "            if len(task_list)==1:\n",
    "                for task in task_list :\n",
    "                    if task == \"left\":\n",
    "                        X_syntheic = X_syntheic[np.random.randint(0,72,percent)]\n",
    "                        y_syntheic = y_syntheic[np.random.randint(0,72,percent)]\n",
    "                    if task == \"right\":\n",
    "                        X_syntheic = X_syntheic[np.random.randint(72,144,percent)]\n",
    "                        y_syntheic = y_syntheic[np.random.randint(72,144,percent)]\n",
    "                    if task == \"foot\":\n",
    "                        X_syntheic = X_syntheic[np.random.randint(144,216,percent)]\n",
    "                        y_syntheic = y_syntheic[np.random.randint(144,216,percent)]\n",
    "                    if task == \"tongue\":\n",
    "                        X_syntheic = X_syntheic[np.random.randint(216,288,percent)]\n",
    "                        y_syntheic = y_syntheic[np.random.randint(216,288,percent)]\n",
    "                X_train = np.concatenate((X_train,X_syntheic),axis = 0)\n",
    "                y_train = np.concatenate((y_train, y_syntheic),axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_train,y_train, test_size=0.3,stratify=y_train)\n",
    "            label_dict = valid_set.datasets[0].windows.event_id.items()\n",
    "            labels = list(dict(sorted(list(label_dict), key=lambda kv: kv[1])).keys())\n",
    "            print('train size',X_train.shape, y_train.shape)\n",
    "            print('test size',X_test.shape, y_test.shape)\n",
    "\n",
    "            batch_size = X_train.shape[2]\n",
    "\n",
    "            train_loader = create_dataloader(X_train, y_train, batch_size=batch_size)\n",
    "            test_loader = create_dataloader(X_test, y_test, batch_size=batch_size)\n",
    "            valid_loader = create_dataloader(X_valid, y_valid, batch_size=batch_size)\n",
    "            n_classes=4\n",
    "            n_chans = X_train.shape[1]\n",
    "\n",
    "            #network_list = ['ATCNet', 'ShallowFBCSPNet', 'Deep4Net', 'EEGNetv4']\n",
    "            if network == 'ATCNet' :\n",
    "                model = ATCNet(\n",
    "                                n_chans,\n",
    "                                n_classes\n",
    "                                )\n",
    "            if network == \"ShallowFBCSPNet\":\n",
    "                model = ShallowFBCSPNet(\n",
    "                n_chans,\n",
    "                n_classes,\n",
    "                input_window_samples=input_window_samples,\n",
    "                final_conv_length=\"auto\",\n",
    "                )\n",
    "            if network == \"EEGITNet\":\n",
    "                model = EEGITNet(\n",
    "                    n_classes,\n",
    "                    n_chans,\n",
    "                    input_window_samples=input_window_samples,\n",
    "                    )\n",
    "            if network == \"Deep4Net\":\n",
    "                model = Deep4Net(\n",
    "                    n_chans,\n",
    "                    n_classes,\n",
    "                    input_window_samples=input_window_samples,\n",
    "                    final_conv_length='auto',\n",
    "                    n_filters_time=25,\n",
    "                    n_filters_spat=25,\n",
    "                    filter_time_length=10,\n",
    "                    pool_time_length=3,\n",
    "                    pool_time_stride=3,\n",
    "                    n_filters_2=50,\n",
    "                    filter_length_2=10,\n",
    "                    n_filters_3=100,\n",
    "                    filter_length_3=10,\n",
    "                    n_filters_4=200,\n",
    "                    filter_length_4=10,\n",
    "                    first_pool_mode=\"max\",\n",
    "                    later_pool_mode=\"max\",\n",
    "                    drop_prob=0.5,\n",
    "                    #double_time_convs=False,\n",
    "                    split_first_layer=True,\n",
    "                    batch_norm=True,\n",
    "                    batch_norm_alpha=0.1,\n",
    "                    stride_before_pool=False\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "            params = {\"Subject number\":subject_id,\n",
    "                      \"learning_rate\": lr ,\n",
    "                      \"optimizer\": \"AdamW\" ,\n",
    "                      \"Network\": network,\n",
    "                      \"Datasets\":\"BNCI2014001+Synthetic_shift_size_{0}.\".format(percent),\n",
    "                      \"sfreq\":dataset.datasets[0].raw.info['sfreq'],\n",
    "                      \"Class number\":n_classes,\n",
    "                      \"Channel number\": train_set[0][0].shape[0],\n",
    "                      \"samples point\" : X_train.shape[2]\n",
    "\n",
    "                      }\n",
    "            run[\"parameters\"] = params\n",
    "            net = model.cuda(0)\n",
    "            optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "            criterion = nn.CrossEntropyLoss()#nn.BCELoss()#\n",
    "            train_loss,valid_loss,train_accuracy,valid_accuracy,part_weights =train(\n",
    "                                                                        model = net,\n",
    "                                                                        gpu_num = 0,\n",
    "                                                                        train_loader = train_loader,\n",
    "                                                                        test_loader = test_loader,\n",
    "                                                                        optimizer = optimizer  ,\n",
    "                                                                        criterion = criterion,\n",
    "                                                                        num_epochs=n_epochs,\n",
    "                                                                        save_weights= True,\n",
    "                                                                        lr=lr\n",
    "                                                                             )\n",
    "            model.load_state_dict(torch.load(part_weights))\n",
    "            eval(model = net,\n",
    "                gpu_num = 0,\n",
    "                valid_loader= valid_loader,\n",
    "                 labels=labels,\n",
    "                 )\n",
    "\n",
    "\n",
    "            run.stop()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "correct=0\n",
    "evaluate_accuracy = []\n",
    "for i, (items, classes) in enumerate(valid_loader):\n",
    "    classes = classes.type(torch.LongTensor)\n",
    "    items = Variable(items)\n",
    "    classes = Variable(classes)\n",
    "    if cuda.is_available():\n",
    "        items = items.cuda(0)\n",
    "        classes = classes.cuda(0)\n",
    "\n",
    "    outputs = model(items)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    y_pred.append(predicted.clone().cpu().numpy())\n",
    "    y_true.append(classes.data.clone().cpu().numpy())\n",
    "    correct += (predicted == classes.data).sum()\n",
    "correct_scalar = np.array([correct.clone().cpu()])[0]\n",
    "evaluate_accuracy.append(correct_scalar / classes.shape[0] )\n",
    "\n",
    "confusion_mat = confusion_matrix(np.array(y_true).T,np.array(y_pred).T )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[37,  5,  6, 24],\n       [ 4, 32, 30,  6],\n       [ 3, 28, 39,  2],\n       [ 8,  2,  5, 57]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_mat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "5002"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5729166666666666"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_scalar/classes.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "288"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

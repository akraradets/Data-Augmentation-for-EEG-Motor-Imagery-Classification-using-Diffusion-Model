{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cbc64cee-c7a9-4a73-b2a9-95259a4a3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#comment this if you are not using puffer?\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb286c65-ec6a-454e-9059-c49f55b8454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import os\n",
    "import sys\n",
    "from mne.datasets import eegbci\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from mne.datasets import eegbci\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cf10281-a709-4313-a8f6-b26b2c3d4859",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG:\n",
    "    def __init__(self, path, base_url, subjects, runs):\n",
    "        self.subpath = ''\n",
    "        self.path = path\n",
    "        self.base_url = base_url\n",
    "        self.subjects = subjects\n",
    "        self.runs = runs\n",
    "        \n",
    "        # download data if does not exist in path.\n",
    "        # self.load_data()\n",
    "        self.data_to_raw()\n",
    "    \n",
    "    def load_data(self):\n",
    "        print(f\">>> Start download from: {self.base_url}.\")\n",
    "        print(f\"Downloading files to: {self.path}.\")\n",
    "        for subject in self.subjects:\n",
    "            eegbci.load_data(subject,self.runs,path=self.path,base_url=self.base_url)\n",
    "        print(\"Done.\")\n",
    "    \n",
    "    \n",
    "        \n",
    "        print(\"Done.\")\n",
    "        return self.raw\n",
    "    def filter(self, freq):\n",
    "        raw = self.raw\n",
    "        low, high = freq\n",
    "        print(f\">>> Apply filter.\")\n",
    "        self.raw.filter(low, high, fir_design='firwin', verbose=20)\n",
    "        return  raw\n",
    "    def raw_ica(self):\n",
    "        raw = self.raw\n",
    "        ica = mne.preprocessing.ICA(n_components=64, max_iter=100)\n",
    "        ica.fit(raw)\n",
    "        ica.exclude = [1, 2]  # details on how we picked these are omitted here\n",
    "        ica.plot_properties(raw, picks=ica.exclude)\n",
    "        ica.apply(raw)\n",
    "        print('ICA DONE ????')\n",
    "        return  raw\n",
    "        \n",
    "    def get_events(self):\n",
    "        event_id = dict(T1=0, T2=1) # the events we want to extract\n",
    "        events, event_id = mne.events_from_annotations(self.raw, event_id=event_id)\n",
    "        return events, event_id\n",
    "    \n",
    "    def get_epochs(self, events, event_id):\n",
    "        picks = mne.pick_types(self.raw.info, eeg=True, exclude='bads')\n",
    "        tmin = 0\n",
    "        tmax = 4\n",
    "        epochs = mne.Epochs(self.raw, events, event_id, tmin, tmax, proj=True, \n",
    "                            picks=picks, baseline=None, preload=True)\n",
    "        return epochs\n",
    "    \n",
    "    def create_epochs(self):\n",
    "        print(\">>> Create Epochs.\")\n",
    "        events, event_id = self.get_events()\n",
    "        self.epochs = self.get_epochs(events, event_id)\n",
    "        return events , event_id\n",
    "        \n",
    "        print(\"Done.\")\n",
    "    \n",
    "    def get_X_y(self):\n",
    "        if self.epochs is None:\n",
    "            events , event_id=self.create_epochs()\n",
    "        self.X = self.epochs.get_data()\n",
    "        self.y = self.epochs.events[:, -1]\n",
    "        return self.X, self.y\n",
    "    \n",
    "    \n",
    "    def data_to_raw(self):\n",
    "        fullpath = os.path.join(self.path, *self.subpath.split(sep='/'))\n",
    "        #print(f\">>> Extract all subjects from: {fullpath}.\")\n",
    "        extension = \"edf\"\n",
    "        raws = []\n",
    "        count = 1\n",
    "        for i, subject in enumerate(self.subjects):\n",
    "            sname = f\"S{str(subject).zfill(3)}\".upper()\n",
    "            \n",
    "            for j, run in enumerate(self.runs):\n",
    "                rname = f\"{sname}R{str(run).zfill(2)}\".upper()\n",
    "                path_file = os.path.join(fullpath, sname, f'{rname}.{extension}')\n",
    "                #print(path_file)\n",
    "                #print(f\"Loading file #{count}/{len(self.subjects)*len(self.runs)}: {f'{rname}.{extension}'}\")\n",
    "                raw = mne.io.read_raw_edf( path_file , preload=True, verbose='WARNING' )\n",
    "                raws.append(raw)\n",
    "                count += 1\n",
    "\n",
    "        raw = mne.io.concatenate_raws(raws)\n",
    "        eegbci.standardize(raw)\n",
    "        montage = mne.channels.make_standard_montage('standard_1005')\n",
    "        raw.set_montage(montage)\n",
    "        self.raw = raw\n",
    "        \n",
    "        \n",
    "        \n",
    "def do_plot(train_loss, valid_loss,ty):\n",
    "    if ty == \"loss\":\n",
    "        plt.figure(figsize=(10,10))\n",
    "        \n",
    "        plt.plot(train_loss, label='train_loss')\n",
    "        plt.plot(valid_loss, label='valid_loss')\n",
    "        plt.title('loss {}'.format(iter))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    if ty == \"acc\":\n",
    "        plt.figure(figsize=(10,10))\n",
    "        \n",
    "        plot_ty=torch.tensor(train_loss, device = 'cpu')\n",
    "        plat_va=torch.tensor(valid_loss, device = 'cpu')\n",
    "        plt.plot(plot_ty, label='train_acc')\n",
    "        plt.plot(plat_va, label='valid_acc')\n",
    "        plt.title('acc {}'.format(iter))\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae451e3-7226-4512-803d-7fb4ac9d97ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw done\n",
      ">>> Apply filter.\n",
      "Filtering raw data in 237 contiguous segments\n",
      "Setting up band-pass filter from 1 - 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 529 samples (3.306 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter done\n",
      ">>> Create Epochs.\n",
      "Used Annotations descriptions: ['T1', 'T2']\n",
      "Not setting metadata\n",
      "3555 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 3555 events and 641 original time points ...\n",
      "16 bad epochs dropped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[    672,       0,       1],\n",
       "        [   2000,       0,       0],\n",
       "        [   3328,       0,       0],\n",
       "        ...,\n",
       "        [4676928,       0,       1],\n",
       "        [4678256,       0,       0],\n",
       "        [4679584,       0,       1]]),\n",
       " {'T1': 0, 'T2': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# home directory + datasets folder\n",
    "#path = '/content/drive/MyDrive/MNE-eegbci-data/files/eegmmidb/'\n",
    "path = 'physionet.org/files/eegmmidb/1.0.0'\n",
    "base_url = 'https://physionet.org/files/eegmmidb/'\n",
    "# subjects = [1]\n",
    "#runs = [3, 4, 7, 8, 11, 12]\n",
    "runs = [4, 8, 12]\n",
    "subjects = [i for i in range(1,80)]\n",
    "#subjects = [1]\n",
    "# runs = [6,10,14]\n",
    "\n",
    "eeg = EEG(path, base_url, subjects, runs)\n",
    "raw=eeg.data_to_raw()\n",
    "print(\"Raw done\")\n",
    "# apply filter\n",
    "freq = (1., 40.)\n",
    "raw=eeg.filter(freq=freq)\n",
    "#raw=eeg.data_to_raw()\n",
    "print(\"Filter done\")\n",
    "#raw=eeg.raw_ica()\n",
    "eeg.create_epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb5bd71f-279e-4593-a5de-e4e5f03e2b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3539, 64, 641) (3539,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3539, 64, 641)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = eeg.get_X_y()\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "X = X[:,:,:]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f873f14e-8b89-467a-90d4-f2e7d32b1a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3539, 2, 641)\n"
     ]
    }
   ],
   "source": [
    "# pick 7 channels.\n",
    "#X2 = X[:, :14, :]\n",
    "# = X2\n",
    "\n",
    "# pick C3 and C4 channels.\n",
    "X2 = X[:, 1:2, :] \n",
    "X3= X[:, 5:6, :]\n",
    "X = np.concatenate((X2,X3), axis=1)\n",
    "# = X4\n",
    "#X = X[:, :,np.newaxis,:]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab9349c4-3deb-4fac-8d1b-34967029df07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size (2477, 2, 641) (2477,)\n",
      "Test size (1062, 2, 641) (1062,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print('train size',X_train.shape, y_train.shape)\n",
    "print('Test size',X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1021a43-9e1d-47b0-90dc-f68a61c0735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "def create_dataloader(X, y, batch_size):\n",
    "    X_tensor = torch.tensor(X).float()\n",
    "    y_tensor = torch.tensor(y).long()\n",
    "    dataset_tensor = TensorDataset(X_tensor, y_tensor)\n",
    "    dl = torch.utils.data.DataLoader(dataset_tensor, batch_size=batch_size, shuffle=True)\n",
    "    return dl\n",
    "train_loader = create_dataloader(X_train, y_train, batch_size=batch_size)\n",
    "test_loader = create_dataloader(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "num_step =math.ceil(len(train_loader.dataset) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31973bf8-ef03-462e-9948-67853413b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        #using sequential helps bind multiple operations together\n",
    "        self.layer1 = nn.Sequential(\n",
    "            #padding = (kernel_size - 1) / 2 = 2\n",
    "            nn.Conv1d(2, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc = nn.Linear(82048, 2)\n",
    "        self.drop_out = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.drop_out(out)\n",
    "        out = out.reshape(out.size(0), -1)   #can also use .view()\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "847904d9-7064-4a92-a855-35b78adc28a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1bephe0c) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/train_accuracy</td><td>▁▁▁▂▂▃▃▄▄▄▄▄▄▅▄▅▅▅▆▆▆▆▆▇▆▆▇▇▇▇▇█████████</td></tr><tr><td>train/train_loss</td><td>▇▇▆█▇▇▃▇▇█▅▃▆▇▄▅▆▆▄▄▇▄▃▆█▆█▅▄▃▃▃█▂▅▅▂▄▁▂</td></tr><tr><td>val/val_accuracy</td><td>▁▂▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>val/val_loss</td><td>███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▃▃▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/train_accuracy</td><td>69.76181</td></tr><tr><td>train/train_loss</td><td>0.66231</td></tr><tr><td>val/val_accuracy</td><td>70.90395</td></tr><tr><td>val/val_loss</td><td>0.59404</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">physionet_ourDATA_CNN_2ch_2class</strong>: <a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery/runs/1bephe0c\" target=\"_blank\">https://wandb.ai/nutapol-1997/Motor-Imagery/runs/1bephe0c</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220830_103217-1bephe0c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1bephe0c). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nutapolt/eeg_mi/wandb/run-20220830_104316-cvmkqpaf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery/runs/cvmkqpaf\" target=\"_blank\">physionet_ourDATA_CNN_2ch_2class</a></strong> to <a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "wand = wandb.init(\n",
    "        \n",
    "      # Set the project where this run will be logged\n",
    "      project=\"Motor-Imagery\", \n",
    "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"physionet_ourDATA_CNN_2ch_2class\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      config={\n",
    "      \"learning_rate\": 0.0000001,\n",
    "      \"architecture\": \"CNN\",\n",
    "      \"dataset\": \"Nutapol T.\",\n",
    "      \"epochs\": 100000,\n",
    "      \"weightname\":\"physionet_ourDATA_CNN_2ch_2class\",\n",
    "      \"num_step_per_epoch\" : num_step, \n",
    "        \n",
    "      }\n",
    "    )\n",
    "\n",
    "config = wand.config\n",
    "print(config.num_step_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9938f13-d236-4986-be96-dcf8267abbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100000, Tr Loss: 0.7325, Tr Acc: 50.0202, Val Loss: 0.6931, Val Acc: 51.0358\n",
      "Epoch 51/100000, Tr Loss: 0.7154, Tr Acc: 53.4114, Val Loss: 0.6921, Val Acc: 52.6365\n",
      "Epoch 101/100000, Tr Loss: 0.7013, Tr Acc: 55.0262, Val Loss: 0.6866, Val Acc: 54.9906\n",
      "Epoch 151/100000, Tr Loss: 0.6944, Tr Acc: 56.0759, Val Loss: 0.6807, Val Acc: 56.6855\n",
      "Epoch 201/100000, Tr Loss: 0.6822, Tr Acc: 57.1256, Val Loss: 0.6748, Val Acc: 58.8512\n",
      "Epoch 251/100000, Tr Loss: 0.6734, Tr Acc: 60.5975, Val Loss: 0.6686, Val Acc: 59.9812\n",
      "Epoch 351/100000, Tr Loss: 0.6576, Tr Acc: 61.1223, Val Loss: 0.6624, Val Acc: 62.9002\n",
      "Epoch 401/100000, Tr Loss: 0.6553, Tr Acc: 62.6161, Val Loss: 0.6541, Val Acc: 63.6535\n",
      "Epoch 451/100000, Tr Loss: 0.6408, Tr Acc: 63.7868, Val Loss: 0.6503, Val Acc: 64.4068\n",
      "Epoch 501/100000, Tr Loss: 0.6315, Tr Acc: 64.7558, Val Loss: 0.6476, Val Acc: 64.8776\n",
      "Epoch 551/100000, Tr Loss: 0.6424, Tr Acc: 63.7061, Val Loss: 0.6483, Val Acc: 65.6309\n",
      "Epoch 601/100000, Tr Loss: 0.6228, Tr Acc: 64.7154, Val Loss: 0.6469, Val Acc: 66.1959\n",
      "Epoch 651/100000, Tr Loss: 0.6141, Tr Acc: 67.0165, Val Loss: 0.6427, Val Acc: 66.5725\n",
      "Epoch 701/100000, Tr Loss: 0.6139, Tr Acc: 67.2184, Val Loss: 0.6351, Val Acc: 67.6083\n",
      "Epoch 751/100000, Tr Loss: 0.6076, Tr Acc: 68.2681, Val Loss: 0.6275, Val Acc: 67.7024\n",
      "Epoch 801/100000, Tr Loss: 0.6082, Tr Acc: 67.5817, Val Loss: 0.6263, Val Acc: 68.0791\n",
      "Epoch 851/100000, Tr Loss: 0.5980, Tr Acc: 68.6314, Val Loss: 0.6228, Val Acc: 68.3616\n",
      "Epoch 901/100000, Tr Loss: 0.5859, Tr Acc: 68.9544, Val Loss: 0.6203, Val Acc: 68.8324\n",
      "Epoch 951/100000, Tr Loss: 0.5730, Tr Acc: 70.4885, Val Loss: 0.6156, Val Acc: 69.0207\n",
      "Epoch 1001/100000, Tr Loss: 0.5819, Tr Acc: 69.3985, Val Loss: 0.6136, Val Acc: 69.4915\n",
      "Epoch 1051/100000, Tr Loss: 0.5758, Tr Acc: 70.1655, Val Loss: 0.6034, Val Acc: 69.2090\n",
      "Epoch 1101/100000, Tr Loss: 0.5620, Tr Acc: 72.3859, Val Loss: 0.6100, Val Acc: 70.0565\n",
      "Epoch 1151/100000, Tr Loss: 0.5583, Tr Acc: 72.7089, Val Loss: 0.5988, Val Acc: 69.8682\n",
      "Epoch 1201/100000, Tr Loss: 0.5464, Tr Acc: 73.1934, Val Loss: 0.5956, Val Acc: 70.0565\n",
      "Epoch 1251/100000, Tr Loss: 0.5529, Tr Acc: 72.7089, Val Loss: 0.5922, Val Acc: 69.9623\n",
      "Epoch 1301/100000, Tr Loss: 0.5530, Tr Acc: 73.4356, Val Loss: 0.5906, Val Acc: 70.2448\n",
      "Epoch 1351/100000, Tr Loss: 0.5422, Tr Acc: 73.7990, Val Loss: 0.5901, Val Acc: 70.4331\n",
      "Epoch 1401/100000, Tr Loss: 0.5326, Tr Acc: 74.9294, Val Loss: 0.5929, Val Acc: 70.4331\n",
      "Epoch 1451/100000, Tr Loss: 0.5299, Tr Acc: 75.3734, Val Loss: 0.5848, Val Acc: 70.7156\n",
      "Epoch 1501/100000, Tr Loss: 0.5309, Tr Acc: 73.7586, Val Loss: 0.5898, Val Acc: 70.8098\n",
      "Epoch 1551/100000, Tr Loss: 0.5288, Tr Acc: 74.6064, Val Loss: 0.5825, Val Acc: 71.1864\n",
      "Epoch 1601/100000, Tr Loss: 0.5299, Tr Acc: 74.2430, Val Loss: 0.5837, Val Acc: 71.4689\n",
      "Epoch 1651/100000, Tr Loss: 0.5170, Tr Acc: 75.4138, Val Loss: 0.5867, Val Acc: 71.4689\n",
      "Epoch 1701/100000, Tr Loss: 0.5180, Tr Acc: 75.3734, Val Loss: 0.5830, Val Acc: 71.4689\n",
      "Epoch 1751/100000, Tr Loss: 0.5132, Tr Acc: 75.3331, Val Loss: 0.5872, Val Acc: 71.3748\n",
      "Epoch 1801/100000, Tr Loss: 0.5139, Tr Acc: 74.6064, Val Loss: 0.5782, Val Acc: 71.7514\n",
      "Epoch 1851/100000, Tr Loss: 0.5162, Tr Acc: 74.4853, Val Loss: 0.5829, Val Acc: 71.9397\n",
      "Epoch 1901/100000, Tr Loss: 0.5014, Tr Acc: 75.9790, Val Loss: 0.5771, Val Acc: 71.8456\n",
      "Epoch 1951/100000, Tr Loss: 0.5012, Tr Acc: 75.8579, Val Loss: 0.5754, Val Acc: 71.6573\n",
      "Epoch 2001/100000, Tr Loss: 0.5021, Tr Acc: 76.3020, Val Loss: 0.5799, Val Acc: 71.8456\n",
      "Epoch 2051/100000, Tr Loss: 0.4917, Tr Acc: 76.2212, Val Loss: 0.5738, Val Acc: 72.0339\n",
      "Epoch 2101/100000, Tr Loss: 0.5039, Tr Acc: 75.9386, Val Loss: 0.5741, Val Acc: 72.3164\n",
      "Epoch 2151/100000, Tr Loss: 0.5004, Tr Acc: 76.7057, Val Loss: 0.5735, Val Acc: 72.0339\n",
      "Epoch 2201/100000, Tr Loss: 0.4929, Tr Acc: 76.1405, Val Loss: 0.5726, Val Acc: 72.2222\n",
      "Epoch 2251/100000, Tr Loss: 0.5012, Tr Acc: 76.6653, Val Loss: 0.5712, Val Acc: 71.8456\n",
      "Epoch 2301/100000, Tr Loss: 0.4902, Tr Acc: 75.9790, Val Loss: 0.5710, Val Acc: 72.0339\n",
      "Epoch 2351/100000, Tr Loss: 0.4926, Tr Acc: 76.3423, Val Loss: 0.5720, Val Acc: 71.9397\n",
      "Epoch 2401/100000, Tr Loss: 0.4854, Tr Acc: 77.5939, Val Loss: 0.5743, Val Acc: 72.0339\n",
      "Epoch 2451/100000, Tr Loss: 0.4853, Tr Acc: 76.4635, Val Loss: 0.5743, Val Acc: 72.2222\n",
      "Epoch 2501/100000, Tr Loss: 0.4796, Tr Acc: 77.8361, Val Loss: 0.5716, Val Acc: 71.7514\n",
      "Epoch 2551/100000, Tr Loss: 0.4839, Tr Acc: 77.3113, Val Loss: 0.5677, Val Acc: 71.8456\n",
      "Epoch 2601/100000, Tr Loss: 0.4820, Tr Acc: 76.3827, Val Loss: 0.5707, Val Acc: 71.9397\n",
      "Epoch 2651/100000, Tr Loss: 0.4722, Tr Acc: 77.8361, Val Loss: 0.5765, Val Acc: 71.9397\n",
      "Epoch 2701/100000, Tr Loss: 0.4713, Tr Acc: 76.9883, Val Loss: 0.5687, Val Acc: 71.8456\n",
      "Epoch 2751/100000, Tr Loss: 0.4670, Tr Acc: 78.7243, Val Loss: 0.5731, Val Acc: 71.7514\n",
      "Epoch 2801/100000, Tr Loss: 0.4762, Tr Acc: 77.4324, Val Loss: 0.5730, Val Acc: 71.7514\n",
      "Epoch 2851/100000, Tr Loss: 0.4747, Tr Acc: 77.1498, Val Loss: 0.5695, Val Acc: 71.3748\n",
      "Epoch 2901/100000, Tr Loss: 0.4678, Tr Acc: 79.0876, Val Loss: 0.5709, Val Acc: 71.4689\n",
      "Epoch 2951/100000, Tr Loss: 0.4633, Tr Acc: 78.2398, Val Loss: 0.5726, Val Acc: 71.5631\n",
      "Epoch 3001/100000, Tr Loss: 0.4713, Tr Acc: 78.5628, Val Loss: 0.5713, Val Acc: 71.3748\n",
      "Epoch 3051/100000, Tr Loss: 0.4628, Tr Acc: 77.9168, Val Loss: 0.5699, Val Acc: 71.5631\n",
      "Epoch 3101/100000, Tr Loss: 0.4572, Tr Acc: 79.0472, Val Loss: 0.5739, Val Acc: 71.4689\n",
      "Epoch 3151/100000, Tr Loss: 0.4591, Tr Acc: 78.2802, Val Loss: 0.5699, Val Acc: 71.2806\n",
      "Epoch 3201/100000, Tr Loss: 0.4584, Tr Acc: 79.5721, Val Loss: 0.5743, Val Acc: 71.0923\n",
      "Epoch 3251/100000, Tr Loss: 0.4591, Tr Acc: 78.4013, Val Loss: 0.5725, Val Acc: 71.1864\n",
      "Epoch 3301/100000, Tr Loss: 0.4519, Tr Acc: 79.6932, Val Loss: 0.5738, Val Acc: 70.9981\n",
      "Epoch 3351/100000, Tr Loss: 0.4600, Tr Acc: 77.8361, Val Loss: 0.5769, Val Acc: 70.9981\n",
      "Epoch 3401/100000, Tr Loss: 0.4461, Tr Acc: 79.5721, Val Loss: 0.5709, Val Acc: 71.2806\n",
      "Epoch 3451/100000, Tr Loss: 0.4579, Tr Acc: 78.6839, Val Loss: 0.5862, Val Acc: 70.7156\n",
      "Epoch 3501/100000, Tr Loss: 0.4495, Tr Acc: 79.6124, Val Loss: 0.5735, Val Acc: 70.9981\n",
      "Epoch 3551/100000, Tr Loss: 0.4496, Tr Acc: 78.6435, Val Loss: 0.5757, Val Acc: 70.7156\n",
      "Epoch 3601/100000, Tr Loss: 0.4471, Tr Acc: 79.6124, Val Loss: 0.5671, Val Acc: 70.9981\n",
      "Epoch 3651/100000, Tr Loss: 0.4494, Tr Acc: 79.0472, Val Loss: 0.5712, Val Acc: 70.7156\n",
      "Epoch 3701/100000, Tr Loss: 0.4456, Tr Acc: 79.5317, Val Loss: 0.5755, Val Acc: 70.5273\n",
      "Epoch 3751/100000, Tr Loss: 0.4481, Tr Acc: 79.6528, Val Loss: 0.5752, Val Acc: 70.7156\n",
      "Epoch 3801/100000, Tr Loss: 0.4522, Tr Acc: 79.0069, Val Loss: 0.5791, Val Acc: 70.5273\n",
      "Epoch 3851/100000, Tr Loss: 0.4400, Tr Acc: 79.5317, Val Loss: 0.5771, Val Acc: 70.3390\n",
      "Epoch 3901/100000, Tr Loss: 0.4397, Tr Acc: 79.6932, Val Loss: 0.5759, Val Acc: 70.3390\n",
      "Epoch 3951/100000, Tr Loss: 0.4364, Tr Acc: 80.2987, Val Loss: 0.5782, Val Acc: 70.3390\n",
      "Epoch 4001/100000, Tr Loss: 0.4422, Tr Acc: 79.2491, Val Loss: 0.5778, Val Acc: 70.2448\n",
      "Epoch 4051/100000, Tr Loss: 0.4383, Tr Acc: 79.6932, Val Loss: 0.5915, Val Acc: 70.1507\n",
      "Epoch 4101/100000, Tr Loss: 0.4434, Tr Acc: 80.0161, Val Loss: 0.5768, Val Acc: 70.3390\n",
      "Epoch 4151/100000, Tr Loss: 0.4335, Tr Acc: 80.2987, Val Loss: 0.5880, Val Acc: 70.4331\n",
      "Epoch 4201/100000, Tr Loss: 0.4343, Tr Acc: 80.3391, Val Loss: 0.5754, Val Acc: 70.4331\n",
      "Epoch 4251/100000, Tr Loss: 0.4347, Tr Acc: 80.0565, Val Loss: 0.5949, Val Acc: 70.5273\n",
      "Epoch 4301/100000, Tr Loss: 0.4321, Tr Acc: 80.5410, Val Loss: 0.5753, Val Acc: 70.3390\n",
      "Epoch 4351/100000, Tr Loss: 0.4306, Tr Acc: 79.9758, Val Loss: 0.5812, Val Acc: 70.3390\n",
      "Epoch 4401/100000, Tr Loss: 0.4317, Tr Acc: 80.9043, Val Loss: 0.5738, Val Acc: 70.3390\n",
      "Epoch 4451/100000, Tr Loss: 0.4311, Tr Acc: 80.2584, Val Loss: 0.5772, Val Acc: 70.4331\n",
      "Epoch 4501/100000, Tr Loss: 0.4207, Tr Acc: 80.8639, Val Loss: 0.5942, Val Acc: 70.7156\n",
      "Epoch 4551/100000, Tr Loss: 0.4229, Tr Acc: 81.3484, Val Loss: 0.5770, Val Acc: 70.4331\n",
      "Epoch 4601/100000, Tr Loss: 0.4245, Tr Acc: 81.0254, Val Loss: 0.5806, Val Acc: 70.8098\n",
      "Epoch 4651/100000, Tr Loss: 0.4244, Tr Acc: 81.1062, Val Loss: 0.5837, Val Acc: 70.5273\n",
      "Epoch 4701/100000, Tr Loss: 0.4196, Tr Acc: 81.1465, Val Loss: 0.5840, Val Acc: 70.6215\n",
      "Epoch 4751/100000, Tr Loss: 0.4202, Tr Acc: 81.2677, Val Loss: 0.5855, Val Acc: 70.9040\n",
      "Epoch 4801/100000, Tr Loss: 0.4248, Tr Acc: 80.4199, Val Loss: 0.5817, Val Acc: 70.8098\n",
      "Epoch 4851/100000, Tr Loss: 0.4279, Tr Acc: 79.9354, Val Loss: 0.5839, Val Acc: 70.7156\n",
      "Epoch 4901/100000, Tr Loss: 0.4125, Tr Acc: 81.1869, Val Loss: 0.5775, Val Acc: 70.8098\n",
      "Epoch 4951/100000, Tr Loss: 0.4193, Tr Acc: 81.1869, Val Loss: 0.5938, Val Acc: 70.7156\n",
      "Epoch 5001/100000, Tr Loss: 0.4157, Tr Acc: 81.4695, Val Loss: 0.5813, Val Acc: 70.6215\n",
      "Epoch 5051/100000, Tr Loss: 0.4233, Tr Acc: 80.5006, Val Loss: 0.5970, Val Acc: 70.7156\n",
      "Epoch 5101/100000, Tr Loss: 0.4079, Tr Acc: 81.2273, Val Loss: 0.5867, Val Acc: 70.7156\n",
      "Epoch 5151/100000, Tr Loss: 0.4143, Tr Acc: 81.1062, Val Loss: 0.5987, Val Acc: 70.6215\n",
      "Epoch 5201/100000, Tr Loss: 0.4093, Tr Acc: 81.5099, Val Loss: 0.5950, Val Acc: 70.8098\n",
      "Epoch 5251/100000, Tr Loss: 0.4209, Tr Acc: 80.4199, Val Loss: 0.5811, Val Acc: 70.5273\n",
      "Epoch 5301/100000, Tr Loss: 0.4183, Tr Acc: 80.6621, Val Loss: 0.5860, Val Acc: 70.7156\n",
      "Epoch 5351/100000, Tr Loss: 0.4139, Tr Acc: 81.6310, Val Loss: 0.5814, Val Acc: 70.9040\n",
      "Epoch 5401/100000, Tr Loss: 0.4176, Tr Acc: 80.3795, Val Loss: 0.5841, Val Acc: 70.8098\n",
      "Epoch 5451/100000, Tr Loss: 0.4041, Tr Acc: 81.6714, Val Loss: 0.5925, Val Acc: 70.6215\n",
      "Epoch 5501/100000, Tr Loss: 0.4157, Tr Acc: 81.9943, Val Loss: 0.5854, Val Acc: 70.6215\n",
      "Epoch 5551/100000, Tr Loss: 0.4004, Tr Acc: 82.4788, Val Loss: 0.5893, Val Acc: 70.9040\n",
      "Epoch 5601/100000, Tr Loss: 0.4068, Tr Acc: 81.7925, Val Loss: 0.5883, Val Acc: 70.9040\n",
      "Epoch 5651/100000, Tr Loss: 0.4145, Tr Acc: 80.5410, Val Loss: 0.5847, Val Acc: 70.6215\n",
      "Epoch 5701/100000, Tr Loss: 0.4011, Tr Acc: 81.9136, Val Loss: 0.5875, Val Acc: 70.8098\n",
      "Epoch 5751/100000, Tr Loss: 0.4049, Tr Acc: 82.5192, Val Loss: 0.5826, Val Acc: 70.9040\n",
      "Epoch 5801/100000, Tr Loss: 0.4012, Tr Acc: 82.3173, Val Loss: 0.6029, Val Acc: 71.3748\n",
      "Epoch 5851/100000, Tr Loss: 0.3988, Tr Acc: 82.1962, Val Loss: 0.5863, Val Acc: 70.9981\n",
      "Epoch 5901/100000, Tr Loss: 0.3996, Tr Acc: 81.8732, Val Loss: 0.5967, Val Acc: 70.8098\n",
      "Epoch 5951/100000, Tr Loss: 0.3961, Tr Acc: 82.6403, Val Loss: 0.5825, Val Acc: 71.0923\n",
      "Epoch 6001/100000, Tr Loss: 0.3991, Tr Acc: 82.1155, Val Loss: 0.5921, Val Acc: 70.9040\n",
      "Epoch 6051/100000, Tr Loss: 0.3956, Tr Acc: 82.1558, Val Loss: 0.5886, Val Acc: 71.0923\n",
      "Epoch 6101/100000, Tr Loss: 0.3955, Tr Acc: 81.7925, Val Loss: 0.5850, Val Acc: 71.1864\n",
      "Epoch 6151/100000, Tr Loss: 0.3922, Tr Acc: 82.4788, Val Loss: 0.5920, Val Acc: 71.0923\n",
      "Epoch 6201/100000, Tr Loss: 0.3967, Tr Acc: 82.5999, Val Loss: 0.5889, Val Acc: 70.9981\n",
      "Epoch 6251/100000, Tr Loss: 0.3985, Tr Acc: 81.9943, Val Loss: 0.5830, Val Acc: 71.0923\n",
      "Epoch 6301/100000, Tr Loss: 0.3920, Tr Acc: 82.3577, Val Loss: 0.6172, Val Acc: 71.1864\n",
      "Epoch 6351/100000, Tr Loss: 0.3920, Tr Acc: 82.4788, Val Loss: 0.5852, Val Acc: 71.0923\n",
      "Epoch 6401/100000, Tr Loss: 0.3870, Tr Acc: 83.0440, Val Loss: 0.5910, Val Acc: 71.0923\n",
      "Epoch 6451/100000, Tr Loss: 0.3910, Tr Acc: 82.4384, Val Loss: 0.6040, Val Acc: 71.0923\n",
      "Epoch 6501/100000, Tr Loss: 0.3869, Tr Acc: 83.5285, Val Loss: 0.5934, Val Acc: 71.4689\n",
      "Epoch 6551/100000, Tr Loss: 0.3803, Tr Acc: 82.8421, Val Loss: 0.5918, Val Acc: 71.3748\n",
      "Epoch 6601/100000, Tr Loss: 0.3917, Tr Acc: 82.1962, Val Loss: 0.5998, Val Acc: 71.4689\n",
      "Epoch 6651/100000, Tr Loss: 0.3867, Tr Acc: 83.0440, Val Loss: 0.5952, Val Acc: 71.4689\n",
      "Epoch 6701/100000, Tr Loss: 0.3908, Tr Acc: 83.2862, Val Loss: 0.5967, Val Acc: 71.2806\n",
      "Epoch 6751/100000, Tr Loss: 0.3798, Tr Acc: 84.2148, Val Loss: 0.5913, Val Acc: 70.9981\n",
      "Epoch 6801/100000, Tr Loss: 0.3825, Tr Acc: 82.8825, Val Loss: 0.5949, Val Acc: 70.9981\n",
      "Epoch 6851/100000, Tr Loss: 0.3839, Tr Acc: 83.2055, Val Loss: 0.5891, Val Acc: 71.1864\n",
      "Epoch 6901/100000, Tr Loss: 0.3824, Tr Acc: 83.7303, Val Loss: 0.5917, Val Acc: 71.1864\n",
      "Epoch 6951/100000, Tr Loss: 0.3792, Tr Acc: 83.4881, Val Loss: 0.6401, Val Acc: 71.1864\n",
      "Epoch 7001/100000, Tr Loss: 0.3811, Tr Acc: 83.2862, Val Loss: 0.6030, Val Acc: 71.0923\n",
      "Epoch 7051/100000, Tr Loss: 0.3858, Tr Acc: 82.7614, Val Loss: 0.5922, Val Acc: 71.0923\n",
      "Epoch 7101/100000, Tr Loss: 0.3689, Tr Acc: 83.6899, Val Loss: 0.6152, Val Acc: 71.4689\n",
      "Epoch 7151/100000, Tr Loss: 0.3762, Tr Acc: 83.6496, Val Loss: 0.6198, Val Acc: 71.1864\n",
      "Epoch 7201/100000, Tr Loss: 0.3819, Tr Acc: 83.4881, Val Loss: 0.6128, Val Acc: 71.2806\n",
      "Epoch 7251/100000, Tr Loss: 0.3778, Tr Acc: 84.0129, Val Loss: 0.5960, Val Acc: 71.2806\n",
      "Epoch 7301/100000, Tr Loss: 0.3723, Tr Acc: 83.6496, Val Loss: 0.6005, Val Acc: 71.2806\n",
      "Epoch 7351/100000, Tr Loss: 0.3777, Tr Acc: 83.4073, Val Loss: 0.6072, Val Acc: 71.3748\n",
      "Epoch 7401/100000, Tr Loss: 0.3706, Tr Acc: 84.4974, Val Loss: 0.6145, Val Acc: 71.5631\n",
      "Epoch 7451/100000, Tr Loss: 0.3746, Tr Acc: 84.4166, Val Loss: 0.6142, Val Acc: 71.3748\n",
      "Epoch 7501/100000, Tr Loss: 0.3747, Tr Acc: 83.4477, Val Loss: 0.5986, Val Acc: 71.3748\n",
      "Epoch 7551/100000, Tr Loss: 0.3710, Tr Acc: 84.3359, Val Loss: 0.6028, Val Acc: 71.4689\n",
      "Epoch 7601/100000, Tr Loss: 0.3704, Tr Acc: 84.4570, Val Loss: 0.5927, Val Acc: 71.3748\n",
      "Epoch 7651/100000, Tr Loss: 0.3740, Tr Acc: 83.0440, Val Loss: 0.6428, Val Acc: 71.4689\n",
      "Epoch 7701/100000, Tr Loss: 0.3805, Tr Acc: 83.2055, Val Loss: 0.5979, Val Acc: 71.3748\n",
      "Epoch 7751/100000, Tr Loss: 0.3659, Tr Acc: 84.3763, Val Loss: 0.6200, Val Acc: 71.5631\n",
      "Epoch 7801/100000, Tr Loss: 0.3636, Tr Acc: 84.4570, Val Loss: 0.5969, Val Acc: 71.2806\n",
      "Epoch 7851/100000, Tr Loss: 0.3613, Tr Acc: 84.5377, Val Loss: 0.6040, Val Acc: 71.5631\n",
      "Epoch 7901/100000, Tr Loss: 0.3671, Tr Acc: 84.5377, Val Loss: 0.6063, Val Acc: 71.4689\n",
      "Epoch 7951/100000, Tr Loss: 0.3570, Tr Acc: 84.7396, Val Loss: 0.6118, Val Acc: 71.5631\n",
      "Epoch 8001/100000, Tr Loss: 0.3652, Tr Acc: 84.1340, Val Loss: 0.6018, Val Acc: 71.2806\n",
      "Epoch 8051/100000, Tr Loss: 0.3738, Tr Acc: 83.4477, Val Loss: 0.6012, Val Acc: 71.3748\n",
      "Epoch 8101/100000, Tr Loss: 0.3579, Tr Acc: 83.7707, Val Loss: 0.6103, Val Acc: 71.7514\n",
      "Epoch 8151/100000, Tr Loss: 0.3664, Tr Acc: 83.8514, Val Loss: 0.6007, Val Acc: 71.5631\n",
      "Epoch 8201/100000, Tr Loss: 0.3613, Tr Acc: 84.6185, Val Loss: 0.6043, Val Acc: 71.4689\n",
      "Epoch 8251/100000, Tr Loss: 0.3622, Tr Acc: 84.8607, Val Loss: 0.6093, Val Acc: 71.3748\n",
      "Epoch 8301/100000, Tr Loss: 0.3637, Tr Acc: 84.4166, Val Loss: 0.6519, Val Acc: 71.3748\n",
      "Epoch 8351/100000, Tr Loss: 0.3645, Tr Acc: 84.0533, Val Loss: 0.6098, Val Acc: 71.4689\n",
      "Epoch 8401/100000, Tr Loss: 0.3585, Tr Acc: 84.2148, Val Loss: 0.6005, Val Acc: 71.4689\n",
      "Epoch 8451/100000, Tr Loss: 0.3634, Tr Acc: 84.4974, Val Loss: 0.6044, Val Acc: 71.2806\n",
      "Epoch 8501/100000, Tr Loss: 0.3546, Tr Acc: 84.5377, Val Loss: 0.6128, Val Acc: 71.3748\n",
      "Epoch 8551/100000, Tr Loss: 0.3547, Tr Acc: 84.9415, Val Loss: 0.6091, Val Acc: 71.4689\n",
      "Epoch 8601/100000, Tr Loss: 0.3667, Tr Acc: 83.6092, Val Loss: 0.6111, Val Acc: 71.5631\n",
      "Epoch 8651/100000, Tr Loss: 0.3576, Tr Acc: 84.1744, Val Loss: 0.6051, Val Acc: 71.5631\n",
      "Epoch 8701/100000, Tr Loss: 0.3536, Tr Acc: 85.1433, Val Loss: 0.6226, Val Acc: 71.3748\n",
      "Epoch 8751/100000, Tr Loss: 0.3501, Tr Acc: 85.2644, Val Loss: 0.6217, Val Acc: 71.5631\n",
      "Epoch 8801/100000, Tr Loss: 0.3569, Tr Acc: 85.0222, Val Loss: 0.6103, Val Acc: 71.5631\n",
      "Epoch 8851/100000, Tr Loss: 0.3586, Tr Acc: 84.8203, Val Loss: 0.6138, Val Acc: 71.4689\n",
      "Epoch 8901/100000, Tr Loss: 0.3528, Tr Acc: 84.8203, Val Loss: 0.6298, Val Acc: 71.3748\n",
      "Epoch 8951/100000, Tr Loss: 0.3489, Tr Acc: 84.6992, Val Loss: 0.6148, Val Acc: 71.3748\n",
      "Epoch 9001/100000, Tr Loss: 0.3555, Tr Acc: 84.6589, Val Loss: 0.6291, Val Acc: 71.3748\n",
      "Epoch 9051/100000, Tr Loss: 0.3444, Tr Acc: 85.2644, Val Loss: 0.6165, Val Acc: 71.5631\n",
      "Epoch 9101/100000, Tr Loss: 0.3479, Tr Acc: 85.7893, Val Loss: 0.6152, Val Acc: 71.2806\n",
      "Epoch 9151/100000, Tr Loss: 0.3424, Tr Acc: 85.0626, Val Loss: 0.6193, Val Acc: 71.4689\n",
      "Epoch 9201/100000, Tr Loss: 0.3464, Tr Acc: 84.8203, Val Loss: 0.6228, Val Acc: 71.1864\n",
      "Epoch 9251/100000, Tr Loss: 0.3518, Tr Acc: 84.7396, Val Loss: 0.6242, Val Acc: 71.0923\n",
      "Epoch 9301/100000, Tr Loss: 0.3434, Tr Acc: 85.4259, Val Loss: 0.6226, Val Acc: 71.2806\n",
      "Epoch 9351/100000, Tr Loss: 0.3438, Tr Acc: 85.8296, Val Loss: 0.6136, Val Acc: 71.1864\n",
      "Epoch 9401/100000, Tr Loss: 0.3421, Tr Acc: 85.2644, Val Loss: 0.6351, Val Acc: 71.1864\n",
      "Epoch 9451/100000, Tr Loss: 0.3434, Tr Acc: 85.7085, Val Loss: 0.6123, Val Acc: 71.4689\n",
      "Epoch 9501/100000, Tr Loss: 0.3448, Tr Acc: 85.3855, Val Loss: 0.6247, Val Acc: 71.2806\n",
      "Epoch 9551/100000, Tr Loss: 0.3367, Tr Acc: 86.1930, Val Loss: 0.6190, Val Acc: 71.2806\n",
      "Epoch 9601/100000, Tr Loss: 0.3459, Tr Acc: 84.9011, Val Loss: 0.6232, Val Acc: 71.4689\n",
      "Epoch 9651/100000, Tr Loss: 0.3438, Tr Acc: 85.3048, Val Loss: 0.6150, Val Acc: 71.1864\n",
      "Epoch 9701/100000, Tr Loss: 0.3284, Tr Acc: 86.0315, Val Loss: 0.6242, Val Acc: 71.3748\n",
      "Epoch 9801/100000, Tr Loss: 0.3408, Tr Acc: 85.3855, Val Loss: 0.6315, Val Acc: 71.6573\n",
      "Epoch 9851/100000, Tr Loss: 0.3400, Tr Acc: 85.8700, Val Loss: 0.6192, Val Acc: 71.1864\n",
      "Epoch 9901/100000, Tr Loss: 0.3385, Tr Acc: 85.3855, Val Loss: 0.6379, Val Acc: 71.1864\n",
      "Epoch 9951/100000, Tr Loss: 0.3368, Tr Acc: 85.3048, Val Loss: 0.6254, Val Acc: 71.2806\n",
      "Epoch 10001/100000, Tr Loss: 0.3345, Tr Acc: 85.7489, Val Loss: 0.6235, Val Acc: 71.1864\n",
      "Epoch 10051/100000, Tr Loss: 0.3375, Tr Acc: 86.1526, Val Loss: 0.6272, Val Acc: 71.1864\n",
      "Epoch 10101/100000, Tr Loss: 0.3334, Tr Acc: 86.1526, Val Loss: 0.6190, Val Acc: 70.9981\n",
      "Epoch 10151/100000, Tr Loss: 0.3294, Tr Acc: 86.0315, Val Loss: 0.6197, Val Acc: 71.1864\n",
      "Epoch 10201/100000, Tr Loss: 0.3350, Tr Acc: 86.2737, Val Loss: 0.6239, Val Acc: 71.3748\n",
      "Epoch 10251/100000, Tr Loss: 0.3351, Tr Acc: 86.3948, Val Loss: 0.6381, Val Acc: 71.3748\n",
      "Epoch 10301/100000, Tr Loss: 0.3336, Tr Acc: 86.2737, Val Loss: 0.6348, Val Acc: 71.3748\n",
      "Epoch 10351/100000, Tr Loss: 0.3277, Tr Acc: 86.1122, Val Loss: 0.6221, Val Acc: 71.4689\n",
      "Epoch 10401/100000, Tr Loss: 0.3276, Tr Acc: 86.4756, Val Loss: 0.6493, Val Acc: 71.1864\n",
      "Epoch 10451/100000, Tr Loss: 0.3314, Tr Acc: 86.4352, Val Loss: 0.6268, Val Acc: 71.0923\n",
      "Epoch 10501/100000, Tr Loss: 0.3249, Tr Acc: 86.9197, Val Loss: 0.6296, Val Acc: 71.3748\n",
      "Epoch 10551/100000, Tr Loss: 0.3250, Tr Acc: 86.3141, Val Loss: 0.6252, Val Acc: 71.1864\n",
      "Epoch 10601/100000, Tr Loss: 0.3222, Tr Acc: 85.4663, Val Loss: 0.6238, Val Acc: 71.1864\n",
      "Epoch 10651/100000, Tr Loss: 0.3250, Tr Acc: 86.1122, Val Loss: 0.6216, Val Acc: 71.4689\n",
      "Epoch 10701/100000, Tr Loss: 0.3248, Tr Acc: 86.7582, Val Loss: 0.6485, Val Acc: 70.9040\n",
      "Epoch 10751/100000, Tr Loss: 0.3275, Tr Acc: 86.0719, Val Loss: 0.6353, Val Acc: 71.7514\n",
      "Epoch 10801/100000, Tr Loss: 0.3254, Tr Acc: 86.3545, Val Loss: 0.6306, Val Acc: 71.2806\n",
      "Epoch 10851/100000, Tr Loss: 0.3199, Tr Acc: 86.7178, Val Loss: 0.6518, Val Acc: 71.0923\n",
      "Epoch 10901/100000, Tr Loss: 0.3282, Tr Acc: 85.9911, Val Loss: 0.6418, Val Acc: 71.2806\n",
      "Epoch 10951/100000, Tr Loss: 0.3260, Tr Acc: 86.3141, Val Loss: 0.6225, Val Acc: 71.1864\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlearning_rate)\n\u001b[1;32m      4\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 7\u001b[0m train_loss,valid_loss,train_accuracy,valid_accuracy \u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwand\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwand\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m         \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m wandb\u001b[38;5;241m.\u001b[39malert(\n\u001b[1;32m     19\u001b[0m             title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinish\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m             text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinishing training\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     21\u001b[0m         )\n",
      "File \u001b[0;32m~/eeg_mi/common.py:68\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, gpu_num, train_loader, test_loader, optimizer, criterion, wand)\u001b[0m\n\u001b[1;32m     65\u001b[0m     classes \u001b[38;5;241m=\u001b[39m classes\u001b[38;5;241m.\u001b[39mcuda(gpu_num)\n\u001b[1;32m     67\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 68\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, classes)\n\u001b[1;32m     71\u001b[0m iter_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 24\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_out(out)\n\u001b[1;32m     26\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:307\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:303\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    301\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    302\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from common import train\n",
    "net = ConvNet().cuda(0)\n",
    "optimizer = optim.Adam(net.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_loss,valid_loss,train_accuracy,valid_accuracy =train(\n",
    "    model = net,\n",
    "    gpu_num = 0,\n",
    "    train_loader = train_loader,\n",
    "    test_loader = test_loader,\n",
    "    optimizer = optimizer  ,\n",
    "    criterion = criterion ,\n",
    "    wand = wand\n",
    "         )\n",
    "\n",
    "\n",
    "wandb.alert(\n",
    "            title='Finish',\n",
    "            text=f'Finishing training',\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2428e1b7-fc22-470d-aa67-5de2a44c55c5",
   "metadata": {},
   "source": [
    "### GET OUR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "025b73f2-d770-4a5a-980c-eb88b7dabc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG:\n",
    "    def __init__(self, path, base_url, subjects, runs):\n",
    "        self.subpath = ''\n",
    "        self.path = path\n",
    "        print(path)\n",
    "        self.base_url = base_url\n",
    "        self.subjects = subjects\n",
    "        self.runs = runs\n",
    "        \n",
    "        # download data if does not exist in path.\n",
    "        # self.load_data()\n",
    "        self.data_to_raw()\n",
    "    \n",
    "    def load_data(self):\n",
    "        print(f\">>> Start download from: {self.base_url}.\")\n",
    "        print(f\"Downloading files to: {self.path}.\")\n",
    "        for subject in self.subjects:\n",
    "            eegbci.load_data(subject,self.runs,path=self.path,base_url=self.base_url)\n",
    "        print(\"Done.\")\n",
    "    \n",
    "    \n",
    "        \n",
    "        print(\"Done.\")\n",
    "        return self.raw\n",
    "    def filter(self, freq):\n",
    "        raw = self.raw\n",
    "        low, high = freq\n",
    "        print(f\">>> Apply filter.\")\n",
    "        self.raw.filter(low, high, fir_design='firwin', verbose=20)\n",
    "        return  raw\n",
    "    def raw_ica(self):\n",
    "        raw = self.raw\n",
    "        ica = mne.preprocessing.ICA(n_components=1, max_iter=100)\n",
    "        ica.fit(raw)\n",
    "        ica.exclude = [1, 2]  # details on how we picked these are omitted here\n",
    "        ica.plot_properties(raw, picks=ica.exclude)\n",
    "        ica.apply(raw)\n",
    "        print('ICA DONE ????')\n",
    "        return  raw\n",
    "        \n",
    "    def get_events(self):\n",
    "        event_id = dict(T1=0, T2=1) # the events we want to extract\n",
    "        events, event_id = mne.events_from_annotations(self.raw, event_id=event_id)\n",
    "        return events, event_id\n",
    "    \n",
    "    def get_epochs(self, events, event_id):\n",
    "        picks = mne.pick_types(self.raw.info, eeg=True, exclude='bads')\n",
    "        tmin = 0\n",
    "        tmax = 4\n",
    "        epochs = mne.Epochs(self.raw, events, event_id, tmin, tmax, proj=True, \n",
    "                            picks=picks, baseline=None, preload=True)\n",
    "        \n",
    "        return epochs\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    def data_to_raw(self):\n",
    "        fullpath = os.path.join(self.path, *self.subpath.split(sep='/'))\n",
    "        #print(f\">>> Extract all subjects from: {fullpath}.\")\n",
    "        extension = \"fif\"\n",
    "        raws = []\n",
    "        count = 1\n",
    "        for i, subject in enumerate(self.subjects):\n",
    "            sname = f\"S{str(subject).zfill(3)}\".upper()\n",
    "            \n",
    "            for j, run in enumerate(self.runs):\n",
    "                rname = f\"{sname}R{str(run).zfill(2)}\".upper()\n",
    "                path_file = os.path.join(fullpath, sname, f'{rname}.{extension}')\n",
    "                #print(path_file)\n",
    "                #print(f\"Loading file #{count}/{len(self.subjects)*len(self.runs)}: {f'{rname}.{extension}'}\")\n",
    "                raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
    "                raws.append(raw)\n",
    "                count += 1\n",
    "\n",
    "        raw = mne.io.concatenate_raws(raws)\n",
    "        eegbci.standardize(raw)\n",
    "        montage = mne.channels.make_standard_montage('standard_1005')\n",
    "        raw.set_montage(montage)\n",
    "        self.raw = raw\n",
    "    \n",
    "    def create_epochs(self):\n",
    "        print(\">>> Create Epochs.\")\n",
    "        \n",
    "        events, event_id = self.get_events()\n",
    "        self.epochs = self.get_epochs(events, event_id)\n",
    "        print(\"Done.\")\n",
    "        return events , event_id\n",
    "# getepoch(raw,4, 10,reject_bad=False,on_missing='warn')    \n",
    "    def get_X_y(self):\n",
    "        events = mne.find_events(raw)\n",
    "        \n",
    "        epochs = mne.Epochs(\n",
    "        raw,\n",
    "        events,\n",
    "        event_id=[1,2,3],\n",
    "        tmin=-4,\n",
    "        tmax=10,\n",
    "        picks=\"data\",\n",
    "        on_missing='warn',\n",
    "        baseline=None,\n",
    "        preload=True\n",
    "    )\n",
    "        epochs = epochs.resample(46)\n",
    "            #events , event_id=self.create_epochs()\n",
    "        self.X = epochs.get_data()\n",
    "        self.y = epochs.events[:, -1]\n",
    "        return self.X, self.y\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "def do_plot(train_loss, valid_loss,ty):\n",
    "    if ty == \"loss\":\n",
    "        plt.figure(figsize=(10,10))\n",
    "        \n",
    "        plt.plot(train_loss, label='train_loss')\n",
    "        plt.plot(valid_loss, label='valid_loss')\n",
    "        plt.title('loss {}'.format(iter))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    if ty == \"acc\":\n",
    "        plt.figure(figsize=(10,10))\n",
    "        \n",
    "        plot_ty=torch.tensor(train_loss, device = 'cpu')\n",
    "        plat_va=torch.tensor(valid_loss, device = 'cpu')\n",
    "        plt.plot(plot_ty, label='train_acc')\n",
    "        plt.plot(plat_va, label='valid_acc')\n",
    "        plt.title('acc {}'.format(iter))\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c6485-e1eb-468e-a416-22271005a522",
   "metadata": {},
   "source": [
    "# home directory + datasets folder\n",
    "#path = '/content/drive/MyDrive/MNE-eegbci-data/files/eegmmidb/'\n",
    "path = 'physionet.org/files/eegmmidb/1.0.0'\n",
    "base_url = 'https://physionet.org/files/eegmmidb/'\n",
    "# subjects = [1]\n",
    "#runs = [3, 4, 7, 8, 11, 12]\n",
    "runs = [4, 8, 12]\n",
    "subjects = [i for i in range(1,80)]\n",
    "#subjects = [1]\n",
    "# runs = [6,10,14]\n",
    "\n",
    "eeg = EEG(path, base_url, subjects, runs)\n",
    "raw=eeg.data_to_raw()\n",
    "print(\"Raw done\")\n",
    "# apply filter\n",
    "freq = (1., 40.)\n",
    "raw=eeg.filter(freq=freq)\n",
    "#raw=eeg.data_to_raw()\n",
    "print(\"Filter done\")\n",
    "#raw=eeg.raw_ica()\n",
    "eeg.create_epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "da1a59b9-880d-4581-b168-ea08bbf1249b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2226/1892461360.py:72: RuntimeWarning: This filename (S002/S002R03.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_2226/1892461360.py:72: RuntimeWarning: This filename (S002/S002R05.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_2226/1892461360.py:72: RuntimeWarning: This filename (S002/S002R07.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_2226/1892461360.py:72: RuntimeWarning: This filename (S002/S002R03.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_2226/1892461360.py:72: RuntimeWarning: This filename (S002/S002R05.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_2226/1892461360.py:72: RuntimeWarning: This filename (S002/S002R07.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw done\n",
      ">>> Apply filter.\n",
      "Filtering raw data in 3 contiguous segments\n",
      "Setting up band-pass filter from 1 - 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 825 samples (3.300 sec)\n",
      "\n",
      "Filter done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# home directory + datasets folder\n",
    "#path = '/content/drive/MyDrive/MNE-eegbci-data/files/eegmmidb/'\n",
    "path = ''\n",
    "base_url = 'https://physionet.org/files/eegmmidb/'\n",
    "# subjects = [1]\n",
    "#runs = [3, 4, 7, 8, 11, 12]\n",
    "runs = [3, 5, 7]\n",
    "subjects = [i for i in range(2,3)]\n",
    "#subjects = [1]\n",
    "# runs = [6,10,14]\n",
    "print(path)\n",
    "eeg = EEG(path, base_url, subjects, runs)\n",
    "raw=eeg.data_to_raw()\n",
    "\n",
    "print(\"Raw done\")\n",
    "# apply filter\n",
    "freq = (1., 40.)\n",
    "raw=eeg.filter(freq=freq)\n",
    "#raw=eeg.data_to_raw()\n",
    "print(\"Filter done\")\n",
    "#raw=eeg.raw_ica()\n",
    "#eeg.create_epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "89d1028c-5817-41c1-85aa-4e7e6f93e602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 events found\n",
      "Event IDs: [1 2 4]\n",
      "Not setting metadata\n",
      "72 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 72 events and 3501 original time points ...\n",
      "6 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2226/1892461360.py:93: RuntimeWarning: No matching events found for 3 (event id 3)\n",
      "  epochs = mne.Epochs(\n"
     ]
    }
   ],
   "source": [
    "X, y = eeg.get_X_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0d584833-27d1-4335-bd68-2a64f13e8838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 2, 641)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X = X[:, :,np.newaxis,:]\n",
    "\n",
    "X=X[:,:,:641]\n",
    "print(X.shape)\n",
    "y=y-1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e3cda817-ff57-451b-ac0d-c9ffb93befbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 2, 641) (46,)\n",
      "(20, 2, 641) (20,)\n",
      "(14, 2, 641) (14,)\n",
      "(6, 2, 641) (6,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "X_test, X_vail, y_test, y_vail = train_test_split(X_test, y_test, test_size=0.3)\n",
    "\n",
    "print(X_test.shape, y_test.shape)\n",
    "print(X_vail.shape, y_vail.shape)\n",
    "\n",
    "#train size (2477, 2, 641) (2477,)\n",
    "#Test size (1062, 2, 641) (1062,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "83eab854-ee11-4b6a-aa3e-f5e67b289318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "def create_dataloader(X, y, batch_size):\n",
    "    X_tensor = torch.tensor(X).float()\n",
    "    y_tensor = torch.tensor(y).long()\n",
    "    dataset_tensor = TensorDataset(X_tensor, y_tensor)\n",
    "    dl = torch.utils.data.DataLoader(dataset_tensor, batch_size=batch_size, shuffle=True)\n",
    "    return dl\n",
    "\n",
    "train_loader = create_dataloader(X_train, y_train, batch_size=batch_size)\n",
    "test_loader = create_dataloader(X_test, y_test, batch_size=batch_size)\n",
    "num_step =math.ceil(len(train_loader.dataset) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "081f0f04-d9c4-47e5-84e4-9a9ed591b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del net\n",
    "net = ConvNet().cuda(0)\n",
    "net.load_state_dict(torch.load('physionet_ourDATA_CNN_2ch_2class'))\n",
    "\n",
    "for name,param in net.named_parameters():\n",
    "    if param.requires_grad and 'layer1' in name:\n",
    "        param.requires_grad = False\n",
    "    if param.requires_grad and 'layer2' in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d4d6e6ea-18ce-4749-a2c4-710049c84904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()),lr=0.00001)\n",
    "#optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=0.00001)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.0000001)\n",
    "#criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8e493d67-e14e-47e2-87d2-331e115df400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:tpq8r26n) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/train_accuracy</td><td>▁▃▅▆█▇██████████████████████████████████</td></tr><tr><td>train/train_loss</td><td>█▇▇▆▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/val_accuracy</td><td>▁▅▁▁▅▅██████████████████████████████████</td></tr><tr><td>val/val_loss</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/train_accuracy</td><td>100.0</td></tr><tr><td>train/train_loss</td><td>0.0056</td></tr><tr><td>val/val_accuracy</td><td>42.85714</td></tr><tr><td>val/val_loss</td><td>1.90575</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">physionet_and_Ourdata_CNN_2ch_2class_findtune</strong>: <a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery/runs/tpq8r26n\" target=\"_blank\">https://wandb.ai/nutapol-1997/Motor-Imagery/runs/tpq8r26n</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220830_131607-tpq8r26n/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:tpq8r26n). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nutapolt/eeg_mi/wandb/run-20220830_131741-3tu0hn8x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery/runs/3tu0hn8x\" target=\"_blank\">physionet_and_Ourdata_CNN_2ch_2class_findtune</a></strong> to <a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "wand = wandb.init(\n",
    "        \n",
    "      # Set the project where this run will be logged\n",
    "      project=\"Motor-Imagery\", \n",
    "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"physionet_and_Ourdata_CNN_2ch_2class_findtune\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      config={\n",
    "      \"learning_rate\": 0.000000001,\n",
    "      \"architecture\": \"CNN\",\n",
    "      \"dataset\": \"Nutapol T.\",\n",
    "      \"epochs\": 1000000,\n",
    "      \"weightname\":\"physionet_and_Ourdata_CNN_2ch_2class_findtune\",\n",
    "      \"num_step_per_epoch\" : num_step, \n",
    "        \n",
    "      }\n",
    "    )\n",
    "\n",
    "config = wand.config\n",
    "print(config.num_step_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b4287-c03c-48e1-9db5-0b3b644b47aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000000, Tr Loss: 0.7505, Tr Acc: 34.7826, Val Loss: 0.6966, Val Acc: 35.7143\n",
      "Epoch 51/1000000, Tr Loss: 0.7127, Tr Acc: 50.0000, Val Loss: 0.6871, Val Acc: 78.5714\n",
      "Epoch 101/1000000, Tr Loss: 0.7001, Tr Acc: 47.8261, Val Loss: 0.6956, Val Acc: 57.1429\n",
      "Epoch 151/1000000, Tr Loss: 0.7372, Tr Acc: 36.9565, Val Loss: 0.6956, Val Acc: 57.1429\n",
      "Epoch 201/1000000, Tr Loss: 0.6956, Tr Acc: 60.8696, Val Loss: 0.6956, Val Acc: 57.1429\n",
      "Epoch 251/1000000, Tr Loss: 0.6851, Tr Acc: 52.1739, Val Loss: 0.6958, Val Acc: 57.1429\n",
      "Epoch 301/1000000, Tr Loss: 0.7063, Tr Acc: 45.6522, Val Loss: 0.6957, Val Acc: 57.1429\n",
      "Epoch 351/1000000, Tr Loss: 0.7215, Tr Acc: 43.4783, Val Loss: 0.6958, Val Acc: 57.1429\n",
      "Epoch 401/1000000, Tr Loss: 0.6916, Tr Acc: 60.8696, Val Loss: 0.6958, Val Acc: 57.1429\n",
      "Epoch 451/1000000, Tr Loss: 0.7100, Tr Acc: 39.1304, Val Loss: 0.6958, Val Acc: 57.1429\n",
      "Epoch 501/1000000, Tr Loss: 0.6863, Tr Acc: 63.0435, Val Loss: 0.6959, Val Acc: 57.1429\n",
      "Epoch 551/1000000, Tr Loss: 0.7231, Tr Acc: 45.6522, Val Loss: 0.6960, Val Acc: 57.1429\n",
      "Epoch 601/1000000, Tr Loss: 0.6916, Tr Acc: 52.1739, Val Loss: 0.6960, Val Acc: 57.1429\n",
      "Epoch 651/1000000, Tr Loss: 0.7184, Tr Acc: 45.6522, Val Loss: 0.6961, Val Acc: 57.1429\n",
      "Epoch 701/1000000, Tr Loss: 0.7558, Tr Acc: 36.9565, Val Loss: 0.6961, Val Acc: 57.1429\n",
      "Epoch 751/1000000, Tr Loss: 0.6865, Tr Acc: 56.5217, Val Loss: 0.6961, Val Acc: 57.1429\n",
      "Epoch 801/1000000, Tr Loss: 0.6803, Tr Acc: 58.6957, Val Loss: 0.6962, Val Acc: 57.1429\n",
      "Epoch 851/1000000, Tr Loss: 0.7056, Tr Acc: 47.8261, Val Loss: 0.6962, Val Acc: 57.1429\n",
      "Epoch 901/1000000, Tr Loss: 0.7011, Tr Acc: 54.3478, Val Loss: 0.6963, Val Acc: 57.1429\n",
      "Epoch 951/1000000, Tr Loss: 0.6776, Tr Acc: 50.0000, Val Loss: 0.6963, Val Acc: 57.1429\n",
      "Epoch 1001/1000000, Tr Loss: 0.6985, Tr Acc: 45.6522, Val Loss: 0.6963, Val Acc: 57.1429\n",
      "Epoch 1051/1000000, Tr Loss: 0.7221, Tr Acc: 32.6087, Val Loss: 0.6964, Val Acc: 57.1429\n",
      "Epoch 1101/1000000, Tr Loss: 0.7002, Tr Acc: 43.4783, Val Loss: 0.6965, Val Acc: 57.1429\n",
      "Epoch 1151/1000000, Tr Loss: 0.7518, Tr Acc: 41.3043, Val Loss: 0.6964, Val Acc: 57.1429\n",
      "Epoch 1201/1000000, Tr Loss: 0.6993, Tr Acc: 54.3478, Val Loss: 0.6965, Val Acc: 57.1429\n",
      "Epoch 1251/1000000, Tr Loss: 0.6893, Tr Acc: 60.8696, Val Loss: 0.6966, Val Acc: 57.1429\n",
      "Epoch 1301/1000000, Tr Loss: 0.6973, Tr Acc: 56.5217, Val Loss: 0.6966, Val Acc: 57.1429\n",
      "Epoch 1351/1000000, Tr Loss: 0.7197, Tr Acc: 50.0000, Val Loss: 0.6966, Val Acc: 57.1429\n",
      "Epoch 1401/1000000, Tr Loss: 0.6890, Tr Acc: 56.5217, Val Loss: 0.6966, Val Acc: 57.1429\n",
      "Epoch 1451/1000000, Tr Loss: 0.7094, Tr Acc: 43.4783, Val Loss: 0.6968, Val Acc: 57.1429\n",
      "Epoch 1501/1000000, Tr Loss: 0.7105, Tr Acc: 50.0000, Val Loss: 0.6968, Val Acc: 57.1429\n",
      "Epoch 1551/1000000, Tr Loss: 0.6898, Tr Acc: 52.1739, Val Loss: 0.6968, Val Acc: 57.1429\n",
      "Epoch 1601/1000000, Tr Loss: 0.7047, Tr Acc: 43.4783, Val Loss: 0.6969, Val Acc: 57.1429\n",
      "Epoch 1651/1000000, Tr Loss: 0.7221, Tr Acc: 30.4348, Val Loss: 0.6969, Val Acc: 57.1429\n",
      "Epoch 1701/1000000, Tr Loss: 0.6887, Tr Acc: 52.1739, Val Loss: 0.6969, Val Acc: 57.1429\n",
      "Epoch 1751/1000000, Tr Loss: 0.6673, Tr Acc: 50.0000, Val Loss: 0.6970, Val Acc: 57.1429\n",
      "Epoch 1801/1000000, Tr Loss: 0.6949, Tr Acc: 54.3478, Val Loss: 0.6970, Val Acc: 57.1429\n",
      "Epoch 1851/1000000, Tr Loss: 0.7121, Tr Acc: 47.8261, Val Loss: 0.6971, Val Acc: 57.1429\n",
      "Epoch 1901/1000000, Tr Loss: 0.7027, Tr Acc: 39.1304, Val Loss: 0.6971, Val Acc: 57.1429\n",
      "Epoch 1951/1000000, Tr Loss: 0.6863, Tr Acc: 54.3478, Val Loss: 0.6972, Val Acc: 57.1429\n",
      "Epoch 2001/1000000, Tr Loss: 0.7331, Tr Acc: 43.4783, Val Loss: 0.6972, Val Acc: 57.1429\n",
      "Epoch 2051/1000000, Tr Loss: 0.7042, Tr Acc: 50.0000, Val Loss: 0.6972, Val Acc: 57.1429\n",
      "Epoch 2101/1000000, Tr Loss: 0.7302, Tr Acc: 43.4783, Val Loss: 0.6973, Val Acc: 57.1429\n",
      "Epoch 2151/1000000, Tr Loss: 0.7030, Tr Acc: 52.1739, Val Loss: 0.6974, Val Acc: 57.1429\n",
      "Epoch 2201/1000000, Tr Loss: 0.6755, Tr Acc: 50.0000, Val Loss: 0.6973, Val Acc: 57.1429\n",
      "Epoch 2251/1000000, Tr Loss: 0.7065, Tr Acc: 47.8261, Val Loss: 0.6974, Val Acc: 57.1429\n",
      "Epoch 2301/1000000, Tr Loss: 0.7087, Tr Acc: 41.3043, Val Loss: 0.6975, Val Acc: 57.1429\n",
      "Epoch 2351/1000000, Tr Loss: 0.6821, Tr Acc: 58.6957, Val Loss: 0.6976, Val Acc: 57.1429\n",
      "Epoch 2401/1000000, Tr Loss: 0.6932, Tr Acc: 50.0000, Val Loss: 0.6976, Val Acc: 57.1429\n",
      "Epoch 2451/1000000, Tr Loss: 0.7093, Tr Acc: 47.8261, Val Loss: 0.6976, Val Acc: 57.1429\n",
      "Epoch 2501/1000000, Tr Loss: 0.7042, Tr Acc: 39.1304, Val Loss: 0.6976, Val Acc: 57.1429\n",
      "Epoch 2551/1000000, Tr Loss: 0.7299, Tr Acc: 47.8261, Val Loss: 0.6977, Val Acc: 57.1429\n",
      "Epoch 2601/1000000, Tr Loss: 0.7213, Tr Acc: 50.0000, Val Loss: 0.6978, Val Acc: 57.1429\n",
      "Epoch 2651/1000000, Tr Loss: 0.7106, Tr Acc: 50.0000, Val Loss: 0.6978, Val Acc: 57.1429\n",
      "Epoch 2701/1000000, Tr Loss: 0.7036, Tr Acc: 50.0000, Val Loss: 0.6978, Val Acc: 57.1429\n",
      "Epoch 2751/1000000, Tr Loss: 0.7261, Tr Acc: 41.3043, Val Loss: 0.6979, Val Acc: 57.1429\n",
      "Epoch 2801/1000000, Tr Loss: 0.6952, Tr Acc: 43.4783, Val Loss: 0.6979, Val Acc: 57.1429\n",
      "Epoch 2851/1000000, Tr Loss: 0.7183, Tr Acc: 43.4783, Val Loss: 0.6980, Val Acc: 57.1429\n",
      "Epoch 2901/1000000, Tr Loss: 0.7039, Tr Acc: 36.9565, Val Loss: 0.6980, Val Acc: 57.1429\n",
      "Epoch 2951/1000000, Tr Loss: 0.7054, Tr Acc: 39.1304, Val Loss: 0.6981, Val Acc: 57.1429\n",
      "Epoch 3001/1000000, Tr Loss: 0.6953, Tr Acc: 41.3043, Val Loss: 0.6981, Val Acc: 57.1429\n",
      "Epoch 3051/1000000, Tr Loss: 0.7099, Tr Acc: 47.8261, Val Loss: 0.6981, Val Acc: 57.1429\n",
      "Epoch 3101/1000000, Tr Loss: 0.7149, Tr Acc: 45.6522, Val Loss: 0.6981, Val Acc: 57.1429\n",
      "Epoch 3151/1000000, Tr Loss: 0.7034, Tr Acc: 50.0000, Val Loss: 0.6982, Val Acc: 57.1429\n",
      "Epoch 3201/1000000, Tr Loss: 0.7101, Tr Acc: 52.1739, Val Loss: 0.6983, Val Acc: 57.1429\n",
      "Epoch 3251/1000000, Tr Loss: 0.6976, Tr Acc: 41.3043, Val Loss: 0.6983, Val Acc: 57.1429\n",
      "Epoch 3301/1000000, Tr Loss: 0.7143, Tr Acc: 47.8261, Val Loss: 0.6983, Val Acc: 57.1429\n",
      "Epoch 3351/1000000, Tr Loss: 0.6909, Tr Acc: 52.1739, Val Loss: 0.6983, Val Acc: 57.1429\n",
      "Epoch 3401/1000000, Tr Loss: 0.6834, Tr Acc: 50.0000, Val Loss: 0.6984, Val Acc: 57.1429\n",
      "Epoch 3451/1000000, Tr Loss: 0.7193, Tr Acc: 39.1304, Val Loss: 0.6984, Val Acc: 57.1429\n",
      "Epoch 3501/1000000, Tr Loss: 0.7041, Tr Acc: 54.3478, Val Loss: 0.6985, Val Acc: 57.1429\n",
      "Epoch 3551/1000000, Tr Loss: 0.7086, Tr Acc: 50.0000, Val Loss: 0.6985, Val Acc: 57.1429\n",
      "Epoch 3601/1000000, Tr Loss: 0.7226, Tr Acc: 45.6522, Val Loss: 0.6986, Val Acc: 57.1429\n",
      "Epoch 3651/1000000, Tr Loss: 0.7120, Tr Acc: 47.8261, Val Loss: 0.6986, Val Acc: 57.1429\n",
      "Epoch 3701/1000000, Tr Loss: 0.6949, Tr Acc: 43.4783, Val Loss: 0.6987, Val Acc: 57.1429\n",
      "Epoch 3751/1000000, Tr Loss: 0.7320, Tr Acc: 36.9565, Val Loss: 0.6987, Val Acc: 57.1429\n",
      "Epoch 3801/1000000, Tr Loss: 0.7150, Tr Acc: 36.9565, Val Loss: 0.6988, Val Acc: 57.1429\n",
      "Epoch 3851/1000000, Tr Loss: 0.7167, Tr Acc: 45.6522, Val Loss: 0.6989, Val Acc: 57.1429\n",
      "Epoch 3901/1000000, Tr Loss: 0.6954, Tr Acc: 60.8696, Val Loss: 0.6988, Val Acc: 57.1429\n",
      "Epoch 3951/1000000, Tr Loss: 0.6875, Tr Acc: 54.3478, Val Loss: 0.6988, Val Acc: 57.1429\n",
      "Epoch 4001/1000000, Tr Loss: 0.7051, Tr Acc: 39.1304, Val Loss: 0.6989, Val Acc: 57.1429\n",
      "Epoch 4051/1000000, Tr Loss: 0.7035, Tr Acc: 41.3043, Val Loss: 0.6989, Val Acc: 57.1429\n",
      "Epoch 4101/1000000, Tr Loss: 0.7049, Tr Acc: 45.6522, Val Loss: 0.6990, Val Acc: 57.1429\n",
      "Epoch 4151/1000000, Tr Loss: 0.6941, Tr Acc: 45.6522, Val Loss: 0.6990, Val Acc: 57.1429\n",
      "Epoch 4201/1000000, Tr Loss: 0.7257, Tr Acc: 45.6522, Val Loss: 0.6991, Val Acc: 57.1429\n",
      "Epoch 4251/1000000, Tr Loss: 0.6740, Tr Acc: 58.6957, Val Loss: 0.6991, Val Acc: 57.1429\n",
      "Epoch 4301/1000000, Tr Loss: 0.7042, Tr Acc: 45.6522, Val Loss: 0.6991, Val Acc: 57.1429\n",
      "Epoch 4351/1000000, Tr Loss: 0.6985, Tr Acc: 45.6522, Val Loss: 0.6992, Val Acc: 57.1429\n",
      "Epoch 4401/1000000, Tr Loss: 0.7151, Tr Acc: 41.3043, Val Loss: 0.6993, Val Acc: 57.1429\n",
      "Epoch 4451/1000000, Tr Loss: 0.6802, Tr Acc: 52.1739, Val Loss: 0.6993, Val Acc: 57.1429\n",
      "Epoch 4501/1000000, Tr Loss: 0.6907, Tr Acc: 52.1739, Val Loss: 0.6993, Val Acc: 57.1429\n",
      "Epoch 4551/1000000, Tr Loss: 0.7165, Tr Acc: 50.0000, Val Loss: 0.6994, Val Acc: 57.1429\n",
      "Epoch 4601/1000000, Tr Loss: 0.6918, Tr Acc: 52.1739, Val Loss: 0.6994, Val Acc: 57.1429\n",
      "Epoch 4651/1000000, Tr Loss: 0.6756, Tr Acc: 63.0435, Val Loss: 0.6994, Val Acc: 57.1429\n",
      "Epoch 4701/1000000, Tr Loss: 0.7185, Tr Acc: 39.1304, Val Loss: 0.6996, Val Acc: 57.1429\n",
      "Epoch 4751/1000000, Tr Loss: 0.7083, Tr Acc: 45.6522, Val Loss: 0.6995, Val Acc: 57.1429\n",
      "Epoch 4801/1000000, Tr Loss: 0.7150, Tr Acc: 41.3043, Val Loss: 0.6996, Val Acc: 57.1429\n",
      "Epoch 4851/1000000, Tr Loss: 0.6916, Tr Acc: 50.0000, Val Loss: 0.6996, Val Acc: 57.1429\n",
      "Epoch 4901/1000000, Tr Loss: 0.7178, Tr Acc: 43.4783, Val Loss: 0.6997, Val Acc: 50.0000\n",
      "Epoch 4951/1000000, Tr Loss: 0.6929, Tr Acc: 52.1739, Val Loss: 0.6997, Val Acc: 50.0000\n",
      "Epoch 5001/1000000, Tr Loss: 0.7299, Tr Acc: 45.6522, Val Loss: 0.6998, Val Acc: 50.0000\n",
      "Epoch 5051/1000000, Tr Loss: 0.7105, Tr Acc: 50.0000, Val Loss: 0.6998, Val Acc: 50.0000\n",
      "Epoch 5101/1000000, Tr Loss: 0.7303, Tr Acc: 54.3478, Val Loss: 0.6999, Val Acc: 50.0000\n",
      "Epoch 5151/1000000, Tr Loss: 0.6799, Tr Acc: 47.8261, Val Loss: 0.6999, Val Acc: 50.0000\n",
      "Epoch 5201/1000000, Tr Loss: 0.6696, Tr Acc: 54.3478, Val Loss: 0.6999, Val Acc: 50.0000\n",
      "Epoch 5251/1000000, Tr Loss: 0.6806, Tr Acc: 54.3478, Val Loss: 0.6999, Val Acc: 50.0000\n",
      "Epoch 5301/1000000, Tr Loss: 0.7524, Tr Acc: 43.4783, Val Loss: 0.7000, Val Acc: 50.0000\n",
      "Epoch 5351/1000000, Tr Loss: 0.6880, Tr Acc: 58.6957, Val Loss: 0.7001, Val Acc: 50.0000\n",
      "Epoch 5401/1000000, Tr Loss: 0.6764, Tr Acc: 52.1739, Val Loss: 0.7001, Val Acc: 50.0000\n",
      "Epoch 5451/1000000, Tr Loss: 0.6883, Tr Acc: 50.0000, Val Loss: 0.7001, Val Acc: 50.0000\n",
      "Epoch 5501/1000000, Tr Loss: 0.6899, Tr Acc: 54.3478, Val Loss: 0.7002, Val Acc: 50.0000\n",
      "Epoch 5551/1000000, Tr Loss: 0.6774, Tr Acc: 56.5217, Val Loss: 0.7002, Val Acc: 50.0000\n",
      "Epoch 5601/1000000, Tr Loss: 0.7033, Tr Acc: 45.6522, Val Loss: 0.7003, Val Acc: 50.0000\n",
      "Epoch 5651/1000000, Tr Loss: 0.7003, Tr Acc: 52.1739, Val Loss: 0.7003, Val Acc: 50.0000\n",
      "Epoch 5701/1000000, Tr Loss: 0.6923, Tr Acc: 47.8261, Val Loss: 0.7003, Val Acc: 50.0000\n",
      "Epoch 5751/1000000, Tr Loss: 0.7060, Tr Acc: 41.3043, Val Loss: 0.7004, Val Acc: 50.0000\n",
      "Epoch 5801/1000000, Tr Loss: 0.6944, Tr Acc: 45.6522, Val Loss: 0.7004, Val Acc: 50.0000\n",
      "Epoch 5851/1000000, Tr Loss: 0.6910, Tr Acc: 60.8696, Val Loss: 0.7004, Val Acc: 50.0000\n",
      "Epoch 5901/1000000, Tr Loss: 0.6860, Tr Acc: 52.1739, Val Loss: 0.7004, Val Acc: 50.0000\n",
      "Epoch 5951/1000000, Tr Loss: 0.6828, Tr Acc: 54.3478, Val Loss: 0.7005, Val Acc: 50.0000\n",
      "Epoch 6001/1000000, Tr Loss: 0.7125, Tr Acc: 45.6522, Val Loss: 0.7005, Val Acc: 50.0000\n",
      "Epoch 6051/1000000, Tr Loss: 0.7226, Tr Acc: 43.4783, Val Loss: 0.7007, Val Acc: 50.0000\n",
      "Epoch 6101/1000000, Tr Loss: 0.7025, Tr Acc: 45.6522, Val Loss: 0.7006, Val Acc: 50.0000\n",
      "Epoch 6151/1000000, Tr Loss: 0.6840, Tr Acc: 60.8696, Val Loss: 0.7006, Val Acc: 50.0000\n",
      "Epoch 6201/1000000, Tr Loss: 0.6908, Tr Acc: 47.8261, Val Loss: 0.7007, Val Acc: 50.0000\n",
      "Epoch 6251/1000000, Tr Loss: 0.7089, Tr Acc: 54.3478, Val Loss: 0.7008, Val Acc: 50.0000\n",
      "Epoch 6301/1000000, Tr Loss: 0.6904, Tr Acc: 56.5217, Val Loss: 0.7007, Val Acc: 50.0000\n",
      "Epoch 6351/1000000, Tr Loss: 0.6989, Tr Acc: 52.1739, Val Loss: 0.7008, Val Acc: 50.0000\n",
      "Epoch 6401/1000000, Tr Loss: 0.6823, Tr Acc: 50.0000, Val Loss: 0.7009, Val Acc: 50.0000\n",
      "Epoch 6451/1000000, Tr Loss: 0.7143, Tr Acc: 45.6522, Val Loss: 0.7009, Val Acc: 57.1429\n",
      "Epoch 6501/1000000, Tr Loss: 0.6662, Tr Acc: 71.7391, Val Loss: 0.7010, Val Acc: 57.1429\n",
      "Epoch 6551/1000000, Tr Loss: 0.6699, Tr Acc: 60.8696, Val Loss: 0.7010, Val Acc: 57.1429\n",
      "Epoch 6601/1000000, Tr Loss: 0.6867, Tr Acc: 52.1739, Val Loss: 0.7011, Val Acc: 57.1429\n",
      "Epoch 6651/1000000, Tr Loss: 0.6958, Tr Acc: 54.3478, Val Loss: 0.7011, Val Acc: 57.1429\n",
      "Epoch 6701/1000000, Tr Loss: 0.6908, Tr Acc: 45.6522, Val Loss: 0.7012, Val Acc: 57.1429\n",
      "Epoch 6751/1000000, Tr Loss: 0.7055, Tr Acc: 54.3478, Val Loss: 0.7011, Val Acc: 57.1429\n",
      "Epoch 6801/1000000, Tr Loss: 0.7032, Tr Acc: 47.8261, Val Loss: 0.7012, Val Acc: 57.1429\n",
      "Epoch 6851/1000000, Tr Loss: 0.6844, Tr Acc: 52.1739, Val Loss: 0.7013, Val Acc: 57.1429\n",
      "Epoch 6901/1000000, Tr Loss: 0.6921, Tr Acc: 56.5217, Val Loss: 0.7013, Val Acc: 57.1429\n",
      "Epoch 6951/1000000, Tr Loss: 0.6874, Tr Acc: 54.3478, Val Loss: 0.7013, Val Acc: 57.1429\n",
      "Epoch 7001/1000000, Tr Loss: 0.6858, Tr Acc: 58.6957, Val Loss: 0.7013, Val Acc: 57.1429\n",
      "Epoch 7051/1000000, Tr Loss: 0.7084, Tr Acc: 52.1739, Val Loss: 0.7014, Val Acc: 57.1429\n",
      "Epoch 7101/1000000, Tr Loss: 0.7022, Tr Acc: 54.3478, Val Loss: 0.7014, Val Acc: 57.1429\n",
      "Epoch 7151/1000000, Tr Loss: 0.6979, Tr Acc: 43.4783, Val Loss: 0.7015, Val Acc: 57.1429\n",
      "Epoch 7201/1000000, Tr Loss: 0.6871, Tr Acc: 58.6957, Val Loss: 0.7016, Val Acc: 57.1429\n",
      "Epoch 7251/1000000, Tr Loss: 0.6885, Tr Acc: 56.5217, Val Loss: 0.7015, Val Acc: 57.1429\n",
      "Epoch 7301/1000000, Tr Loss: 0.7526, Tr Acc: 39.1304, Val Loss: 0.7016, Val Acc: 57.1429\n",
      "Epoch 7351/1000000, Tr Loss: 0.6822, Tr Acc: 58.6957, Val Loss: 0.7016, Val Acc: 57.1429\n",
      "Epoch 7401/1000000, Tr Loss: 0.6960, Tr Acc: 58.6957, Val Loss: 0.7016, Val Acc: 57.1429\n",
      "Epoch 7451/1000000, Tr Loss: 0.7178, Tr Acc: 52.1739, Val Loss: 0.7016, Val Acc: 57.1429\n",
      "Epoch 7501/1000000, Tr Loss: 0.6708, Tr Acc: 69.5652, Val Loss: 0.7018, Val Acc: 57.1429\n",
      "Epoch 7551/1000000, Tr Loss: 0.6906, Tr Acc: 56.5217, Val Loss: 0.7018, Val Acc: 57.1429\n",
      "Epoch 7601/1000000, Tr Loss: 0.7005, Tr Acc: 41.3043, Val Loss: 0.7018, Val Acc: 57.1429\n",
      "Epoch 7651/1000000, Tr Loss: 0.7000, Tr Acc: 52.1739, Val Loss: 0.7019, Val Acc: 57.1429\n",
      "Epoch 7701/1000000, Tr Loss: 0.7173, Tr Acc: 39.1304, Val Loss: 0.7019, Val Acc: 57.1429\n",
      "Epoch 7751/1000000, Tr Loss: 0.7102, Tr Acc: 60.8696, Val Loss: 0.7020, Val Acc: 57.1429\n",
      "Epoch 7801/1000000, Tr Loss: 0.6894, Tr Acc: 56.5217, Val Loss: 0.7020, Val Acc: 57.1429\n",
      "Epoch 7851/1000000, Tr Loss: 0.6951, Tr Acc: 54.3478, Val Loss: 0.7021, Val Acc: 57.1429\n",
      "Epoch 7901/1000000, Tr Loss: 0.6943, Tr Acc: 43.4783, Val Loss: 0.7020, Val Acc: 57.1429\n",
      "Epoch 7951/1000000, Tr Loss: 0.6758, Tr Acc: 58.6957, Val Loss: 0.7021, Val Acc: 57.1429\n",
      "Epoch 8001/1000000, Tr Loss: 0.6690, Tr Acc: 63.0435, Val Loss: 0.7022, Val Acc: 57.1429\n",
      "Epoch 8051/1000000, Tr Loss: 0.7035, Tr Acc: 50.0000, Val Loss: 0.7022, Val Acc: 57.1429\n",
      "Epoch 8101/1000000, Tr Loss: 0.7149, Tr Acc: 43.4783, Val Loss: 0.7022, Val Acc: 57.1429\n",
      "Epoch 8151/1000000, Tr Loss: 0.7116, Tr Acc: 45.6522, Val Loss: 0.7022, Val Acc: 57.1429\n",
      "Epoch 8201/1000000, Tr Loss: 0.7261, Tr Acc: 47.8261, Val Loss: 0.7023, Val Acc: 57.1429\n",
      "Epoch 8251/1000000, Tr Loss: 0.7131, Tr Acc: 50.0000, Val Loss: 0.7024, Val Acc: 57.1429\n",
      "Epoch 8301/1000000, Tr Loss: 0.6650, Tr Acc: 63.0435, Val Loss: 0.7024, Val Acc: 57.1429\n",
      "Epoch 8351/1000000, Tr Loss: 0.6840, Tr Acc: 50.0000, Val Loss: 0.7024, Val Acc: 57.1429\n",
      "Epoch 8401/1000000, Tr Loss: 0.7001, Tr Acc: 45.6522, Val Loss: 0.7026, Val Acc: 57.1429\n",
      "Epoch 8451/1000000, Tr Loss: 0.6894, Tr Acc: 52.1739, Val Loss: 0.7025, Val Acc: 57.1429\n",
      "Epoch 8501/1000000, Tr Loss: 0.6936, Tr Acc: 50.0000, Val Loss: 0.7025, Val Acc: 57.1429\n",
      "Epoch 8551/1000000, Tr Loss: 0.6984, Tr Acc: 54.3478, Val Loss: 0.7026, Val Acc: 57.1429\n",
      "Epoch 8601/1000000, Tr Loss: 0.6990, Tr Acc: 58.6957, Val Loss: 0.7026, Val Acc: 57.1429\n",
      "Epoch 8651/1000000, Tr Loss: 0.7080, Tr Acc: 47.8261, Val Loss: 0.7026, Val Acc: 57.1429\n",
      "Epoch 8701/1000000, Tr Loss: 0.6739, Tr Acc: 54.3478, Val Loss: 0.7026, Val Acc: 57.1429\n",
      "Epoch 8751/1000000, Tr Loss: 0.6832, Tr Acc: 60.8696, Val Loss: 0.7027, Val Acc: 57.1429\n",
      "Epoch 8801/1000000, Tr Loss: 0.7023, Tr Acc: 47.8261, Val Loss: 0.7027, Val Acc: 57.1429\n",
      "Epoch 8851/1000000, Tr Loss: 0.7003, Tr Acc: 43.4783, Val Loss: 0.7028, Val Acc: 57.1429\n",
      "Epoch 8901/1000000, Tr Loss: 0.7156, Tr Acc: 39.1304, Val Loss: 0.7028, Val Acc: 57.1429\n",
      "Epoch 8951/1000000, Tr Loss: 0.6963, Tr Acc: 47.8261, Val Loss: 0.7029, Val Acc: 57.1429\n",
      "Epoch 9001/1000000, Tr Loss: 0.7301, Tr Acc: 45.6522, Val Loss: 0.7029, Val Acc: 57.1429\n",
      "Epoch 9051/1000000, Tr Loss: 0.7132, Tr Acc: 45.6522, Val Loss: 0.7030, Val Acc: 57.1429\n",
      "Epoch 9101/1000000, Tr Loss: 0.7122, Tr Acc: 47.8261, Val Loss: 0.7030, Val Acc: 57.1429\n",
      "Epoch 9151/1000000, Tr Loss: 0.6935, Tr Acc: 52.1739, Val Loss: 0.7030, Val Acc: 57.1429\n",
      "Epoch 9201/1000000, Tr Loss: 0.6862, Tr Acc: 60.8696, Val Loss: 0.7031, Val Acc: 57.1429\n",
      "Epoch 9251/1000000, Tr Loss: 0.6997, Tr Acc: 45.6522, Val Loss: 0.7031, Val Acc: 57.1429\n",
      "Epoch 9301/1000000, Tr Loss: 0.7072, Tr Acc: 50.0000, Val Loss: 0.7032, Val Acc: 57.1429\n",
      "Epoch 9351/1000000, Tr Loss: 0.6767, Tr Acc: 54.3478, Val Loss: 0.7032, Val Acc: 57.1429\n",
      "Epoch 9401/1000000, Tr Loss: 0.6535, Tr Acc: 73.9130, Val Loss: 0.7032, Val Acc: 57.1429\n",
      "Epoch 9451/1000000, Tr Loss: 0.6861, Tr Acc: 54.3478, Val Loss: 0.7033, Val Acc: 57.1429\n",
      "Epoch 9501/1000000, Tr Loss: 0.7092, Tr Acc: 56.5217, Val Loss: 0.7033, Val Acc: 57.1429\n",
      "Epoch 9551/1000000, Tr Loss: 0.6927, Tr Acc: 47.8261, Val Loss: 0.7033, Val Acc: 57.1429\n",
      "Epoch 9601/1000000, Tr Loss: 0.6863, Tr Acc: 60.8696, Val Loss: 0.7034, Val Acc: 57.1429\n",
      "Epoch 9651/1000000, Tr Loss: 0.6915, Tr Acc: 56.5217, Val Loss: 0.7034, Val Acc: 57.1429\n",
      "Epoch 9701/1000000, Tr Loss: 0.6868, Tr Acc: 50.0000, Val Loss: 0.7034, Val Acc: 57.1429\n",
      "Epoch 9751/1000000, Tr Loss: 0.7123, Tr Acc: 54.3478, Val Loss: 0.7035, Val Acc: 57.1429\n",
      "Epoch 9801/1000000, Tr Loss: 0.6955, Tr Acc: 60.8696, Val Loss: 0.7035, Val Acc: 57.1429\n",
      "Epoch 9851/1000000, Tr Loss: 0.7201, Tr Acc: 43.4783, Val Loss: 0.7036, Val Acc: 57.1429\n",
      "Epoch 9901/1000000, Tr Loss: 0.7220, Tr Acc: 36.9565, Val Loss: 0.7036, Val Acc: 57.1429\n",
      "Epoch 9951/1000000, Tr Loss: 0.6786, Tr Acc: 58.6957, Val Loss: 0.7037, Val Acc: 57.1429\n",
      "Epoch 10001/1000000, Tr Loss: 0.6829, Tr Acc: 54.3478, Val Loss: 0.7037, Val Acc: 57.1429\n",
      "Epoch 10051/1000000, Tr Loss: 0.6815, Tr Acc: 56.5217, Val Loss: 0.7037, Val Acc: 57.1429\n",
      "Epoch 10101/1000000, Tr Loss: 0.7158, Tr Acc: 43.4783, Val Loss: 0.7038, Val Acc: 57.1429\n",
      "Epoch 10151/1000000, Tr Loss: 0.6853, Tr Acc: 56.5217, Val Loss: 0.7038, Val Acc: 57.1429\n",
      "Epoch 10201/1000000, Tr Loss: 0.6538, Tr Acc: 60.8696, Val Loss: 0.7039, Val Acc: 57.1429\n",
      "Epoch 10251/1000000, Tr Loss: 0.7075, Tr Acc: 54.3478, Val Loss: 0.7038, Val Acc: 57.1429\n",
      "Epoch 10301/1000000, Tr Loss: 0.7008, Tr Acc: 52.1739, Val Loss: 0.7039, Val Acc: 57.1429\n",
      "Epoch 10351/1000000, Tr Loss: 0.6964, Tr Acc: 56.5217, Val Loss: 0.7040, Val Acc: 57.1429\n",
      "Epoch 10401/1000000, Tr Loss: 0.6806, Tr Acc: 50.0000, Val Loss: 0.7040, Val Acc: 57.1429\n",
      "Epoch 10451/1000000, Tr Loss: 0.6859, Tr Acc: 50.0000, Val Loss: 0.7041, Val Acc: 57.1429\n",
      "Epoch 10501/1000000, Tr Loss: 0.7073, Tr Acc: 60.8696, Val Loss: 0.7040, Val Acc: 57.1429\n",
      "Epoch 10551/1000000, Tr Loss: 0.6688, Tr Acc: 52.1739, Val Loss: 0.7041, Val Acc: 57.1429\n",
      "Epoch 10601/1000000, Tr Loss: 0.7131, Tr Acc: 50.0000, Val Loss: 0.7041, Val Acc: 57.1429\n",
      "Epoch 10651/1000000, Tr Loss: 0.7109, Tr Acc: 41.3043, Val Loss: 0.7042, Val Acc: 57.1429\n",
      "Epoch 10701/1000000, Tr Loss: 0.6722, Tr Acc: 67.3913, Val Loss: 0.7042, Val Acc: 57.1429\n",
      "Epoch 10751/1000000, Tr Loss: 0.7152, Tr Acc: 39.1304, Val Loss: 0.7042, Val Acc: 57.1429\n",
      "Epoch 10801/1000000, Tr Loss: 0.6925, Tr Acc: 50.0000, Val Loss: 0.7043, Val Acc: 57.1429\n",
      "Epoch 10851/1000000, Tr Loss: 0.7061, Tr Acc: 50.0000, Val Loss: 0.7043, Val Acc: 57.1429\n",
      "Epoch 10901/1000000, Tr Loss: 0.6920, Tr Acc: 58.6957, Val Loss: 0.7044, Val Acc: 57.1429\n",
      "Epoch 10951/1000000, Tr Loss: 0.6916, Tr Acc: 58.6957, Val Loss: 0.7044, Val Acc: 57.1429\n",
      "Epoch 11001/1000000, Tr Loss: 0.6696, Tr Acc: 56.5217, Val Loss: 0.7043, Val Acc: 57.1429\n",
      "Epoch 11051/1000000, Tr Loss: 0.6986, Tr Acc: 52.1739, Val Loss: 0.7045, Val Acc: 57.1429\n",
      "Epoch 11101/1000000, Tr Loss: 0.7053, Tr Acc: 47.8261, Val Loss: 0.7045, Val Acc: 50.0000\n",
      "Epoch 11151/1000000, Tr Loss: 0.6464, Tr Acc: 69.5652, Val Loss: 0.7045, Val Acc: 57.1429\n",
      "Epoch 11201/1000000, Tr Loss: 0.6759, Tr Acc: 56.5217, Val Loss: 0.7046, Val Acc: 50.0000\n",
      "Epoch 11251/1000000, Tr Loss: 0.6850, Tr Acc: 47.8261, Val Loss: 0.7047, Val Acc: 50.0000\n",
      "Epoch 11301/1000000, Tr Loss: 0.6838, Tr Acc: 52.1739, Val Loss: 0.7046, Val Acc: 50.0000\n",
      "Epoch 11351/1000000, Tr Loss: 0.6981, Tr Acc: 41.3043, Val Loss: 0.7047, Val Acc: 50.0000\n",
      "Epoch 11401/1000000, Tr Loss: 0.6852, Tr Acc: 56.5217, Val Loss: 0.7047, Val Acc: 50.0000\n",
      "Epoch 11451/1000000, Tr Loss: 0.7020, Tr Acc: 54.3478, Val Loss: 0.7048, Val Acc: 50.0000\n",
      "Epoch 11501/1000000, Tr Loss: 0.6933, Tr Acc: 58.6957, Val Loss: 0.7048, Val Acc: 50.0000\n",
      "Epoch 11551/1000000, Tr Loss: 0.6936, Tr Acc: 56.5217, Val Loss: 0.7049, Val Acc: 50.0000\n",
      "Epoch 11601/1000000, Tr Loss: 0.7031, Tr Acc: 34.7826, Val Loss: 0.7049, Val Acc: 50.0000\n",
      "Epoch 11651/1000000, Tr Loss: 0.6960, Tr Acc: 36.9565, Val Loss: 0.7049, Val Acc: 50.0000\n",
      "Epoch 11701/1000000, Tr Loss: 0.6988, Tr Acc: 45.6522, Val Loss: 0.7050, Val Acc: 50.0000\n",
      "Epoch 11751/1000000, Tr Loss: 0.7077, Tr Acc: 47.8261, Val Loss: 0.7050, Val Acc: 50.0000\n",
      "Epoch 11801/1000000, Tr Loss: 0.6654, Tr Acc: 56.5217, Val Loss: 0.7050, Val Acc: 50.0000\n"
     ]
    }
   ],
   "source": [
    "from common import train\n",
    "net = ConvNet().cuda(0)\n",
    "optimizer = optim.Adam(net.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_loss,valid_loss,train_accuracy,valid_accuracy =train(\n",
    "    model = net,\n",
    "    gpu_num = 0,\n",
    "    train_loader = train_loader,\n",
    "    test_loader = test_loader,\n",
    "    optimizer = optimizer  ,\n",
    "    criterion = criterion ,\n",
    "    wand = wand\n",
    "         )\n",
    "\n",
    "\n",
    "wandb.alert(\n",
    "            title='Finish',\n",
    "            text=f'Finishing training',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd6954d-b00e-4e87-a1b3-d8f2b3ff9252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

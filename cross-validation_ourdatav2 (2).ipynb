{
 "cells": [
  {
   "cell_type": "raw",
   "id": "edc0735b-62f8-4efa-b828-7f7a11868b90",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/pytorch-ignite/pytorch-ignite.ai/blob/gh-pages/how-to-guides/07-cross-validation.ipynb#scrollTo=iyrEEGHD9Ie5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d83b651-50a4-492e-bf4d-c28546b01977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#comment this if you are not using puffer?\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19e59fe9-7be7-48c3-8612-02cbd5fc9742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import os\n",
    "import sys\n",
    "from mne.datasets import eegbci\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from mne.datasets import eegbci\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader,SubsetRandomSampler\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from common import setup_dataflow\n",
    "import wandb\n",
    "from mne.datasets import sample\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98d94ed6-61da-4ac6-be64-0c19e27023d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8543d9f-1cda-4609-97d8-a02ad0208221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbf7aecb-9c1f-4ad6-a018-1619a1a218de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG:\n",
    "    def __init__(self, path, base_url, subjects, runs):\n",
    "        self.subpath = ''\n",
    "        self.path = path\n",
    "        print(path)\n",
    "        self.base_url = base_url\n",
    "        self.subjects = subjects\n",
    "        self.runs = runs\n",
    "        \n",
    "        # download data if does not exist in path.\n",
    "        # self.load_data()\n",
    "        self.data_to_raw()\n",
    "    \n",
    "    def load_data(self):\n",
    "        print(f\">>> Start download from: {self.base_url}.\")\n",
    "        print(f\"Downloading files to: {self.path}.\")\n",
    "        for subject in self.subjects:\n",
    "            eegbci.load_data(subject,self.runs,path=self.path,base_url=self.base_url)\n",
    "        print(\"Done.\")\n",
    "    \n",
    "    \n",
    "        \n",
    "        print(\"Done.\")\n",
    "        return self.raw\n",
    "    def filter(self, freq):\n",
    "        raw = self.raw\n",
    "        low, high = freq\n",
    "        print(f\">>> Apply filter.\")\n",
    "        self.raw.filter(low, high, fir_design='firwin', verbose=20)\n",
    "        return  raw\n",
    "    def raw_ica(self):\n",
    "        raw = self.raw\n",
    "        ica = mne.preprocessing.ICA(n_components=1, max_iter=100)\n",
    "        ica.fit(raw)\n",
    "        ica.exclude = [1, 2]  # details on how we picked these are omitted here\n",
    "        ica.plot_properties(raw, picks=ica.exclude)\n",
    "        ica.apply(raw)\n",
    "        print('ICA DONE ????')\n",
    "        return  raw\n",
    "        \n",
    "    def get_events(self):\n",
    "        event_id = dict(T1=0, T2=1) # the events we want to extract\n",
    "        events, event_id = mne.events_from_annotations(self.raw, event_id=event_id)\n",
    "        return events, event_id\n",
    "    \n",
    "    def get_epochs(self, events, event_id):\n",
    "        picks = mne.pick_types(self.raw.info, eeg=True, exclude='bads')\n",
    "        tmin = 0\n",
    "        tmax = 4\n",
    "        epochs = mne.Epochs(self.raw, events, event_id, tmin, tmax, proj=True, \n",
    "                            picks=picks, baseline=None, preload=True)\n",
    "        return epochs\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    def data_to_raw(self):\n",
    "        fullpath = os.path.join(self.path, *self.subpath.split(sep='/'))\n",
    "        #print(f\">>> Extract all subjects from: {fullpath}.\")\n",
    "        extension = \"fif\"\n",
    "        raws = []\n",
    "        count = 1\n",
    "        for i, subject in enumerate(self.subjects):\n",
    "            sname = f\"S{str(subject).zfill(3)}\".upper()\n",
    "            \n",
    "            for j, run in enumerate(self.runs):\n",
    "                rname = f\"{sname}R{str(run).zfill(2)}\".upper()\n",
    "                path_file = os.path.join(fullpath, sname, f'{rname}.{extension}')\n",
    "                #print(path_file)\n",
    "                #print(f\"Loading file #{count}/{len(self.subjects)*len(self.runs)}: {f'{rname}.{extension}'}\")\n",
    "                raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
    "                raws.append(raw)\n",
    "                count += 1\n",
    "\n",
    "        raw = mne.io.concatenate_raws(raws)\n",
    "        eegbci.standardize(raw)\n",
    "        montage = mne.channels.make_standard_montage('standard_1005')\n",
    "        raw.set_montage(montage)\n",
    "        self.raw = raw\n",
    "    \n",
    "    def create_epochs(self):\n",
    "        print(\">>> Create Epochs.\")\n",
    "        \n",
    "        events, event_id = self.get_events()\n",
    "        self.epochs = self.get_epochs(events, event_id)\n",
    "        print(\"Done.\")\n",
    "        return events , event_id\n",
    "# getepoch(raw,4, 10,reject_bad=False,on_missing='warn')    \n",
    "    def get_X_y(self):\n",
    "        events = mne.find_events(raw)\n",
    "        \n",
    "        epochs = mne.Epochs(\n",
    "        raw,\n",
    "        events,\n",
    "        event_id=[1,2,3],\n",
    "        tmin=0,\n",
    "        tmax=5,\n",
    "        picks=\"data\",\n",
    "        on_missing='warn',\n",
    "        baseline=None,\n",
    "            preload=True\n",
    "    )\n",
    "        #epochs=epochs.resample(160)\n",
    "            #events , event_id=self.create_epochs()\n",
    "        self.X = epochs.get_data()\n",
    "        self.y = epochs.events[:, -1]\n",
    "        return self.X, self.y \n",
    "\n",
    "        \n",
    "def getepoch(raws,trial_duration, calibration_duration,reject_bad=False,on_missing='warn'):\n",
    "    #reject_criteria = dict(eeg=100e-6)  # 100 µV\n",
    "    #flat_criteria = dict(eeg=1e-6)  # 1 µV\n",
    "    epochs_list = []\n",
    "    raws = [raws]\n",
    "    print(len(raws))\n",
    "    for raw in raws:\n",
    "        print(raw)\n",
    "        events = mne.find_events(raw)\n",
    "        epochs = mne.Epochs(\n",
    "            raw,\n",
    "            events,\n",
    "            event_id=[1,2,3],\n",
    "            tmin=-calibration_duration,\n",
    "            tmax=trial_duration,\n",
    "            picks=\"data\",\n",
    "            on_missing=on_missing,\n",
    "            baseline=None,\n",
    "            preload=True\n",
    "        )\n",
    "        \n",
    "        epochs_list.append(epochs)\n",
    "    epochs = mne.concatenate_epochs(epochs_list)\n",
    "    labels = epochs.events[:,-1]\n",
    "    \n",
    "    print(f'Found {len(labels)} epochs')\n",
    "    print(epochs)\n",
    "    print(labels)\n",
    "\n",
    "    return epochs.get_data(),epochs,labels\n",
    "\n",
    "        \n",
    "        \n",
    "def do_plot(train_loss, valid_loss,ty):\n",
    "    if ty == \"loss\":\n",
    "        plt.figure(figsize=(10,10))\n",
    "        \n",
    "        plt.plot(train_loss, label='train_loss')\n",
    "        plt.plot(valid_loss, label='valid_loss')\n",
    "        plt.title('loss {}'.format(iter))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    if ty == \"acc\":\n",
    "        plt.figure(figsize=(10,10))\n",
    "        \n",
    "        plot_ty=torch.tensor(train_loss, device = 'cpu')\n",
    "        plat_va=torch.tensor(valid_loss, device = 'cpu')\n",
    "        plt.plot(plot_ty, label='train_acc')\n",
    "        plt.plot(plat_va, label='valid_acc')\n",
    "        plt.title('acc {}'.format(iter))\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0dd9b5d-1ed9-4cf2-a34f-6626c35159db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14752/492028251.py:71: RuntimeWarning: This filename (S003/S003R03.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_14752/492028251.py:71: RuntimeWarning: This filename (S003/S003R05.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_14752/492028251.py:71: RuntimeWarning: This filename (S003/S003R07.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_14752/492028251.py:71: RuntimeWarning: This filename (S003/S003R03.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_14752/492028251.py:71: RuntimeWarning: This filename (S003/S003R05.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_14752/492028251.py:71: RuntimeWarning: This filename (S003/S003R07.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw done\n",
      ">>> Apply filter.\n",
      "Filtering raw data in 3 contiguous segments\n",
      "Setting up band-pass filter from 1 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-6 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 825 samples (3.300 sec)\n",
      "\n",
      "Filter done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# home directory + datasets folder\n",
    "#path = '/content/drive/MyDrive/MNE-eegbci-data/files/eegmmidb/'\n",
    "path = ''\n",
    "base_url = 'https://physionet.org/files/eegmmidb/'\n",
    "# subjects = [1]\n",
    "#runs = [3, 4, 7, 8, 11, 12]\n",
    "runs = [3, 5, 7]\n",
    "subjects = [i for i in range(3,4)]\n",
    "#subjects = [1]\n",
    "# runs = [6,10,14]\n",
    "print(path)\n",
    "eeg = EEG(path, base_url, subjects, runs)\n",
    "raw=eeg.data_to_raw()\n",
    "\n",
    "print(\"Raw done\")\n",
    "# apply filter\n",
    "freq = (1., 45.)\n",
    "raw=eeg.filter(freq=freq)\n",
    "#raw=eeg.data_to_raw()\n",
    "print(\"Filter done\")\n",
    "#raw=eeg.raw_ica()\n",
    "#eeg.create_epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a751921c-5ff5-4a08-ad1f-d503e6062002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 events found\n",
      "Event IDs: [1 2 4]\n",
      "Not setting metadata\n",
      "72 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 72 events and 1251 original time points ...\n",
      "0 bad epochs dropped\n",
      "(72, 2, 801) (72,)\n",
      "(72, 2, 801) (72,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14752/492028251.py:92: RuntimeWarning: No matching events found for 3 (event id 3)\n",
      "  epochs = mne.Epochs(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(72, 2, 801)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = eeg.get_X_y()\n",
    "print(X.shape, y.shape)\n",
    "#X = X[:, :,np.newaxis,:]\n",
    "print(X.shape, y.shape)\n",
    "#X = X[:,:,:]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f59a7188-2295-4501-8f8f-f00df822f73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=y-1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87f728da-0a9a-4875-927d-b012cba21e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size (64, 2, 801) (64,)\n",
      "Test size (8, 2, 801) (8,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "print('Train size',X_train.shape, y_train.shape)\n",
    "print('Test size',X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "595d124d-a3ef-4806-8643-a3974599ea79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b040bf58-7c18-43f5-897d-bc2651a97a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X_test).float()\n",
    "y_tensor = torch.tensor(y_test).long()\n",
    "\n",
    "dataset_vail = TensorDataset(X_tensor, y_tensor)\n",
    "vail_loader = torch.utils.data.DataLoader(dataset_vail, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b45b6f4-78e1-4c82-a544-bb2abed895cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset\n",
    "X_tensor_train = torch.tensor(X_train).float()\n",
    "y_tensor_train = torch.tensor(y_train).long()\n",
    "\n",
    "\n",
    "\n",
    "dataset = TensorDataset(X_tensor_train, y_tensor_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d940d-5dac-4cbd-a00b-d0d691f19c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc4f40c8-dac3-4b28-b20b-dad2fe492377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        #using sequential helps bind multiple operations together\n",
    "        self.layer1 = nn.Sequential(\n",
    "            #padding = (kernel_size - 1) / 2 = 2\n",
    "            nn.Conv1d(2, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc = nn.Linear(160128, 2)\n",
    "        self.drop_out = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.drop_out(out)\n",
    "        out = out.reshape(out.size(0), -1)   #can also use .view()\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519df3e-478b-4497-9971-9912335cfec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fcd2047-d53e-4480-ad09-c9d7ec30b205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nutapolt/eeg_mi/wandb/run-20220907_085943-b95peneh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery/runs/b95peneh\" target=\"_blank\">ourdata_CNN_2ch_2class_cross_ourdata</a></strong> to <a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "wand = wandb.init(\n",
    "        \n",
    "      # Set the project where this run will be logged\n",
    "      project=\"Motor-Imagery\", \n",
    "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"ourdata_CNN_2ch_2class_cross_ourdata\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      config={\n",
    "      \"learning_rate\": 0.00001,\n",
    "      \"architecture\": \"CNN\",\n",
    "      \"dataset\": \"Nutapol T.\",\n",
    "      \"epochs\": 1000,\n",
    "      \"weightname\":\"ourdata_CNN_2ch_2class_cross_update_kernal\",\n",
    "       \n",
    "        \n",
    "      }\n",
    "    )\n",
    "\n",
    "config = wand.config\n",
    "#print(config.num_step_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e751a47-f30f-4187-a43d-698e941345a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44735353-f086-40c5-8035-1197643e4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 3\n",
    "#splits = KFold(n_splits=num_folds,shuffle=True,random_state=42)\n",
    "splits = KFold(n_splits=num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da7aa5d0-fd03-4e29-9f23-13664c413e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNet().cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd638e-4114-456f-b194-cd37b7447463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09b5210f-8b9c-4970-8c45-1159cafdea08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "torch.Size([42, 2, 1251])\n",
      "torch.Size([42])\n",
      "Fold 2\n",
      "torch.Size([43, 2, 1251])\n",
      "torch.Size([43])\n",
      "Fold 3\n",
      "torch.Size([43, 2, 1251])\n",
      "torch.Size([43])\n"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "        \n",
    "\n",
    "        print('Fold {}'.format(fold_idx + 1))\n",
    "        #print('Fold {}'.format(fold_idx + 1))\n",
    "        print(X_tensor_train[train_idx].shape)\n",
    "        print(y_tensor_train[train_idx].shape)\n",
    "        #train_loader, val_loader = setup_dataflow(dataset, train_idx, val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7ef542f-6886-49be-b91b-32a7ad651b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Num 0\n",
      "Fold 1\n",
      "Epoch 1/1000, Tr Loss: 0.7238, Tr Acc: 50.0000, Val Loss: 0.6937, Val Acc: 40.9091\n",
      "Val Loss: 0.6936781406402588, Val Acc: 87.5\n",
      "Epoch 101/1000, Tr Loss: 0.1139, Tr Acc: 97.6190, Val Loss: 3.3459, Val Acc: 40.9091\n",
      "Val Loss: 3.3458588123321533, Val Acc: 87.5\n",
      "Epoch 201/1000, Tr Loss: 0.0413, Tr Acc: 100.0000, Val Loss: 1.7529, Val Acc: 45.4545\n",
      "Val Loss: 1.752923607826233, Val Acc: 87.5\n",
      "Epoch 301/1000, Tr Loss: 0.0194, Tr Acc: 100.0000, Val Loss: 1.8306, Val Acc: 45.4545\n",
      "Val Loss: 1.830591082572937, Val Acc: 87.5\n",
      "Epoch 401/1000, Tr Loss: 0.0114, Tr Acc: 100.0000, Val Loss: 1.9270, Val Acc: 54.5455\n",
      "Val Loss: 1.9270472526550293, Val Acc: 75.0\n",
      "Epoch 501/1000, Tr Loss: 0.0074, Tr Acc: 100.0000, Val Loss: 2.0163, Val Acc: 54.5455\n",
      "Val Loss: 2.016313314437866, Val Acc: 75.0\n",
      "Epoch 601/1000, Tr Loss: 0.0058, Tr Acc: 100.0000, Val Loss: 2.1112, Val Acc: 54.5455\n",
      "Val Loss: 2.111225128173828, Val Acc: 75.0\n",
      "Epoch 701/1000, Tr Loss: 0.0038, Tr Acc: 100.0000, Val Loss: 2.1169, Val Acc: 59.0909\n",
      "Val Loss: 2.1168980598449707, Val Acc: 62.5\n",
      "Epoch 801/1000, Tr Loss: 0.0026, Tr Acc: 100.0000, Val Loss: 2.0517, Val Acc: 59.0909\n",
      "Val Loss: 2.051715612411499, Val Acc: 62.5\n",
      "Epoch 901/1000, Tr Loss: 0.0024, Tr Acc: 100.0000, Val Loss: 2.2241, Val Acc: 59.0909\n",
      "Val Loss: 2.2240612506866455, Val Acc: 62.5\n",
      "Fold 2\n",
      "Epoch 1/1000, Tr Loss: 0.6306, Tr Acc: 81.3953, Val Loss: 0.2429, Val Acc: 80.9524\n",
      "Val Loss: 0.24294662475585938, Val Acc: 87.5\n",
      "Epoch 101/1000, Tr Loss: 0.0145, Tr Acc: 100.0000, Val Loss: 0.0201, Val Acc: 100.0000\n",
      "Val Loss: 0.020066188648343086, Val Acc: 75.0\n",
      "Epoch 201/1000, Tr Loss: 0.0068, Tr Acc: 100.0000, Val Loss: 0.0118, Val Acc: 100.0000\n",
      "Val Loss: 0.01176536362618208, Val Acc: 75.0\n",
      "Epoch 301/1000, Tr Loss: 0.0039, Tr Acc: 100.0000, Val Loss: 0.0074, Val Acc: 100.0000\n",
      "Val Loss: 0.00738564133644104, Val Acc: 50.0\n",
      "Epoch 401/1000, Tr Loss: 0.0028, Tr Acc: 100.0000, Val Loss: 0.0082, Val Acc: 100.0000\n",
      "Val Loss: 0.008205876685678959, Val Acc: 50.0\n",
      "Epoch 501/1000, Tr Loss: 0.0020, Tr Acc: 100.0000, Val Loss: 0.0113, Val Acc: 100.0000\n",
      "Val Loss: 0.01129815261811018, Val Acc: 50.0\n",
      "Epoch 601/1000, Tr Loss: 0.0013, Tr Acc: 100.0000, Val Loss: 0.0130, Val Acc: 100.0000\n",
      "Val Loss: 0.013030249625444412, Val Acc: 62.5\n",
      "Epoch 701/1000, Tr Loss: 0.0013, Tr Acc: 100.0000, Val Loss: 0.0171, Val Acc: 100.0000\n",
      "Val Loss: 0.017059287056326866, Val Acc: 62.5\n",
      "Epoch 801/1000, Tr Loss: 0.0010, Tr Acc: 100.0000, Val Loss: 0.0146, Val Acc: 100.0000\n",
      "Val Loss: 0.01463689561933279, Val Acc: 62.5\n",
      "Epoch 901/1000, Tr Loss: 0.0010, Tr Acc: 100.0000, Val Loss: 0.0218, Val Acc: 100.0000\n",
      "Val Loss: 0.021756520494818687, Val Acc: 62.5\n",
      "Fold 3\n",
      "Epoch 1/1000, Tr Loss: 0.0529, Tr Acc: 97.6744, Val Loss: 0.0003, Val Acc: 100.0000\n",
      "Val Loss: 0.0003073325497098267, Val Acc: 37.5\n",
      "Epoch 101/1000, Tr Loss: 0.0011, Tr Acc: 100.0000, Val Loss: 0.0010, Val Acc: 100.0000\n",
      "Val Loss: 0.0009826298337429762, Val Acc: 50.0\n",
      "Epoch 201/1000, Tr Loss: 0.0006, Tr Acc: 100.0000, Val Loss: 0.0007, Val Acc: 100.0000\n",
      "Val Loss: 0.000684418948367238, Val Acc: 50.0\n",
      "Epoch 301/1000, Tr Loss: 0.0007, Tr Acc: 100.0000, Val Loss: 0.0006, Val Acc: 100.0000\n",
      "Val Loss: 0.0005715700099244714, Val Acc: 50.0\n",
      "Epoch 401/1000, Tr Loss: 0.0005, Tr Acc: 100.0000, Val Loss: 0.0004, Val Acc: 100.0000\n",
      "Val Loss: 0.00044371740659698844, Val Acc: 50.0\n",
      "Epoch 501/1000, Tr Loss: 0.0003, Tr Acc: 100.0000, Val Loss: 0.0005, Val Acc: 100.0000\n",
      "Val Loss: 0.0005059068207629025, Val Acc: 50.0\n",
      "Epoch 601/1000, Tr Loss: 0.0003, Tr Acc: 100.0000, Val Loss: 0.0005, Val Acc: 100.0000\n",
      "Val Loss: 0.0005130763165652752, Val Acc: 50.0\n",
      "Epoch 701/1000, Tr Loss: 0.0003, Tr Acc: 100.0000, Val Loss: 0.0005, Val Acc: 100.0000\n",
      "Val Loss: 0.0004938272759318352, Val Acc: 50.0\n",
      "Epoch 801/1000, Tr Loss: 0.0002, Tr Acc: 100.0000, Val Loss: 0.0005, Val Acc: 100.0000\n",
      "Val Loss: 0.0005068377358838916, Val Acc: 50.0\n",
      "Epoch 901/1000, Tr Loss: 0.0001, Tr Acc: 100.0000, Val Loss: 0.0005, Val Acc: 100.0000\n",
      "Val Loss: 0.000517957960255444, Val Acc: 50.0\n"
     ]
    }
   ],
   "source": [
    "results_per_fold = []\n",
    "batch_size = 32\n",
    "from common import train\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch_B in range(1):\n",
    "    print(\"Epoch Num {}\".format(epoch_B))\n",
    "    for fold_idx, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "\n",
    "        print('Fold {}'.format(fold_idx + 1))\n",
    "\n",
    "        train_loader, test_loader = setup_dataflow(X_tensor_train,y_tensor_train, train_idx, val_idx)\n",
    "        num_step =math.ceil(len(train_loader.dataset) / batch_size)\n",
    "        config.num_step_per_epoch=num_step\n",
    "        train_loss,valid_loss,train_accuracy,valid_accuracy =train(\n",
    "                                                                model = net,\n",
    "                                                                gpu_num = 0,\n",
    "                                                                train_loader = train_loader,\n",
    "                                                                test_loader = test_loader,\n",
    "                                                                vail_loader = vail_loader,\n",
    "                                                                optimizer = optimizer  ,\n",
    "                                                                criterion = criterion ,\n",
    "                                                                wand = wand\n",
    "                                                                     )\n",
    "        results_per_fold.append([train_accuracy, valid_accuracy])\n",
    "wandb.alert(\n",
    "            title='Finish',\n",
    "            text=f'Finishing training',\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41f393-f99e-4774-bcd9-f42279f4b195",
   "metadata": {},
   "source": [
    "### GAMENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b75311fb-331b-4a9c-9f96-c6f222f5e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gamenet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(gamenet,self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Sequential(\n",
    "            #in_channel = 16\n",
    "            #out_channel or Filter size = 100\n",
    "            #kernel size = (1,25)\n",
    "            #stride = 1\n",
    "            #padding = Same\n",
    "            #Relu\n",
    "            nn.Conv2d(1,100,kernel_size=(1,2),stride=1,padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(100)\n",
    "        )\n",
    "        self.l2 = nn.Sequential(\n",
    "            #in_channel = 100\n",
    "            #out_channel or Filter size = 100\n",
    "            #kernel size = (16,1)\n",
    "            #stride = 1\n",
    "            #padding = Valid\n",
    "            #Relu\n",
    "            nn.Conv2d(100,100,kernel_size=(1,16),stride=1,padding='valid'),\n",
    "            #nn.Conv2d(100,100,kernel_size=(64,1),stride=1,padding='valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(100)\n",
    "        )\n",
    "        self.l3 = nn.Sequential(\n",
    "            #in_channel = 100\n",
    "            #out_channel = 50\n",
    "            #kernel size = (1,30)\n",
    "            #stride = 1\n",
    "            #padding = Same\n",
    "            #Relu\n",
    "            nn.Conv2d(100,50,kernel_size=(1,30),stride=1,padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(50)\n",
    "        )\n",
    "        self.maxpooling1 = nn.MaxPool2d(kernel_size=(1,7),stride=5)\n",
    "        self.l4 = nn.Sequential(\n",
    "            #in_channel = 50\n",
    "            #out_channel = 50\n",
    "            #kernel size = (1,30)\n",
    "            #stride = 1\n",
    "            #padding = Same\n",
    "            #Relu\n",
    "            nn.Conv2d(50,50,kernel_size=(1,30),stride=1,padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(50)\n",
    "        )\n",
    "        self.maxpooling2 = nn.MaxPool2d(kernel_size=(1,3),stride=2)\n",
    "        \n",
    "\n",
    "        self.flatten = nn.Flatten()   #Sequential(nn.Flatten(),nn.BatchNorm1d(3050),nn.Dropout(0.15))\n",
    "        #mat1 and mat2 shapes cannot be multiplied (32x6100 and 3050x1024)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(6100,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc5 = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc6 = nn.Sequential(\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.softmax = nn.Sequential(\n",
    "            nn.Linear(32,2),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        \n",
    "        out = self.l1(x)\n",
    "        out = self.l2(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.maxpooling1(out)\n",
    "        out = self.l4(out)\n",
    "        out = self.maxpooling2(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7dba703b-48bc-49b3-aea3-f5474218b2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 events found\n",
      "Event IDs: [1 2 4]\n",
      "Not setting metadata\n",
      "72 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 72 events and 1251 original time points ...\n",
      "0 bad epochs dropped\n",
      "(72, 2, 1251) (72,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14181/1373606278.py:92: RuntimeWarning: No matching events found for 3 (event id 3)\n",
      "  epochs = mne.Epochs(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = eeg.get_X_y()\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "#X = X[:,:,:]\n",
    "X.shape\n",
    "y=y-1\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "692bb76e-272a-4bfe-80c9-2bb2be7041b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 1, 2, 1251)\n"
     ]
    }
   ],
   "source": [
    "X = X[:,np.newaxis,:,:]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "562792ae-75b4-477d-9862-6ceea1e470df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size (50, 1, 2, 1251) (50,)\n",
      "Test size (22, 1, 2, 1251) (22,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print('train size',X_train.shape, y_train.shape)\n",
    "print('Test size',X_test.shape, y_test.shape)\n",
    "\n",
    "X_tensor = torch.tensor(X_test).float()\n",
    "y_tensor = torch.tensor(y_test).long()\n",
    "\n",
    "dataset_vail = TensorDataset(X_tensor, y_tensor)\n",
    "vail_loader = torch.utils.data.DataLoader(dataset_vail, batch_size=32, shuffle=True)\n",
    "\n",
    "X_tensor_train = torch.tensor(X_train).float()\n",
    "y_tensor_train = torch.tensor(y_train).long()\n",
    "\n",
    "\n",
    "\n",
    "dataset = TensorDataset(X_tensor_train, y_tensor_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd57ab05-f027-4df3-8bf0-f18b826dc319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:fs9ba7de) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/train_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/train_loss</td><td>0.74796</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">physionet_gamenet_2ch_2class_cross_vail</strong>: <a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery/runs/fs9ba7de\" target=\"_blank\">https://wandb.ai/nutapol-1997/Motor-Imagery/runs/fs9ba7de</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220907_090731-fs9ba7de/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:fs9ba7de). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nutapolt/eeg_mi/wandb/run-20220907_091122-3g7xgrv0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery/runs/3g7xgrv0\" target=\"_blank\">physionet_gamenet_2ch_2class_cross_vail</a></strong> to <a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "wand = wandb.init(\n",
    "        \n",
    "      # Set the project where this run will be logged\n",
    "      project=\"Motor-Imagery\", \n",
    "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"physionet_gamenet_2ch_2class_cross_vail\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      config={\n",
    "      \"learning_rate\": 0.00001,\n",
    "      \"architecture\": \"gamenet\",\n",
    "      \"dataset\": \"Nutapol T.\",\n",
    "      \"epochs\": 10,\n",
    "      \"weightname\":\"physionet_gamenet_2ch_2class_cross_vail\",\n",
    "       \n",
    "        \n",
    "      }\n",
    "    )\n",
    "\n",
    "config = wand.config\n",
    "#print(config.num_step_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f00c68f4-c3c7-49a3-b150-a929cf68206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net = gamenet().cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1fbfed80-d631-413f-bbe3-a690e40b4de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Num 0\n",
      "Fold 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 1024])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         num_step \u001b[38;5;241m=\u001b[39mmath\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;241m/\u001b[39m batch_size)\n\u001b[1;32m     16\u001b[0m         config\u001b[38;5;241m.\u001b[39mnum_step_per_epoch\u001b[38;5;241m=\u001b[39mnum_step\n\u001b[0;32m---> 17\u001b[0m         train_loss,valid_loss,train_accuracy,valid_accuracy \u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mgpu_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mvail_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvail_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mwand\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwand\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                                                                     \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m         results_per_fold\u001b[38;5;241m.\u001b[39mappend([train_accuracy, valid_accuracy])\n\u001b[1;32m     28\u001b[0m wandb\u001b[38;5;241m.\u001b[39malert(\n\u001b[1;32m     29\u001b[0m             title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinish\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     30\u001b[0m             text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinishing training\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m         )\n",
      "File \u001b[0;32m~/eeg_mi/common.py:78\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, gpu_num, train_loader, test_loader, vail_loader, optimizer, criterion, wand)\u001b[0m\n\u001b[1;32m     75\u001b[0m     classes \u001b[38;5;241m=\u001b[39m classes\u001b[38;5;241m.\u001b[39mcuda(gpu_num)\n\u001b[1;32m     77\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 78\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, classes)\n\u001b[1;32m     81\u001b[0m iter_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [75]\u001b[0m, in \u001b[0;36mgamenet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpooling2(out)\n\u001b[1;32m    111\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(out)\n\u001b[0;32m--> 112\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(out)\n\u001b[1;32m    114\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:2436\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2424\u001b[0m         batch_norm,\n\u001b[1;32m   2425\u001b[0m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2433\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2434\u001b[0m     )\n\u001b[1;32m   2435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m-> 2436\u001b[0m     \u001b[43m_verify_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2439\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[1;32m   2440\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:2404\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2402\u001b[0m     size_prods \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m size[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   2403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_prods \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(size))\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 1024])"
     ]
    }
   ],
   "source": [
    "results_per_fold = []\n",
    "batch_size = 32\n",
    "from common import train\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch_B in range(1):\n",
    "    print(\"Epoch Num {}\".format(epoch_B))\n",
    "    for fold_idx, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "\n",
    "        print('Fold {}'.format(fold_idx + 1))\n",
    "\n",
    "        train_loader, test_loader = setup_dataflow(X_tensor_train,y_tensor_train, train_idx, val_idx)\n",
    "        num_step =math.ceil(len(train_loader.dataset) / batch_size)\n",
    "        config.num_step_per_epoch=num_step\n",
    "        train_loss,valid_loss,train_accuracy,valid_accuracy =train(\n",
    "                                                                model = net,\n",
    "                                                                gpu_num = 0,\n",
    "                                                                train_loader = train_loader,\n",
    "                                                                test_loader = test_loader,\n",
    "                                                                vail_loader = vail_loader,\n",
    "                                                                optimizer = optimizer  ,\n",
    "                                                                criterion = criterion ,\n",
    "                                                                wand = wand\n",
    "                                                                     )\n",
    "        results_per_fold.append([train_accuracy, valid_accuracy])\n",
    "wandb.alert(\n",
    "            title='Finish',\n",
    "            text=f'Finishing training',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee728d9-5484-472b-b052-5edb26585493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

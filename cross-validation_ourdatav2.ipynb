{
 "cells": [
  {
   "cell_type": "raw",
   "id": "edc0735b-62f8-4efa-b828-7f7a11868b90",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/pytorch-ignite/pytorch-ignite.ai/blob/gh-pages/how-to-guides/07-cross-validation.ipynb#scrollTo=iyrEEGHD9Ie5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d83b651-50a4-492e-bf4d-c28546b01977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#comment this if you are not using puffer?\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19e59fe9-7be7-48c3-8612-02cbd5fc9742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import os\n",
    "import sys\n",
    "from mne.datasets import eegbci\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from mne.datasets import eegbci\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader,SubsetRandomSampler\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from common import setup_dataflow\n",
    "import wandb\n",
    "from mne.datasets import sample\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98d94ed6-61da-4ac6-be64-0c19e27023d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8543d9f-1cda-4609-97d8-a02ad0208221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbf7aecb-9c1f-4ad6-a018-1619a1a218de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG:\n",
    "    def __init__(self, path, base_url, subjects, runs):\n",
    "        self.subpath = ''\n",
    "        self.path = path\n",
    "        print(path)\n",
    "        self.base_url = base_url\n",
    "        self.subjects = subjects\n",
    "        self.runs = runs\n",
    "        \n",
    "        # download data if does not exist in path.\n",
    "        # self.load_data()\n",
    "        self.data_to_raw()\n",
    "    \n",
    "    def load_data(self):\n",
    "        print(f\">>> Start download from: {self.base_url}.\")\n",
    "        print(f\"Downloading files to: {self.path}.\")\n",
    "        for subject in self.subjects:\n",
    "            eegbci.load_data(subject,self.runs,path=self.path,base_url=self.base_url)\n",
    "        print(\"Done.\")\n",
    "    \n",
    "    \n",
    "        \n",
    "        print(\"Done.\")\n",
    "        return self.raw\n",
    "    def filter(self, freq):\n",
    "        raw = self.raw\n",
    "        low, high = freq\n",
    "        print(f\">>> Apply filter.\")\n",
    "        self.raw.filter(low, high, fir_design='firwin', verbose=20)\n",
    "        return  raw\n",
    "    def raw_ica(self):\n",
    "        raw = self.raw\n",
    "        ica = mne.preprocessing.ICA(n_components=1, max_iter=100)\n",
    "        ica.fit(raw)\n",
    "        ica.exclude = [1, 2]  # details on how we picked these are omitted here\n",
    "        ica.plot_properties(raw, picks=ica.exclude)\n",
    "        ica.apply(raw)\n",
    "        print('ICA DONE ????')\n",
    "        return  raw\n",
    "        \n",
    "    def get_events(self):\n",
    "        event_id = dict(T1=0, T2=1) # the events we want to extract\n",
    "        events, event_id = mne.events_from_annotations(self.raw, event_id=event_id)\n",
    "        return events, event_id\n",
    "    \n",
    "    def get_epochs(self, events, event_id):\n",
    "        picks = mne.pick_types(self.raw.info, eeg=True, exclude='bads')\n",
    "        tmin = 0\n",
    "        tmax = 4\n",
    "        epochs = mne.Epochs(self.raw, events, event_id, tmin, tmax, proj=True, \n",
    "                            picks=picks, baseline=None, preload=True)\n",
    "        return epochs\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    def data_to_raw(self):\n",
    "        fullpath = os.path.join(self.path, *self.subpath.split(sep='/'))\n",
    "        #print(f\">>> Extract all subjects from: {fullpath}.\")\n",
    "        extension = \"fif\"\n",
    "        raws = []\n",
    "        count = 1\n",
    "        for i, subject in enumerate(self.subjects):\n",
    "            sname = f\"S{str(subject).zfill(3)}\".upper()\n",
    "            \n",
    "            for j, run in enumerate(self.runs):\n",
    "                rname = f\"{sname}R{str(run).zfill(2)}\".upper()\n",
    "                path_file = os.path.join(fullpath, sname, f'{rname}.{extension}')\n",
    "                #print(path_file)\n",
    "                #print(f\"Loading file #{count}/{len(self.subjects)*len(self.runs)}: {f'{rname}.{extension}'}\")\n",
    "                raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
    "                raws.append(raw)\n",
    "                count += 1\n",
    "\n",
    "        raw = mne.io.concatenate_raws(raws)\n",
    "        eegbci.standardize(raw)\n",
    "        montage = mne.channels.make_standard_montage('standard_1005')\n",
    "        raw.set_montage(montage)\n",
    "        self.raw = raw\n",
    "    \n",
    "    def create_epochs(self):\n",
    "        print(\">>> Create Epochs.\")\n",
    "        \n",
    "        events, event_id = self.get_events()\n",
    "        self.epochs = self.get_epochs(events, event_id)\n",
    "        print(\"Done.\")\n",
    "        return events , event_id\n",
    "# getepoch(raw,4, 10,reject_bad=False,on_missing='warn')    \n",
    "    def get_X_y(self):\n",
    "        events = mne.find_events(raw)\n",
    "        \n",
    "        epochs = mne.Epochs(\n",
    "        raw,\n",
    "        events,\n",
    "        event_id=[1,2,3],\n",
    "        tmin=0,\n",
    "        tmax=5,\n",
    "        picks=\"data\",\n",
    "        on_missing='warn',\n",
    "        baseline=None,\n",
    "            preload=True\n",
    "    )\n",
    "        #epochs=epochs.resample(160)\n",
    "            #events , event_id=self.create_epochs()\n",
    "        self.X = epochs.get_data()\n",
    "        self.y = epochs.events[:, -1]\n",
    "        return self.X, self.y \n",
    "\n",
    "        \n",
    "def getepoch(raws,trial_duration, calibration_duration,reject_bad=False,on_missing='warn'):\n",
    "    #reject_criteria = dict(eeg=100e-6)  # 100 µV\n",
    "    #flat_criteria = dict(eeg=1e-6)  # 1 µV\n",
    "    epochs_list = []\n",
    "    raws = [raws]\n",
    "    print(len(raws))\n",
    "    for raw in raws:\n",
    "        print(raw)\n",
    "        events = mne.find_events(raw)\n",
    "        epochs = mne.Epochs(\n",
    "            raw,\n",
    "            events,\n",
    "            event_id=[1,2,3],\n",
    "            tmin=-calibration_duration,\n",
    "            tmax=trial_duration,\n",
    "            picks=\"data\",\n",
    "            on_missing=on_missing,\n",
    "            baseline=None,\n",
    "            preload=True\n",
    "        )\n",
    "        \n",
    "        epochs_list.append(epochs)\n",
    "    epochs = mne.concatenate_epochs(epochs_list)\n",
    "    labels = epochs.events[:,-1]\n",
    "    \n",
    "    print(f'Found {len(labels)} epochs')\n",
    "    print(epochs)\n",
    "    print(labels)\n",
    "\n",
    "    return epochs.get_data(),epochs,labels\n",
    "\n",
    "        \n",
    "        \n",
    "def do_plot(train_loss, valid_loss,ty):\n",
    "    if ty == \"loss\":\n",
    "        plt.figure(figsize=(10,10))\n",
    "        \n",
    "        plt.plot(train_loss, label='train_loss')\n",
    "        plt.plot(valid_loss, label='valid_loss')\n",
    "        plt.title('loss {}'.format(iter))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    if ty == \"acc\":\n",
    "        plt.figure(figsize=(10,10))\n",
    "        \n",
    "        plot_ty=torch.tensor(train_loss, device = 'cpu')\n",
    "        plat_va=torch.tensor(valid_loss, device = 'cpu')\n",
    "        plt.plot(plot_ty, label='train_acc')\n",
    "        plt.plot(plat_va, label='valid_acc')\n",
    "        plt.title('acc {}'.format(iter))\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0dd9b5d-1ed9-4cf2-a34f-6626c35159db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17852/3583653774.py:71: RuntimeWarning: This filename (S004/S004R03.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_17852/3583653774.py:71: RuntimeWarning: This filename (S004/S004R04.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_17852/3583653774.py:71: RuntimeWarning: This filename (S004/S004R05.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_17852/3583653774.py:71: RuntimeWarning: This filename (S004/S004R06.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_17852/3583653774.py:71: RuntimeWarning: This filename (S004/S004R07.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_17852/3583653774.py:71: RuntimeWarning: This filename (S004/S004R08.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_17852/3583653774.py:71: RuntimeWarning: This filename (S004/S004R03.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_17852/3583653774.py:71: RuntimeWarning: This filename (S004/S004R04.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_17852/3583653774.py:71: RuntimeWarning: This filename (S004/S004R05.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_17852/3583653774.py:71: RuntimeWarning: This filename (S004/S004R06.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_17852/3583653774.py:71: RuntimeWarning: This filename (S004/S004R07.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n",
      "/tmp/ipykernel_17852/3583653774.py:71: RuntimeWarning: This filename (S004/S004R08.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw = mne.io.read_raw_fif( path_file , preload=True, verbose='WARNING' )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw done\n",
      ">>> Apply filter.\n",
      "Filtering raw data in 6 contiguous segments\n",
      "Setting up band-pass filter from 1 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-6 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 825 samples (3.300 sec)\n",
      "\n",
      "Filter done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# home directory + datasets folder\n",
    "#path = '/content/drive/MyDrive/MNE-eegbci-data/files/eegmmidb/'\n",
    "path = ''\n",
    "base_url = 'https://physionet.org/files/eegmmidb/'\n",
    "# subjects = [1]\n",
    "#runs = [3, 4, 7, 8, 11, 12]\n",
    "runs = [3,4, 5,6, 7,8]\n",
    "subjects = [i for i in range(4,5)]\n",
    "#subjects = [1]\n",
    "# runs = [6,10,14]\n",
    "print(path)\n",
    "eeg = EEG(path, base_url, subjects, runs)\n",
    "raw=eeg.data_to_raw()\n",
    "\n",
    "print(\"Raw done\")\n",
    "# apply filter \n",
    "freq = (1., 45.)\n",
    "raw=eeg.filter(freq=freq)\n",
    "#raw=eeg.data_to_raw()\n",
    "print(\"Filter done\")\n",
    "#raw=eeg.raw_ica()\n",
    "#eeg.create_epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a751921c-5ff5-4a08-ad1f-d503e6062002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 events found\n",
      "Event IDs: [1 2 4]\n",
      "Not setting metadata\n",
      "144 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 144 events and 1251 original time points ...\n",
      "0 bad epochs dropped\n",
      "(144, 2, 1251) (144,)\n",
      "(144, 2, 1251) (144,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17852/3583653774.py:92: RuntimeWarning: No matching events found for 3 (event id 3)\n",
      "  epochs = mne.Epochs(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(144, 2, 1251)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = eeg.get_X_y()\n",
    "print(X.shape, y.shape)\n",
    "#X = X[:, :,np.newaxis,:]\n",
    "print(X.shape, y.shape)\n",
    "#X = X[:,:,:]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f59a7188-2295-4501-8f8f-f00df822f73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=y-1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87f728da-0a9a-4875-927d-b012cba21e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size (100, 2, 1251) (100,)\n",
      "Test size (44, 2, 1251) (44,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y)\n",
    "\n",
    "print('Train size',X_train.shape, y_train.shape)\n",
    "print('Test size',X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "595d124d-a3ef-4806-8643-a3974599ea79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b040bf58-7c18-43f5-897d-bc2651a97a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X_test).float()\n",
    "y_tensor = torch.tensor(y_test).long()\n",
    "\n",
    "dataset_vail = TensorDataset(X_tensor, y_tensor)\n",
    "vail_loader = torch.utils.data.DataLoader(dataset_vail, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b45b6f4-78e1-4c82-a544-bb2abed895cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset\n",
    "X_tensor_train = torch.tensor(X_train).float()\n",
    "y_tensor_train = torch.tensor(y_train).long()\n",
    "\n",
    "\n",
    "\n",
    "dataset = TensorDataset(X_tensor_train, y_tensor_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d940d-5dac-4cbd-a00b-d0d691f19c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc4f40c8-dac3-4b28-b20b-dad2fe492377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        #using sequential helps bind multiple operations together\n",
    "        self.layer1 = nn.Sequential(\n",
    "            #padding = (kernel_size - 1) / 2 = 2\n",
    "            nn.Conv1d(2, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc = nn.Linear(160128, 2)\n",
    "        self.drop_out = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.drop_out(out)\n",
    "        out = out.reshape(out.size(0), -1)   #can also use .view()\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519df3e-478b-4497-9971-9912335cfec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5fcd2047-d53e-4480-ad09-c9d7ec30b205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4xyo6yq5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test/Test_accuracy</td><td>▃▂▂▁▁▂▁▁▁▁▁▁▁▁▇▇▆▇▆▆▆▆▆▆▆▆▆█████████████</td></tr><tr><td>Test/Test_loss</td><td>▅▅▅▅▆▆▆▇▇▇▇███▃▄▅▆▆▆▆▇▇▇██▇▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/train_accuracy</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>train/train_loss</td><td>█▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test/Test_accuracy</td><td>100.0</td></tr><tr><td>Test/Test_loss</td><td>0.00864</td></tr><tr><td>train/train_accuracy</td><td>100.0</td></tr><tr><td>train/train_loss</td><td>0.00023</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">ourdata_CNN_2ch_2class_cross_ourdata_s4</strong>: <a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery/runs/4xyo6yq5\" target=\"_blank\">https://wandb.ai/nutapol-1997/Motor-Imagery/runs/4xyo6yq5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220914_065756-4xyo6yq5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4xyo6yq5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nutapolt/eeg_mi/wandb/run-20220914_070109-2vttbdh4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery/runs/2vttbdh4\" target=\"_blank\">ourdata_CNN_2ch_2class_cross_ourdata_s4</a></strong> to <a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e751a47-f30f-4187-a43d-698e941345a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44735353-f086-40c5-8035-1197643e4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 3\n",
    "#splits = KFold(n_splits=num_folds,shuffle=True,random_state=42)\n",
    "splits = KFold(n_splits=num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da7aa5d0-fd03-4e29-9f23-13664c413e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNet().cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd638e-4114-456f-b194-cd37b7447463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09b5210f-8b9c-4970-8c45-1159cafdea08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "torch.Size([66, 2, 1251])\n",
      "torch.Size([66])\n",
      "Fold 2\n",
      "torch.Size([67, 2, 1251])\n",
      "torch.Size([67])\n",
      "Fold 3\n",
      "torch.Size([67, 2, 1251])\n",
      "torch.Size([67])\n"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "        \n",
    "\n",
    "        print('Fold {}'.format(fold_idx + 1))\n",
    "        #print('Fold {}'.format(fold_idx + 1))\n",
    "        print(X_tensor_train[train_idx].shape)\n",
    "        print(y_tensor_train[train_idx].shape)\n",
    "        #train_loader, val_loader = setup_dataflow(dataset, train_idx, val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7ef542f-6886-49be-b91b-32a7ad651b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Num 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1petjb69) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test/Test_accuracy</td><td>▁▁▃▃▃▄▄▂▂▂▁▁▁▁████▇▆▅▅▅▅▄▄▄█████████████</td></tr><tr><td>Test/Test_loss</td><td>▇▇▇▇▇▇▇▇▇▇████▂▃▃▄▄▅▅▆▆▆▆▇▇▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/train_accuracy</td><td>▁▂▄▅▅▆▇▇▇▇▇███▅▆▆▇▇▇▇▇█████▇█▇▇█████████</td></tr><tr><td>train/train_loss</td><td>▇█▇▆▂▂▅▂▂▅▅▃▁▂▅▁▁▆▃▃▄▃▂▁▁▂▅▃▃▃▂▂▁▂▁▁▂▂▂▁</td></tr><tr><td>val/val_accuracy</td><td>▅▃▅▆▆▃▃▂▂▂▂▂▁▁▂▄▄▃▃▂▄▄▄▄▄▄▄▃▄▄▅▅▆▇▇▇▇██▇</td></tr><tr><td>val/val_loss</td><td>▂▂▁▁▁▁▂▂▂▂▃▄▄▄▄▄▂▅▄▅▅▄▃▃▅▄▆▆▅▃▆▅▆▆▆▆▆▅▃█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test/Test_accuracy</td><td>96.9697</td></tr><tr><td>Test/Test_loss</td><td>0.06954</td></tr><tr><td>train/train_accuracy</td><td>100.0</td></tr><tr><td>train/train_loss</td><td>0.00859</td></tr><tr><td>val/val_accuracy</td><td>66.66667</td></tr><tr><td>val/val_loss</td><td>0.94889</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">ourdata_CNN_2ch_2class_cross_ourdata_s4</strong>: <a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery/runs/1petjb69\" target=\"_blank\">https://wandb.ai/nutapol-1997/Motor-Imagery/runs/1petjb69</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220914_070154-1petjb69/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1petjb69). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nutapolt/eeg_mi/wandb/run-20220914_072455-2n4ldimc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery/runs/2n4ldimc\" target=\"_blank\">ourdata_CNN_2ch_2class_cross_ourdata_s4</a></strong> to <a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/50000, Tr Loss: 0.7244, Tr Acc: 51.5152, Val Loss: 0.6950, Val Acc: 44.1176\n",
      "Val Loss: 0.6921749711036682, Val Acc: 50.0\n",
      "Epoch 101/50000, Tr Loss: 0.6205, Tr Acc: 57.5758, Val Loss: 0.7017, Val Acc: 55.8824\n",
      "Val Loss: 0.7181984484195709, Val Acc: 50.0\n",
      "Epoch 201/50000, Tr Loss: 0.6902, Tr Acc: 50.0000, Val Loss: 0.7006, Val Acc: 55.8824\n",
      "Val Loss: 0.7097906768321991, Val Acc: 50.0\n",
      "Epoch 301/50000, Tr Loss: 0.6254, Tr Acc: 50.0000, Val Loss: 0.7003, Val Acc: 50.0000\n",
      "Val Loss: 0.7111593782901764, Val Acc: 50.0\n",
      "Epoch 401/50000, Tr Loss: 0.6997, Tr Acc: 59.0909, Val Loss: 0.6998, Val Acc: 50.0000\n",
      "Val Loss: 0.7196997702121735, Val Acc: 52.27272727272727\n",
      "Epoch 501/50000, Tr Loss: 0.6098, Tr Acc: 46.9697, Val Loss: 0.6999, Val Acc: 52.9412\n",
      "Val Loss: 0.7342279255390167, Val Acc: 52.27272727272727\n",
      "Epoch 601/50000, Tr Loss: 0.6182, Tr Acc: 54.5455, Val Loss: 0.6996, Val Acc: 52.9412\n",
      "Val Loss: 0.7149656414985657, Val Acc: 52.27272727272727\n",
      "Epoch 701/50000, Tr Loss: 0.6160, Tr Acc: 56.0606, Val Loss: 0.6997, Val Acc: 55.8824\n",
      "Val Loss: 0.7529277205467224, Val Acc: 52.27272727272727\n",
      "Epoch 801/50000, Tr Loss: 0.6030, Tr Acc: 51.5152, Val Loss: 0.6997, Val Acc: 55.8824\n",
      "Val Loss: 0.7147190570831299, Val Acc: 52.27272727272727\n",
      "Epoch 901/50000, Tr Loss: 0.6011, Tr Acc: 46.9697, Val Loss: 0.6999, Val Acc: 55.8824\n",
      "Val Loss: 0.7436507642269135, Val Acc: 52.27272727272727\n",
      "Epoch 1001/50000, Tr Loss: 0.6332, Tr Acc: 57.5758, Val Loss: 0.7002, Val Acc: 55.8824\n",
      "Val Loss: 0.7329489290714264, Val Acc: 52.27272727272727\n",
      "Epoch 1101/50000, Tr Loss: 0.6397, Tr Acc: 50.0000, Val Loss: 0.7006, Val Acc: 55.8824\n",
      "Val Loss: 0.7363347113132477, Val Acc: 52.27272727272727\n",
      "Epoch 1201/50000, Tr Loss: 0.6704, Tr Acc: 56.0606, Val Loss: 0.7005, Val Acc: 58.8235\n",
      "Val Loss: 0.7423513233661652, Val Acc: 52.27272727272727\n",
      "Epoch 1301/50000, Tr Loss: 0.6598, Tr Acc: 46.9697, Val Loss: 0.7005, Val Acc: 58.8235\n",
      "Val Loss: 0.7382919490337372, Val Acc: 52.27272727272727\n",
      "Epoch 1401/50000, Tr Loss: 0.6044, Tr Acc: 53.0303, Val Loss: 0.7008, Val Acc: 58.8235\n",
      "Val Loss: 0.7225029766559601, Val Acc: 52.27272727272727\n",
      "Epoch 1501/50000, Tr Loss: 0.6917, Tr Acc: 59.0909, Val Loss: 0.7009, Val Acc: 58.8235\n",
      "Val Loss: 0.7276736199855804, Val Acc: 52.27272727272727\n",
      "Epoch 1601/50000, Tr Loss: 0.6116, Tr Acc: 53.0303, Val Loss: 0.7011, Val Acc: 58.8235\n",
      "Val Loss: 0.755683571100235, Val Acc: 54.54545454545454\n",
      "Epoch 1701/50000, Tr Loss: 0.6492, Tr Acc: 46.9697, Val Loss: 0.7013, Val Acc: 58.8235\n",
      "Val Loss: 0.7427853345870972, Val Acc: 54.54545454545454\n",
      "Epoch 1801/50000, Tr Loss: 0.5838, Tr Acc: 57.5758, Val Loss: 0.7013, Val Acc: 58.8235\n",
      "Val Loss: 0.7880104184150696, Val Acc: 54.54545454545454\n",
      "Epoch 1901/50000, Tr Loss: 0.5981, Tr Acc: 50.0000, Val Loss: 0.7015, Val Acc: 58.8235\n",
      "Val Loss: 0.7587826550006866, Val Acc: 54.54545454545454\n",
      "Epoch 2001/50000, Tr Loss: 0.6685, Tr Acc: 56.0606, Val Loss: 0.7016, Val Acc: 58.8235\n",
      "Val Loss: 0.750304788351059, Val Acc: 54.54545454545454\n",
      "Epoch 2101/50000, Tr Loss: 0.6362, Tr Acc: 46.9697, Val Loss: 0.7017, Val Acc: 58.8235\n",
      "Val Loss: 0.7492748200893402, Val Acc: 54.54545454545454\n",
      "Epoch 2201/50000, Tr Loss: 0.5556, Tr Acc: 51.5152, Val Loss: 0.7016, Val Acc: 58.8235\n",
      "Val Loss: 0.7383663058280945, Val Acc: 52.27272727272727\n",
      "Epoch 2301/50000, Tr Loss: 0.5686, Tr Acc: 60.6061, Val Loss: 0.7016, Val Acc: 58.8235\n",
      "Val Loss: 0.7719183266162872, Val Acc: 52.27272727272727\n",
      "Epoch 2401/50000, Tr Loss: 0.5868, Tr Acc: 48.4848, Val Loss: 0.7018, Val Acc: 58.8235\n",
      "Val Loss: 0.7314914762973785, Val Acc: 52.27272727272727\n",
      "Epoch 2501/50000, Tr Loss: 0.6580, Tr Acc: 46.9697, Val Loss: 0.7019, Val Acc: 61.7647\n",
      "Val Loss: 0.7495312690734863, Val Acc: 52.27272727272727\n",
      "Epoch 2601/50000, Tr Loss: 0.6122, Tr Acc: 54.5455, Val Loss: 0.7019, Val Acc: 61.7647\n",
      "Val Loss: 0.8074935376644135, Val Acc: 52.27272727272727\n",
      "Epoch 2701/50000, Tr Loss: 0.5629, Tr Acc: 63.6364, Val Loss: 0.7020, Val Acc: 61.7647\n",
      "Val Loss: 0.740647166967392, Val Acc: 52.27272727272727\n",
      "Epoch 2801/50000, Tr Loss: 0.7255, Tr Acc: 53.0303, Val Loss: 0.7020, Val Acc: 61.7647\n",
      "Val Loss: 0.7745711803436279, Val Acc: 54.54545454545454\n",
      "Epoch 2901/50000, Tr Loss: 0.7013, Tr Acc: 48.4848, Val Loss: 0.7020, Val Acc: 61.7647\n",
      "Val Loss: 0.7656008303165436, Val Acc: 54.54545454545454\n",
      "Epoch 3001/50000, Tr Loss: 0.6109, Tr Acc: 51.5152, Val Loss: 0.7020, Val Acc: 61.7647\n",
      "Val Loss: 0.7465278804302216, Val Acc: 52.27272727272727\n",
      "Epoch 3101/50000, Tr Loss: 0.6073, Tr Acc: 57.5758, Val Loss: 0.7016, Val Acc: 61.7647\n",
      "Val Loss: 0.7490118145942688, Val Acc: 52.27272727272727\n",
      "Epoch 3201/50000, Tr Loss: 0.5947, Tr Acc: 63.6364, Val Loss: 0.7016, Val Acc: 61.7647\n",
      "Val Loss: 0.7242695987224579, Val Acc: 52.27272727272727\n",
      "Epoch 3301/50000, Tr Loss: 0.6193, Tr Acc: 51.5152, Val Loss: 0.7016, Val Acc: 61.7647\n",
      "Val Loss: 0.7685447335243225, Val Acc: 52.27272727272727\n",
      "Epoch 3401/50000, Tr Loss: 0.6918, Tr Acc: 51.5152, Val Loss: 0.7015, Val Acc: 61.7647\n",
      "Val Loss: 0.7364011704921722, Val Acc: 50.0\n",
      "Epoch 3501/50000, Tr Loss: 0.6062, Tr Acc: 56.0606, Val Loss: 0.7016, Val Acc: 61.7647\n",
      "Val Loss: 0.6939233243465424, Val Acc: 50.0\n",
      "Epoch 3601/50000, Tr Loss: 0.5743, Tr Acc: 51.5152, Val Loss: 0.7012, Val Acc: 61.7647\n",
      "Val Loss: 0.7282159924507141, Val Acc: 50.0\n",
      "Epoch 3701/50000, Tr Loss: 0.5712, Tr Acc: 51.5152, Val Loss: 0.7011, Val Acc: 61.7647\n",
      "Val Loss: 0.7968710362911224, Val Acc: 50.0\n",
      "Epoch 3801/50000, Tr Loss: 0.6112, Tr Acc: 54.5455, Val Loss: 0.7018, Val Acc: 61.7647\n",
      "Val Loss: 0.7693014740943909, Val Acc: 50.0\n",
      "Epoch 3901/50000, Tr Loss: 0.5485, Tr Acc: 57.5758, Val Loss: 0.7016, Val Acc: 61.7647\n",
      "Val Loss: 0.7658011317253113, Val Acc: 50.0\n",
      "Epoch 4001/50000, Tr Loss: 0.6006, Tr Acc: 51.5152, Val Loss: 0.7017, Val Acc: 61.7647\n",
      "Val Loss: 0.7478438913822174, Val Acc: 50.0\n",
      "Epoch 4101/50000, Tr Loss: 0.7083, Tr Acc: 51.5152, Val Loss: 0.7014, Val Acc: 61.7647\n",
      "Val Loss: 0.7314378917217255, Val Acc: 47.72727272727273\n",
      "Epoch 4201/50000, Tr Loss: 0.5708, Tr Acc: 48.4848, Val Loss: 0.7012, Val Acc: 61.7647\n",
      "Val Loss: 0.75077024102211, Val Acc: 45.45454545454545\n",
      "Epoch 4301/50000, Tr Loss: 0.6066, Tr Acc: 53.0303, Val Loss: 0.7015, Val Acc: 61.7647\n",
      "Val Loss: 0.7471022009849548, Val Acc: 45.45454545454545\n",
      "Epoch 4401/50000, Tr Loss: 0.5710, Tr Acc: 53.0303, Val Loss: 0.7013, Val Acc: 61.7647\n",
      "Val Loss: 0.7413757741451263, Val Acc: 47.72727272727273\n",
      "Epoch 4501/50000, Tr Loss: 0.6513, Tr Acc: 60.6061, Val Loss: 0.7016, Val Acc: 58.8235\n",
      "Val Loss: 0.7894394099712372, Val Acc: 47.72727272727273\n",
      "Epoch 4601/50000, Tr Loss: 0.5827, Tr Acc: 62.1212, Val Loss: 0.7016, Val Acc: 58.8235\n",
      "Val Loss: 0.7521504461765289, Val Acc: 47.72727272727273\n",
      "Epoch 4701/50000, Tr Loss: 0.6017, Tr Acc: 54.5455, Val Loss: 0.7013, Val Acc: 58.8235\n",
      "Val Loss: 0.7340620458126068, Val Acc: 50.0\n",
      "Epoch 4801/50000, Tr Loss: 0.5640, Tr Acc: 59.0909, Val Loss: 0.7013, Val Acc: 58.8235\n",
      "Val Loss: 0.7183670997619629, Val Acc: 50.0\n",
      "Epoch 4901/50000, Tr Loss: 0.5621, Tr Acc: 60.6061, Val Loss: 0.7012, Val Acc: 61.7647\n",
      "Val Loss: 0.727493166923523, Val Acc: 50.0\n",
      "Epoch 5001/50000, Tr Loss: 0.5587, Tr Acc: 60.6061, Val Loss: 0.7010, Val Acc: 61.7647\n",
      "Val Loss: 0.7642489075660706, Val Acc: 52.27272727272727\n",
      "Epoch 5101/50000, Tr Loss: 0.5388, Tr Acc: 56.0606, Val Loss: 0.7013, Val Acc: 61.7647\n",
      "Val Loss: 0.7517784237861633, Val Acc: 52.27272727272727\n",
      "Epoch 5201/50000, Tr Loss: 0.5383, Tr Acc: 63.6364, Val Loss: 0.7012, Val Acc: 61.7647\n",
      "Val Loss: 0.7646503150463104, Val Acc: 50.0\n",
      "Epoch 5301/50000, Tr Loss: 0.5819, Tr Acc: 45.4545, Val Loss: 0.7012, Val Acc: 61.7647\n",
      "Val Loss: 0.7694990336894989, Val Acc: 52.27272727272727\n",
      "Epoch 5401/50000, Tr Loss: 0.5483, Tr Acc: 56.0606, Val Loss: 0.7009, Val Acc: 61.7647\n",
      "Val Loss: 0.7824653685092926, Val Acc: 52.27272727272727\n",
      "Epoch 5501/50000, Tr Loss: 0.6248, Tr Acc: 56.0606, Val Loss: 0.7009, Val Acc: 64.7059\n",
      "Val Loss: 0.7672767043113708, Val Acc: 52.27272727272727\n",
      "Epoch 5601/50000, Tr Loss: 0.5415, Tr Acc: 65.1515, Val Loss: 0.7007, Val Acc: 64.7059\n",
      "Val Loss: 0.7514823377132416, Val Acc: 52.27272727272727\n",
      "Epoch 5701/50000, Tr Loss: 0.5672, Tr Acc: 56.0606, Val Loss: 0.7008, Val Acc: 64.7059\n",
      "Val Loss: 0.737440288066864, Val Acc: 52.27272727272727\n",
      "Epoch 5801/50000, Tr Loss: 0.5185, Tr Acc: 63.6364, Val Loss: 0.7006, Val Acc: 64.7059\n",
      "Val Loss: 0.7776573598384857, Val Acc: 52.27272727272727\n",
      "Epoch 5901/50000, Tr Loss: 0.5144, Tr Acc: 62.1212, Val Loss: 0.7006, Val Acc: 64.7059\n",
      "Val Loss: 0.7434907853603363, Val Acc: 52.27272727272727\n",
      "Epoch 6001/50000, Tr Loss: 0.5258, Tr Acc: 60.6061, Val Loss: 0.7004, Val Acc: 64.7059\n",
      "Val Loss: 0.7653216123580933, Val Acc: 52.27272727272727\n",
      "Epoch 6101/50000, Tr Loss: 0.5057, Tr Acc: 60.6061, Val Loss: 0.7003, Val Acc: 64.7059\n",
      "Val Loss: 0.7635881900787354, Val Acc: 52.27272727272727\n",
      "Epoch 6201/50000, Tr Loss: 0.5011, Tr Acc: 60.6061, Val Loss: 0.7002, Val Acc: 64.7059\n",
      "Val Loss: 0.758587121963501, Val Acc: 52.27272727272727\n",
      "Epoch 6301/50000, Tr Loss: 0.5302, Tr Acc: 63.6364, Val Loss: 0.7005, Val Acc: 64.7059\n",
      "Val Loss: 0.7505253851413727, Val Acc: 52.27272727272727\n",
      "Epoch 6401/50000, Tr Loss: 0.5490, Tr Acc: 65.1515, Val Loss: 0.7004, Val Acc: 64.7059\n",
      "Val Loss: 0.7529628276824951, Val Acc: 52.27272727272727\n",
      "Epoch 6501/50000, Tr Loss: 0.6381, Tr Acc: 54.5455, Val Loss: 0.7004, Val Acc: 64.7059\n",
      "Val Loss: 0.7885275185108185, Val Acc: 52.27272727272727\n",
      "Epoch 6601/50000, Tr Loss: 0.6309, Tr Acc: 51.5152, Val Loss: 0.7003, Val Acc: 64.7059\n",
      "Val Loss: 0.7102669775485992, Val Acc: 52.27272727272727\n",
      "Epoch 6701/50000, Tr Loss: 0.5391, Tr Acc: 62.1212, Val Loss: 0.7003, Val Acc: 64.7059\n",
      "Val Loss: 0.7524979412555695, Val Acc: 52.27272727272727\n",
      "Epoch 6801/50000, Tr Loss: 0.5986, Tr Acc: 56.0606, Val Loss: 0.7000, Val Acc: 64.7059\n",
      "Val Loss: 0.7136858701705933, Val Acc: 52.27272727272727\n",
      "Epoch 6901/50000, Tr Loss: 0.5163, Tr Acc: 54.5455, Val Loss: 0.7000, Val Acc: 64.7059\n",
      "Val Loss: 0.8064659535884857, Val Acc: 52.27272727272727\n",
      "Epoch 7001/50000, Tr Loss: 0.5718, Tr Acc: 60.6061, Val Loss: 0.7000, Val Acc: 64.7059\n",
      "Val Loss: 0.7710496187210083, Val Acc: 52.27272727272727\n",
      "Epoch 7101/50000, Tr Loss: 0.5203, Tr Acc: 56.0606, Val Loss: 0.7000, Val Acc: 64.7059\n",
      "Val Loss: 0.7664091289043427, Val Acc: 52.27272727272727\n",
      "Epoch 7201/50000, Tr Loss: 0.5468, Tr Acc: 57.5758, Val Loss: 0.6996, Val Acc: 64.7059\n",
      "Val Loss: 0.791101485490799, Val Acc: 52.27272727272727\n",
      "Epoch 7301/50000, Tr Loss: 0.5410, Tr Acc: 53.0303, Val Loss: 0.6999, Val Acc: 64.7059\n",
      "Val Loss: 0.777422308921814, Val Acc: 52.27272727272727\n",
      "Epoch 7401/50000, Tr Loss: 0.5736, Tr Acc: 56.0606, Val Loss: 0.6997, Val Acc: 64.7059\n",
      "Val Loss: 0.7815676629543304, Val Acc: 52.27272727272727\n",
      "Epoch 7501/50000, Tr Loss: 0.5005, Tr Acc: 60.6061, Val Loss: 0.6997, Val Acc: 64.7059\n",
      "Val Loss: 0.7569544017314911, Val Acc: 52.27272727272727\n",
      "Epoch 7601/50000, Tr Loss: 0.5377, Tr Acc: 62.1212, Val Loss: 0.6995, Val Acc: 64.7059\n",
      "Val Loss: 0.7785867154598236, Val Acc: 52.27272727272727\n",
      "Epoch 7701/50000, Tr Loss: 0.5078, Tr Acc: 66.6667, Val Loss: 0.6997, Val Acc: 64.7059\n",
      "Val Loss: 0.7542848885059357, Val Acc: 52.27272727272727\n",
      "Epoch 7801/50000, Tr Loss: 0.5427, Tr Acc: 53.0303, Val Loss: 0.6995, Val Acc: 64.7059\n",
      "Val Loss: 0.7565390169620514, Val Acc: 52.27272727272727\n",
      "Epoch 7901/50000, Tr Loss: 0.5138, Tr Acc: 59.0909, Val Loss: 0.6993, Val Acc: 64.7059\n",
      "Val Loss: 0.7959286272525787, Val Acc: 52.27272727272727\n",
      "Epoch 8001/50000, Tr Loss: 0.6061, Tr Acc: 50.0000, Val Loss: 0.6992, Val Acc: 64.7059\n",
      "Val Loss: 0.763676643371582, Val Acc: 52.27272727272727\n",
      "Epoch 8101/50000, Tr Loss: 0.5636, Tr Acc: 57.5758, Val Loss: 0.6991, Val Acc: 64.7059\n",
      "Val Loss: 0.715734988451004, Val Acc: 52.27272727272727\n",
      "Epoch 8201/50000, Tr Loss: 0.5366, Tr Acc: 62.1212, Val Loss: 0.6993, Val Acc: 64.7059\n",
      "Val Loss: 0.7681485712528229, Val Acc: 52.27272727272727\n",
      "Epoch 8301/50000, Tr Loss: 0.4677, Tr Acc: 62.1212, Val Loss: 0.6990, Val Acc: 64.7059\n",
      "Val Loss: 0.7621784508228302, Val Acc: 52.27272727272727\n",
      "Epoch 8401/50000, Tr Loss: 0.5495, Tr Acc: 63.6364, Val Loss: 0.6992, Val Acc: 64.7059\n",
      "Val Loss: 0.7506066560745239, Val Acc: 52.27272727272727\n",
      "Epoch 8501/50000, Tr Loss: 0.5025, Tr Acc: 65.1515, Val Loss: 0.6990, Val Acc: 61.7647\n",
      "Val Loss: 0.7502647042274475, Val Acc: 50.0\n",
      "Epoch 8601/50000, Tr Loss: 0.5097, Tr Acc: 65.1515, Val Loss: 0.6987, Val Acc: 61.7647\n",
      "Val Loss: 0.7576563358306885, Val Acc: 50.0\n",
      "Epoch 8701/50000, Tr Loss: 0.5546, Tr Acc: 60.6061, Val Loss: 0.6988, Val Acc: 61.7647\n",
      "Val Loss: 0.7662746906280518, Val Acc: 50.0\n",
      "Epoch 8801/50000, Tr Loss: 0.5796, Tr Acc: 63.6364, Val Loss: 0.6989, Val Acc: 61.7647\n",
      "Val Loss: 0.8049158155918121, Val Acc: 50.0\n",
      "Epoch 8901/50000, Tr Loss: 0.5571, Tr Acc: 59.0909, Val Loss: 0.6985, Val Acc: 61.7647\n",
      "Val Loss: 0.7489875853061676, Val Acc: 50.0\n",
      "Epoch 9001/50000, Tr Loss: 0.4871, Tr Acc: 57.5758, Val Loss: 0.6987, Val Acc: 61.7647\n",
      "Val Loss: 0.7693749964237213, Val Acc: 50.0\n",
      "Epoch 9101/50000, Tr Loss: 0.5609, Tr Acc: 56.0606, Val Loss: 0.6982, Val Acc: 61.7647\n",
      "Val Loss: 0.7543803453445435, Val Acc: 47.72727272727273\n",
      "Epoch 9201/50000, Tr Loss: 0.4694, Tr Acc: 66.6667, Val Loss: 0.6982, Val Acc: 61.7647\n",
      "Val Loss: 0.7978384792804718, Val Acc: 47.72727272727273\n",
      "Epoch 9301/50000, Tr Loss: 0.5229, Tr Acc: 54.5455, Val Loss: 0.6981, Val Acc: 61.7647\n",
      "Val Loss: 0.7673990428447723, Val Acc: 47.72727272727273\n",
      "Epoch 9401/50000, Tr Loss: 0.4841, Tr Acc: 68.1818, Val Loss: 0.6978, Val Acc: 61.7647\n",
      "Val Loss: 0.781433641910553, Val Acc: 47.72727272727273\n",
      "Epoch 9501/50000, Tr Loss: 0.5169, Tr Acc: 60.6061, Val Loss: 0.6981, Val Acc: 61.7647\n",
      "Val Loss: 0.776718407869339, Val Acc: 47.72727272727273\n",
      "Epoch 9601/50000, Tr Loss: 0.4748, Tr Acc: 68.1818, Val Loss: 0.6978, Val Acc: 61.7647\n",
      "Val Loss: 0.7549245655536652, Val Acc: 47.72727272727273\n",
      "Epoch 9701/50000, Tr Loss: 0.5146, Tr Acc: 62.1212, Val Loss: 0.6979, Val Acc: 61.7647\n",
      "Val Loss: 0.8049630522727966, Val Acc: 47.72727272727273\n",
      "Epoch 9801/50000, Tr Loss: 0.4903, Tr Acc: 65.1515, Val Loss: 0.6977, Val Acc: 61.7647\n",
      "Val Loss: 0.7558969855308533, Val Acc: 47.72727272727273\n",
      "Epoch 9901/50000, Tr Loss: 0.4809, Tr Acc: 63.6364, Val Loss: 0.6975, Val Acc: 61.7647\n",
      "Val Loss: 0.7400572001934052, Val Acc: 47.72727272727273\n",
      "Epoch 10001/50000, Tr Loss: 0.5075, Tr Acc: 60.6061, Val Loss: 0.6979, Val Acc: 61.7647\n",
      "Val Loss: 0.8030509948730469, Val Acc: 47.72727272727273\n",
      "Epoch 10101/50000, Tr Loss: 0.5589, Tr Acc: 62.1212, Val Loss: 0.6977, Val Acc: 61.7647\n",
      "Val Loss: 0.7414211630821228, Val Acc: 47.72727272727273\n",
      "Epoch 10201/50000, Tr Loss: 0.4889, Tr Acc: 69.6970, Val Loss: 0.6971, Val Acc: 61.7647\n",
      "Val Loss: 0.746961772441864, Val Acc: 47.72727272727273\n",
      "Epoch 10301/50000, Tr Loss: 0.5251, Tr Acc: 60.6061, Val Loss: 0.6975, Val Acc: 61.7647\n",
      "Val Loss: 0.7738830745220184, Val Acc: 47.72727272727273\n",
      "Epoch 10401/50000, Tr Loss: 0.4570, Tr Acc: 62.1212, Val Loss: 0.6974, Val Acc: 61.7647\n",
      "Val Loss: 0.7993994355201721, Val Acc: 47.72727272727273\n",
      "Epoch 10501/50000, Tr Loss: 0.5249, Tr Acc: 62.1212, Val Loss: 0.6972, Val Acc: 61.7647\n",
      "Val Loss: 0.7432186603546143, Val Acc: 47.72727272727273\n",
      "Epoch 10601/50000, Tr Loss: 0.5088, Tr Acc: 63.6364, Val Loss: 0.6970, Val Acc: 61.7647\n",
      "Val Loss: 0.738135039806366, Val Acc: 47.72727272727273\n",
      "Epoch 10701/50000, Tr Loss: 0.5198, Tr Acc: 66.6667, Val Loss: 0.6970, Val Acc: 61.7647\n",
      "Val Loss: 0.7473472058773041, Val Acc: 47.72727272727273\n",
      "Epoch 10801/50000, Tr Loss: 0.5144, Tr Acc: 63.6364, Val Loss: 0.6970, Val Acc: 61.7647\n",
      "Val Loss: 0.7143807709217072, Val Acc: 47.72727272727273\n",
      "Epoch 10901/50000, Tr Loss: 0.5208, Tr Acc: 65.1515, Val Loss: 0.6969, Val Acc: 61.7647\n",
      "Val Loss: 0.7573883831501007, Val Acc: 47.72727272727273\n",
      "Epoch 11001/50000, Tr Loss: 0.5208, Tr Acc: 59.0909, Val Loss: 0.6968, Val Acc: 61.7647\n",
      "Val Loss: 0.7820672988891602, Val Acc: 47.72727272727273\n",
      "Epoch 11101/50000, Tr Loss: 0.4936, Tr Acc: 62.1212, Val Loss: 0.6969, Val Acc: 61.7647\n",
      "Val Loss: 0.7891320884227753, Val Acc: 47.72727272727273\n",
      "Epoch 11201/50000, Tr Loss: 0.5305, Tr Acc: 59.0909, Val Loss: 0.6966, Val Acc: 61.7647\n",
      "Val Loss: 0.7521458864212036, Val Acc: 47.72727272727273\n",
      "Epoch 11301/50000, Tr Loss: 0.5015, Tr Acc: 66.6667, Val Loss: 0.6965, Val Acc: 61.7647\n",
      "Val Loss: 0.7748865485191345, Val Acc: 47.72727272727273\n",
      "Epoch 11401/50000, Tr Loss: 0.5215, Tr Acc: 60.6061, Val Loss: 0.6961, Val Acc: 61.7647\n",
      "Val Loss: 0.7517053782939911, Val Acc: 47.72727272727273\n",
      "Epoch 11501/50000, Tr Loss: 0.4984, Tr Acc: 62.1212, Val Loss: 0.6964, Val Acc: 61.7647\n",
      "Val Loss: 0.7866155803203583, Val Acc: 47.72727272727273\n",
      "Epoch 11601/50000, Tr Loss: 0.4955, Tr Acc: 68.1818, Val Loss: 0.6963, Val Acc: 61.7647\n",
      "Val Loss: 0.7418807446956635, Val Acc: 47.72727272727273\n",
      "Epoch 11701/50000, Tr Loss: 0.4912, Tr Acc: 62.1212, Val Loss: 0.6959, Val Acc: 61.7647\n",
      "Val Loss: 0.803100973367691, Val Acc: 47.72727272727273\n",
      "Epoch 11801/50000, Tr Loss: 0.4714, Tr Acc: 71.2121, Val Loss: 0.6959, Val Acc: 61.7647\n",
      "Val Loss: 0.7429128289222717, Val Acc: 47.72727272727273\n",
      "Epoch 11901/50000, Tr Loss: 0.5599, Tr Acc: 60.6061, Val Loss: 0.6959, Val Acc: 61.7647\n",
      "Val Loss: 0.8039398491382599, Val Acc: 47.72727272727273\n",
      "Epoch 12001/50000, Tr Loss: 0.5017, Tr Acc: 65.1515, Val Loss: 0.6960, Val Acc: 61.7647\n",
      "Val Loss: 0.7730929851531982, Val Acc: 47.72727272727273\n",
      "Epoch 12101/50000, Tr Loss: 0.5278, Tr Acc: 63.6364, Val Loss: 0.6953, Val Acc: 61.7647\n",
      "Val Loss: 0.7731891572475433, Val Acc: 47.72727272727273\n",
      "Epoch 12201/50000, Tr Loss: 0.4934, Tr Acc: 66.6667, Val Loss: 0.6955, Val Acc: 61.7647\n",
      "Val Loss: 0.7496253252029419, Val Acc: 47.72727272727273\n",
      "Epoch 12301/50000, Tr Loss: 0.4866, Tr Acc: 65.1515, Val Loss: 0.6952, Val Acc: 61.7647\n",
      "Val Loss: 0.7340571582317352, Val Acc: 47.72727272727273\n",
      "Epoch 12401/50000, Tr Loss: 0.4789, Tr Acc: 63.6364, Val Loss: 0.6952, Val Acc: 61.7647\n",
      "Val Loss: 0.7918337881565094, Val Acc: 47.72727272727273\n",
      "Epoch 12501/50000, Tr Loss: 0.4707, Tr Acc: 65.1515, Val Loss: 0.6950, Val Acc: 61.7647\n",
      "Val Loss: 0.7228634357452393, Val Acc: 47.72727272727273\n",
      "Epoch 12601/50000, Tr Loss: 0.4555, Tr Acc: 68.1818, Val Loss: 0.6953, Val Acc: 61.7647\n",
      "Val Loss: 0.7606869637966156, Val Acc: 47.72727272727273\n",
      "Epoch 12701/50000, Tr Loss: 0.5000, Tr Acc: 62.1212, Val Loss: 0.6952, Val Acc: 61.7647\n",
      "Val Loss: 0.7588646113872528, Val Acc: 47.72727272727273\n",
      "Epoch 12801/50000, Tr Loss: 0.5031, Tr Acc: 60.6061, Val Loss: 0.6950, Val Acc: 61.7647\n",
      "Val Loss: 0.7959302663803101, Val Acc: 47.72727272727273\n",
      "Epoch 12901/50000, Tr Loss: 0.4922, Tr Acc: 65.1515, Val Loss: 0.6947, Val Acc: 61.7647\n",
      "Val Loss: 0.7484056949615479, Val Acc: 47.72727272727273\n",
      "Epoch 13001/50000, Tr Loss: 0.4545, Tr Acc: 62.1212, Val Loss: 0.6948, Val Acc: 61.7647\n",
      "Val Loss: 0.76082843542099, Val Acc: 47.72727272727273\n",
      "Epoch 13101/50000, Tr Loss: 0.4882, Tr Acc: 59.0909, Val Loss: 0.6948, Val Acc: 61.7647\n",
      "Val Loss: 0.7805218398571014, Val Acc: 47.72727272727273\n",
      "Epoch 13201/50000, Tr Loss: 0.4529, Tr Acc: 65.1515, Val Loss: 0.6947, Val Acc: 61.7647\n",
      "Val Loss: 0.76895672082901, Val Acc: 47.72727272727273\n",
      "Epoch 13301/50000, Tr Loss: 0.4994, Tr Acc: 59.0909, Val Loss: 0.6947, Val Acc: 61.7647\n",
      "Val Loss: 0.80032679438591, Val Acc: 47.72727272727273\n",
      "Epoch 13401/50000, Tr Loss: 0.4912, Tr Acc: 68.1818, Val Loss: 0.6947, Val Acc: 61.7647\n",
      "Val Loss: 0.7228842377662659, Val Acc: 47.72727272727273\n",
      "Epoch 13501/50000, Tr Loss: 0.4520, Tr Acc: 63.6364, Val Loss: 0.6945, Val Acc: 61.7647\n",
      "Val Loss: 0.8048817217350006, Val Acc: 47.72727272727273\n",
      "Epoch 13601/50000, Tr Loss: 0.4806, Tr Acc: 60.6061, Val Loss: 0.6942, Val Acc: 61.7647\n",
      "Val Loss: 0.7438225150108337, Val Acc: 47.72727272727273\n",
      "Epoch 13701/50000, Tr Loss: 0.4992, Tr Acc: 62.1212, Val Loss: 0.6943, Val Acc: 61.7647\n",
      "Val Loss: 0.8189499974250793, Val Acc: 47.72727272727273\n",
      "Epoch 13801/50000, Tr Loss: 0.5018, Tr Acc: 63.6364, Val Loss: 0.6943, Val Acc: 61.7647\n",
      "Val Loss: 0.7624349296092987, Val Acc: 47.72727272727273\n",
      "Epoch 13901/50000, Tr Loss: 0.5740, Tr Acc: 60.6061, Val Loss: 0.6941, Val Acc: 61.7647\n",
      "Val Loss: 0.7388020157814026, Val Acc: 47.72727272727273\n",
      "Epoch 14001/50000, Tr Loss: 0.5375, Tr Acc: 63.6364, Val Loss: 0.6938, Val Acc: 61.7647\n",
      "Val Loss: 0.7585097551345825, Val Acc: 47.72727272727273\n",
      "Epoch 14101/50000, Tr Loss: 0.5012, Tr Acc: 65.1515, Val Loss: 0.6940, Val Acc: 61.7647\n",
      "Val Loss: 0.7850506007671356, Val Acc: 47.72727272727273\n",
      "Epoch 14201/50000, Tr Loss: 0.5016, Tr Acc: 63.6364, Val Loss: 0.6939, Val Acc: 61.7647\n",
      "Val Loss: 0.7716101109981537, Val Acc: 47.72727272727273\n",
      "Epoch 14301/50000, Tr Loss: 0.5062, Tr Acc: 62.1212, Val Loss: 0.6936, Val Acc: 61.7647\n",
      "Val Loss: 0.79227414727211, Val Acc: 47.72727272727273\n",
      "Epoch 14401/50000, Tr Loss: 0.4891, Tr Acc: 62.1212, Val Loss: 0.6937, Val Acc: 61.7647\n",
      "Val Loss: 0.751176118850708, Val Acc: 47.72727272727273\n",
      "Epoch 14501/50000, Tr Loss: 0.4849, Tr Acc: 62.1212, Val Loss: 0.6937, Val Acc: 61.7647\n",
      "Val Loss: 0.7484406232833862, Val Acc: 47.72727272727273\n",
      "Epoch 14601/50000, Tr Loss: 0.4512, Tr Acc: 65.1515, Val Loss: 0.6935, Val Acc: 61.7647\n",
      "Val Loss: 0.7217994928359985, Val Acc: 47.72727272727273\n",
      "Epoch 14701/50000, Tr Loss: 0.4892, Tr Acc: 69.6970, Val Loss: 0.6934, Val Acc: 61.7647\n",
      "Val Loss: 0.7195782363414764, Val Acc: 47.72727272727273\n",
      "Epoch 14801/50000, Tr Loss: 0.4710, Tr Acc: 68.1818, Val Loss: 0.6935, Val Acc: 61.7647\n",
      "Val Loss: 0.7822685241699219, Val Acc: 47.72727272727273\n",
      "Epoch 14901/50000, Tr Loss: 0.4798, Tr Acc: 71.2121, Val Loss: 0.6932, Val Acc: 61.7647\n",
      "Val Loss: 0.7778069078922272, Val Acc: 47.72727272727273\n",
      "Epoch 15001/50000, Tr Loss: 0.4904, Tr Acc: 63.6364, Val Loss: 0.6933, Val Acc: 61.7647\n",
      "Val Loss: 0.7618768513202667, Val Acc: 47.72727272727273\n",
      "Epoch 15101/50000, Tr Loss: 0.4363, Tr Acc: 69.6970, Val Loss: 0.6932, Val Acc: 61.7647\n",
      "Val Loss: 0.7499249279499054, Val Acc: 47.72727272727273\n",
      "Epoch 15201/50000, Tr Loss: 0.4650, Tr Acc: 68.1818, Val Loss: 0.6930, Val Acc: 61.7647\n",
      "Val Loss: 0.7729065716266632, Val Acc: 47.72727272727273\n",
      "Epoch 15301/50000, Tr Loss: 0.4721, Tr Acc: 69.6970, Val Loss: 0.6932, Val Acc: 61.7647\n",
      "Val Loss: 0.788736492395401, Val Acc: 47.72727272727273\n",
      "Epoch 15401/50000, Tr Loss: 0.4522, Tr Acc: 68.1818, Val Loss: 0.6929, Val Acc: 61.7647\n",
      "Val Loss: 0.7224670946598053, Val Acc: 47.72727272727273\n",
      "Epoch 15501/50000, Tr Loss: 0.4482, Tr Acc: 66.6667, Val Loss: 0.6926, Val Acc: 61.7647\n",
      "Val Loss: 0.7719064056873322, Val Acc: 47.72727272727273\n",
      "Epoch 15601/50000, Tr Loss: 0.4286, Tr Acc: 68.1818, Val Loss: 0.6928, Val Acc: 61.7647\n",
      "Val Loss: 0.7578177750110626, Val Acc: 47.72727272727273\n",
      "Epoch 15701/50000, Tr Loss: 0.4359, Tr Acc: 65.1515, Val Loss: 0.6927, Val Acc: 61.7647\n",
      "Val Loss: 0.7546691596508026, Val Acc: 47.72727272727273\n",
      "Epoch 15801/50000, Tr Loss: 0.4787, Tr Acc: 62.1212, Val Loss: 0.6926, Val Acc: 61.7647\n",
      "Val Loss: 0.746891438961029, Val Acc: 47.72727272727273\n",
      "Epoch 15901/50000, Tr Loss: 0.4805, Tr Acc: 68.1818, Val Loss: 0.6924, Val Acc: 61.7647\n",
      "Val Loss: 0.795862078666687, Val Acc: 47.72727272727273\n",
      "Epoch 16001/50000, Tr Loss: 0.4575, Tr Acc: 65.1515, Val Loss: 0.6925, Val Acc: 61.7647\n",
      "Val Loss: 0.7339882850646973, Val Acc: 47.72727272727273\n",
      "Epoch 16101/50000, Tr Loss: 0.4570, Tr Acc: 72.7273, Val Loss: 0.6923, Val Acc: 61.7647\n",
      "Val Loss: 0.7837733626365662, Val Acc: 47.72727272727273\n",
      "Epoch 16201/50000, Tr Loss: 0.4873, Tr Acc: 65.1515, Val Loss: 0.6921, Val Acc: 61.7647\n",
      "Val Loss: 0.780680239200592, Val Acc: 47.72727272727273\n",
      "Epoch 16301/50000, Tr Loss: 0.5037, Tr Acc: 63.6364, Val Loss: 0.6923, Val Acc: 61.7647\n",
      "Val Loss: 0.7957586348056793, Val Acc: 47.72727272727273\n",
      "Epoch 16401/50000, Tr Loss: 0.4597, Tr Acc: 68.1818, Val Loss: 0.6921, Val Acc: 61.7647\n",
      "Val Loss: 0.7769265472888947, Val Acc: 47.72727272727273\n",
      "Epoch 16501/50000, Tr Loss: 0.4444, Tr Acc: 69.6970, Val Loss: 0.6921, Val Acc: 61.7647\n",
      "Val Loss: 0.7559184730052948, Val Acc: 47.72727272727273\n",
      "Epoch 16601/50000, Tr Loss: 0.4676, Tr Acc: 60.6061, Val Loss: 0.6918, Val Acc: 61.7647\n",
      "Val Loss: 0.7682065963745117, Val Acc: 47.72727272727273\n",
      "Epoch 16701/50000, Tr Loss: 0.4283, Tr Acc: 65.1515, Val Loss: 0.6921, Val Acc: 61.7647\n",
      "Val Loss: 0.7582235932350159, Val Acc: 47.72727272727273\n",
      "Epoch 16801/50000, Tr Loss: 0.4642, Tr Acc: 69.6970, Val Loss: 0.6921, Val Acc: 61.7647\n",
      "Val Loss: 0.7451041340827942, Val Acc: 47.72727272727273\n",
      "Epoch 16901/50000, Tr Loss: 0.4562, Tr Acc: 63.6364, Val Loss: 0.6916, Val Acc: 61.7647\n",
      "Val Loss: 0.7859740257263184, Val Acc: 47.72727272727273\n",
      "Epoch 17001/50000, Tr Loss: 0.4467, Tr Acc: 65.1515, Val Loss: 0.6917, Val Acc: 61.7647\n",
      "Val Loss: 0.7481625974178314, Val Acc: 47.72727272727273\n",
      "Epoch 17101/50000, Tr Loss: 0.4474, Tr Acc: 74.2424, Val Loss: 0.6918, Val Acc: 61.7647\n",
      "Val Loss: 0.7272753417491913, Val Acc: 47.72727272727273\n",
      "Epoch 17201/50000, Tr Loss: 0.4646, Tr Acc: 69.6970, Val Loss: 0.6916, Val Acc: 64.7059\n",
      "Val Loss: 0.7622997760772705, Val Acc: 47.72727272727273\n",
      "Epoch 17301/50000, Tr Loss: 0.4474, Tr Acc: 65.1515, Val Loss: 0.6917, Val Acc: 64.7059\n",
      "Val Loss: 0.7557271718978882, Val Acc: 47.72727272727273\n",
      "Epoch 17401/50000, Tr Loss: 0.4899, Tr Acc: 62.1212, Val Loss: 0.6915, Val Acc: 64.7059\n",
      "Val Loss: 0.7761172950267792, Val Acc: 47.72727272727273\n",
      "Epoch 17501/50000, Tr Loss: 0.4597, Tr Acc: 68.1818, Val Loss: 0.6918, Val Acc: 64.7059\n",
      "Val Loss: 0.7369779944419861, Val Acc: 47.72727272727273\n",
      "Epoch 17601/50000, Tr Loss: 0.4585, Tr Acc: 66.6667, Val Loss: 0.6914, Val Acc: 64.7059\n",
      "Val Loss: 0.7496571242809296, Val Acc: 47.72727272727273\n",
      "Epoch 17701/50000, Tr Loss: 0.5055, Tr Acc: 62.1212, Val Loss: 0.6913, Val Acc: 64.7059\n",
      "Val Loss: 0.7559719979763031, Val Acc: 47.72727272727273\n",
      "Epoch 17801/50000, Tr Loss: 0.4465, Tr Acc: 74.2424, Val Loss: 0.6914, Val Acc: 64.7059\n",
      "Val Loss: 0.7689750790596008, Val Acc: 47.72727272727273\n",
      "Epoch 17901/50000, Tr Loss: 0.4515, Tr Acc: 71.2121, Val Loss: 0.6915, Val Acc: 64.7059\n",
      "Val Loss: 0.7600278556346893, Val Acc: 47.72727272727273\n",
      "Epoch 18001/50000, Tr Loss: 0.4472, Tr Acc: 63.6364, Val Loss: 0.6915, Val Acc: 64.7059\n",
      "Val Loss: 0.7337705194950104, Val Acc: 47.72727272727273\n",
      "Epoch 18101/50000, Tr Loss: 0.4606, Tr Acc: 62.1212, Val Loss: 0.6915, Val Acc: 64.7059\n",
      "Val Loss: 0.7539420127868652, Val Acc: 47.72727272727273\n",
      "Epoch 18201/50000, Tr Loss: 0.4447, Tr Acc: 66.6667, Val Loss: 0.6912, Val Acc: 64.7059\n",
      "Val Loss: 0.7642051577568054, Val Acc: 47.72727272727273\n",
      "Epoch 18301/50000, Tr Loss: 0.4605, Tr Acc: 71.2121, Val Loss: 0.6915, Val Acc: 64.7059\n",
      "Val Loss: 0.7666402757167816, Val Acc: 47.72727272727273\n",
      "Epoch 18401/50000, Tr Loss: 0.4465, Tr Acc: 66.6667, Val Loss: 0.6916, Val Acc: 64.7059\n",
      "Val Loss: 0.7392863929271698, Val Acc: 47.72727272727273\n",
      "Epoch 18501/50000, Tr Loss: 0.4309, Tr Acc: 65.1515, Val Loss: 0.6913, Val Acc: 64.7059\n",
      "Val Loss: 0.7559487521648407, Val Acc: 47.72727272727273\n",
      "Epoch 18601/50000, Tr Loss: 0.4420, Tr Acc: 63.6364, Val Loss: 0.6913, Val Acc: 64.7059\n",
      "Val Loss: 0.754706859588623, Val Acc: 47.72727272727273\n",
      "Epoch 18701/50000, Tr Loss: 0.4560, Tr Acc: 56.0606, Val Loss: 0.6913, Val Acc: 64.7059\n",
      "Val Loss: 0.7659759819507599, Val Acc: 47.72727272727273\n",
      "Epoch 18801/50000, Tr Loss: 0.4628, Tr Acc: 60.6061, Val Loss: 0.6911, Val Acc: 64.7059\n",
      "Val Loss: 0.7521935999393463, Val Acc: 47.72727272727273\n",
      "Epoch 18901/50000, Tr Loss: 0.4507, Tr Acc: 65.1515, Val Loss: 0.6912, Val Acc: 64.7059\n",
      "Val Loss: 0.7638270854949951, Val Acc: 47.72727272727273\n",
      "Epoch 19001/50000, Tr Loss: 0.4317, Tr Acc: 68.1818, Val Loss: 0.6911, Val Acc: 64.7059\n",
      "Val Loss: 0.7486940324306488, Val Acc: 47.72727272727273\n",
      "Epoch 19101/50000, Tr Loss: 0.4551, Tr Acc: 68.1818, Val Loss: 0.6913, Val Acc: 64.7059\n",
      "Val Loss: 0.7073827385902405, Val Acc: 47.72727272727273\n",
      "Epoch 19201/50000, Tr Loss: 0.4097, Tr Acc: 72.7273, Val Loss: 0.6911, Val Acc: 64.7059\n",
      "Val Loss: 0.7862108945846558, Val Acc: 47.72727272727273\n",
      "Epoch 19301/50000, Tr Loss: 0.4321, Tr Acc: 65.1515, Val Loss: 0.6911, Val Acc: 64.7059\n",
      "Val Loss: 0.7539161145687103, Val Acc: 47.72727272727273\n",
      "Epoch 19401/50000, Tr Loss: 0.4515, Tr Acc: 72.7273, Val Loss: 0.6913, Val Acc: 64.7059\n",
      "Val Loss: 0.7531006336212158, Val Acc: 47.72727272727273\n",
      "Epoch 19501/50000, Tr Loss: 0.4453, Tr Acc: 74.2424, Val Loss: 0.6910, Val Acc: 64.7059\n",
      "Val Loss: 0.7553037405014038, Val Acc: 47.72727272727273\n",
      "Epoch 19601/50000, Tr Loss: 0.4278, Tr Acc: 66.6667, Val Loss: 0.6908, Val Acc: 61.7647\n",
      "Val Loss: 0.7711364030838013, Val Acc: 47.72727272727273\n",
      "Epoch 19701/50000, Tr Loss: 0.4658, Tr Acc: 62.1212, Val Loss: 0.6912, Val Acc: 61.7647\n",
      "Val Loss: 0.750503659248352, Val Acc: 47.72727272727273\n",
      "Epoch 19801/50000, Tr Loss: 0.4330, Tr Acc: 69.6970, Val Loss: 0.6911, Val Acc: 61.7647\n",
      "Val Loss: 0.7550525367259979, Val Acc: 47.72727272727273\n",
      "Epoch 19901/50000, Tr Loss: 0.4554, Tr Acc: 60.6061, Val Loss: 0.6907, Val Acc: 61.7647\n",
      "Val Loss: 0.7644993960857391, Val Acc: 50.0\n",
      "Epoch 20001/50000, Tr Loss: 0.4668, Tr Acc: 65.1515, Val Loss: 0.6908, Val Acc: 61.7647\n",
      "Val Loss: 0.7323373258113861, Val Acc: 50.0\n",
      "Epoch 20101/50000, Tr Loss: 0.4205, Tr Acc: 66.6667, Val Loss: 0.6907, Val Acc: 61.7647\n",
      "Val Loss: 0.7452451586723328, Val Acc: 50.0\n",
      "Epoch 20201/50000, Tr Loss: 0.5035, Tr Acc: 59.0909, Val Loss: 0.6909, Val Acc: 61.7647\n",
      "Val Loss: 0.7412050366401672, Val Acc: 50.0\n",
      "Epoch 20301/50000, Tr Loss: 0.4135, Tr Acc: 74.2424, Val Loss: 0.6908, Val Acc: 61.7647\n",
      "Val Loss: 0.7856884598731995, Val Acc: 50.0\n",
      "Epoch 20401/50000, Tr Loss: 0.4029, Tr Acc: 69.6970, Val Loss: 0.6907, Val Acc: 61.7647\n",
      "Val Loss: 0.7787615060806274, Val Acc: 50.0\n",
      "Epoch 20501/50000, Tr Loss: 0.4434, Tr Acc: 69.6970, Val Loss: 0.6910, Val Acc: 61.7647\n",
      "Val Loss: 0.7269237339496613, Val Acc: 50.0\n",
      "Epoch 20601/50000, Tr Loss: 0.4547, Tr Acc: 65.1515, Val Loss: 0.6908, Val Acc: 61.7647\n",
      "Val Loss: 0.7562213242053986, Val Acc: 50.0\n",
      "Epoch 20701/50000, Tr Loss: 0.4228, Tr Acc: 71.2121, Val Loss: 0.6908, Val Acc: 61.7647\n",
      "Val Loss: 0.7571607232093811, Val Acc: 50.0\n",
      "Epoch 20801/50000, Tr Loss: 0.3941, Tr Acc: 75.7576, Val Loss: 0.6906, Val Acc: 61.7647\n",
      "Val Loss: 0.7712465524673462, Val Acc: 50.0\n",
      "Epoch 20901/50000, Tr Loss: 0.4497, Tr Acc: 66.6667, Val Loss: 0.6908, Val Acc: 61.7647\n",
      "Val Loss: 0.7497919499874115, Val Acc: 50.0\n",
      "Epoch 21001/50000, Tr Loss: 0.4562, Tr Acc: 69.6970, Val Loss: 0.6909, Val Acc: 61.7647\n",
      "Val Loss: 0.7457507848739624, Val Acc: 50.0\n",
      "Epoch 21101/50000, Tr Loss: 0.4352, Tr Acc: 65.1515, Val Loss: 0.6908, Val Acc: 61.7647\n",
      "Val Loss: 0.7498611509799957, Val Acc: 52.27272727272727\n",
      "Epoch 21201/50000, Tr Loss: 0.4498, Tr Acc: 65.1515, Val Loss: 0.6907, Val Acc: 64.7059\n",
      "Val Loss: 0.7838684618473053, Val Acc: 52.27272727272727\n",
      "Epoch 21301/50000, Tr Loss: 0.4173, Tr Acc: 75.7576, Val Loss: 0.6906, Val Acc: 64.7059\n",
      "Val Loss: 0.7446971535682678, Val Acc: 52.27272727272727\n",
      "Epoch 21401/50000, Tr Loss: 0.4237, Tr Acc: 66.6667, Val Loss: 0.6908, Val Acc: 64.7059\n",
      "Val Loss: 0.7610492408275604, Val Acc: 52.27272727272727\n",
      "Epoch 21501/50000, Tr Loss: 0.4307, Tr Acc: 66.6667, Val Loss: 0.6907, Val Acc: 64.7059\n",
      "Val Loss: 0.73151496052742, Val Acc: 52.27272727272727\n",
      "Epoch 21601/50000, Tr Loss: 0.3989, Tr Acc: 75.7576, Val Loss: 0.6907, Val Acc: 64.7059\n",
      "Val Loss: 0.7473558485507965, Val Acc: 52.27272727272727\n",
      "Epoch 21701/50000, Tr Loss: 0.3936, Tr Acc: 71.2121, Val Loss: 0.6908, Val Acc: 64.7059\n",
      "Val Loss: 0.7594012022018433, Val Acc: 52.27272727272727\n",
      "Epoch 21801/50000, Tr Loss: 0.3944, Tr Acc: 74.2424, Val Loss: 0.6908, Val Acc: 64.7059\n",
      "Val Loss: 0.7183010280132294, Val Acc: 52.27272727272727\n",
      "Epoch 21901/50000, Tr Loss: 0.4674, Tr Acc: 65.1515, Val Loss: 0.6906, Val Acc: 64.7059\n",
      "Val Loss: 0.7501722872257233, Val Acc: 52.27272727272727\n",
      "Epoch 22001/50000, Tr Loss: 0.4233, Tr Acc: 72.7273, Val Loss: 0.6905, Val Acc: 67.6471\n",
      "Val Loss: 0.741872102022171, Val Acc: 52.27272727272727\n",
      "Epoch 22101/50000, Tr Loss: 0.4821, Tr Acc: 63.6364, Val Loss: 0.6907, Val Acc: 67.6471\n",
      "Val Loss: 0.7577615678310394, Val Acc: 52.27272727272727\n",
      "Epoch 22201/50000, Tr Loss: 0.4164, Tr Acc: 71.2121, Val Loss: 0.6907, Val Acc: 67.6471\n",
      "Val Loss: 0.7991411685943604, Val Acc: 52.27272727272727\n",
      "Epoch 22301/50000, Tr Loss: 0.4487, Tr Acc: 66.6667, Val Loss: 0.6907, Val Acc: 67.6471\n",
      "Val Loss: 0.7275271415710449, Val Acc: 52.27272727272727\n",
      "Epoch 22401/50000, Tr Loss: 0.3767, Tr Acc: 75.7576, Val Loss: 0.6907, Val Acc: 67.6471\n",
      "Val Loss: 0.7205655872821808, Val Acc: 52.27272727272727\n",
      "Epoch 22501/50000, Tr Loss: 0.3991, Tr Acc: 78.7879, Val Loss: 0.6905, Val Acc: 67.6471\n",
      "Val Loss: 0.760952889919281, Val Acc: 52.27272727272727\n",
      "Epoch 22601/50000, Tr Loss: 0.4249, Tr Acc: 68.1818, Val Loss: 0.6909, Val Acc: 67.6471\n",
      "Val Loss: 0.7643449008464813, Val Acc: 52.27272727272727\n",
      "Epoch 22701/50000, Tr Loss: 0.4105, Tr Acc: 74.2424, Val Loss: 0.6906, Val Acc: 67.6471\n",
      "Val Loss: 0.7410080432891846, Val Acc: 52.27272727272727\n",
      "Epoch 22801/50000, Tr Loss: 0.4228, Tr Acc: 71.2121, Val Loss: 0.6906, Val Acc: 67.6471\n",
      "Val Loss: 0.763889878988266, Val Acc: 52.27272727272727\n",
      "Epoch 22901/50000, Tr Loss: 0.4045, Tr Acc: 74.2424, Val Loss: 0.6908, Val Acc: 67.6471\n",
      "Val Loss: 0.7446179389953613, Val Acc: 52.27272727272727\n",
      "Epoch 23001/50000, Tr Loss: 0.4591, Tr Acc: 69.6970, Val Loss: 0.6909, Val Acc: 67.6471\n",
      "Val Loss: 0.7406823933124542, Val Acc: 52.27272727272727\n",
      "Epoch 23101/50000, Tr Loss: 0.4071, Tr Acc: 77.2727, Val Loss: 0.6906, Val Acc: 67.6471\n",
      "Val Loss: 0.6996726989746094, Val Acc: 52.27272727272727\n",
      "Epoch 23201/50000, Tr Loss: 0.4035, Tr Acc: 77.2727, Val Loss: 0.6907, Val Acc: 67.6471\n",
      "Val Loss: 0.7457934021949768, Val Acc: 52.27272727272727\n",
      "Epoch 23301/50000, Tr Loss: 0.4220, Tr Acc: 72.7273, Val Loss: 0.6907, Val Acc: 67.6471\n",
      "Val Loss: 0.7517909705638885, Val Acc: 52.27272727272727\n",
      "Epoch 23401/50000, Tr Loss: 0.3868, Tr Acc: 68.1818, Val Loss: 0.6908, Val Acc: 67.6471\n",
      "Val Loss: 0.7251877188682556, Val Acc: 50.0\n",
      "Epoch 23501/50000, Tr Loss: 0.4023, Tr Acc: 71.2121, Val Loss: 0.6907, Val Acc: 67.6471\n",
      "Val Loss: 0.7064513564109802, Val Acc: 50.0\n",
      "Epoch 23601/50000, Tr Loss: 0.4321, Tr Acc: 68.1818, Val Loss: 0.6909, Val Acc: 67.6471\n",
      "Val Loss: 0.7516517341136932, Val Acc: 50.0\n",
      "Epoch 23701/50000, Tr Loss: 0.4342, Tr Acc: 68.1818, Val Loss: 0.6910, Val Acc: 67.6471\n",
      "Val Loss: 0.7134946286678314, Val Acc: 50.0\n",
      "Epoch 23801/50000, Tr Loss: 0.3919, Tr Acc: 74.2424, Val Loss: 0.6907, Val Acc: 67.6471\n",
      "Val Loss: 0.7543204128742218, Val Acc: 50.0\n",
      "Epoch 23901/50000, Tr Loss: 0.4138, Tr Acc: 68.1818, Val Loss: 0.6907, Val Acc: 67.6471\n",
      "Val Loss: 0.7754918336868286, Val Acc: 50.0\n",
      "Epoch 24001/50000, Tr Loss: 0.4455, Tr Acc: 65.1515, Val Loss: 0.6907, Val Acc: 67.6471\n",
      "Val Loss: 0.7354614734649658, Val Acc: 50.0\n",
      "Epoch 24101/50000, Tr Loss: 0.3999, Tr Acc: 78.7879, Val Loss: 0.6909, Val Acc: 67.6471\n",
      "Val Loss: 0.7859750092029572, Val Acc: 50.0\n",
      "Epoch 24201/50000, Tr Loss: 0.4455, Tr Acc: 68.1818, Val Loss: 0.6910, Val Acc: 67.6471\n",
      "Val Loss: 0.7526314854621887, Val Acc: 50.0\n",
      "Epoch 24301/50000, Tr Loss: 0.4288, Tr Acc: 72.7273, Val Loss: 0.6912, Val Acc: 67.6471\n",
      "Val Loss: 0.7391803562641144, Val Acc: 50.0\n",
      "Epoch 24401/50000, Tr Loss: 0.3816, Tr Acc: 74.2424, Val Loss: 0.6912, Val Acc: 67.6471\n",
      "Val Loss: 0.7655238509178162, Val Acc: 50.0\n",
      "Epoch 24501/50000, Tr Loss: 0.4011, Tr Acc: 71.2121, Val Loss: 0.6910, Val Acc: 67.6471\n",
      "Val Loss: 0.7369283735752106, Val Acc: 50.0\n",
      "Epoch 24601/50000, Tr Loss: 0.3965, Tr Acc: 77.2727, Val Loss: 0.6912, Val Acc: 67.6471\n",
      "Val Loss: 0.7225698530673981, Val Acc: 50.0\n",
      "Epoch 24701/50000, Tr Loss: 0.3758, Tr Acc: 68.1818, Val Loss: 0.6912, Val Acc: 67.6471\n",
      "Val Loss: 0.7552041411399841, Val Acc: 50.0\n",
      "Epoch 24801/50000, Tr Loss: 0.3973, Tr Acc: 77.2727, Val Loss: 0.6911, Val Acc: 67.6471\n",
      "Val Loss: 0.8028875589370728, Val Acc: 50.0\n",
      "Epoch 24901/50000, Tr Loss: 0.4017, Tr Acc: 75.7576, Val Loss: 0.6910, Val Acc: 67.6471\n",
      "Val Loss: 0.7093425393104553, Val Acc: 50.0\n",
      "Epoch 25001/50000, Tr Loss: 0.4010, Tr Acc: 69.6970, Val Loss: 0.6909, Val Acc: 67.6471\n",
      "Val Loss: 0.7742806375026703, Val Acc: 50.0\n",
      "Epoch 25101/50000, Tr Loss: 0.3938, Tr Acc: 72.7273, Val Loss: 0.6909, Val Acc: 67.6471\n",
      "Val Loss: 0.7785792350769043, Val Acc: 50.0\n",
      "Epoch 25201/50000, Tr Loss: 0.4211, Tr Acc: 69.6970, Val Loss: 0.6912, Val Acc: 67.6471\n",
      "Val Loss: 0.7438090145587921, Val Acc: 50.0\n",
      "Epoch 25301/50000, Tr Loss: 0.4013, Tr Acc: 74.2424, Val Loss: 0.6911, Val Acc: 67.6471\n",
      "Val Loss: 0.7325560748577118, Val Acc: 50.0\n",
      "Epoch 25401/50000, Tr Loss: 0.4062, Tr Acc: 74.2424, Val Loss: 0.6911, Val Acc: 67.6471\n",
      "Val Loss: 0.7319009900093079, Val Acc: 50.0\n",
      "Epoch 25501/50000, Tr Loss: 0.3942, Tr Acc: 69.6970, Val Loss: 0.6912, Val Acc: 67.6471\n",
      "Val Loss: 0.7200137972831726, Val Acc: 50.0\n",
      "Epoch 25601/50000, Tr Loss: 0.4104, Tr Acc: 74.2424, Val Loss: 0.6913, Val Acc: 67.6471\n",
      "Val Loss: 0.7138936817646027, Val Acc: 50.0\n",
      "Epoch 25701/50000, Tr Loss: 0.3996, Tr Acc: 69.6970, Val Loss: 0.6912, Val Acc: 67.6471\n",
      "Val Loss: 0.735485702753067, Val Acc: 50.0\n",
      "Epoch 25801/50000, Tr Loss: 0.4388, Tr Acc: 66.6667, Val Loss: 0.6912, Val Acc: 67.6471\n",
      "Val Loss: 0.7554351091384888, Val Acc: 50.0\n",
      "Epoch 25901/50000, Tr Loss: 0.4140, Tr Acc: 74.2424, Val Loss: 0.6912, Val Acc: 67.6471\n",
      "Val Loss: 0.7311338186264038, Val Acc: 50.0\n",
      "Epoch 26001/50000, Tr Loss: 0.4275, Tr Acc: 74.2424, Val Loss: 0.6913, Val Acc: 67.6471\n",
      "Val Loss: 0.7504098117351532, Val Acc: 50.0\n",
      "Epoch 26101/50000, Tr Loss: 0.3894, Tr Acc: 69.6970, Val Loss: 0.6913, Val Acc: 67.6471\n",
      "Val Loss: 0.7128688097000122, Val Acc: 50.0\n",
      "Epoch 26201/50000, Tr Loss: 0.4553, Tr Acc: 69.6970, Val Loss: 0.6912, Val Acc: 67.6471\n",
      "Val Loss: 0.7296209037303925, Val Acc: 50.0\n",
      "Epoch 26301/50000, Tr Loss: 0.4180, Tr Acc: 72.7273, Val Loss: 0.6911, Val Acc: 67.6471\n",
      "Val Loss: 0.7936233580112457, Val Acc: 50.0\n",
      "Epoch 26401/50000, Tr Loss: 0.4086, Tr Acc: 71.2121, Val Loss: 0.6913, Val Acc: 67.6471\n",
      "Val Loss: 0.7432566285133362, Val Acc: 50.0\n",
      "Epoch 26501/50000, Tr Loss: 0.3702, Tr Acc: 75.7576, Val Loss: 0.6915, Val Acc: 67.6471\n",
      "Val Loss: 0.7604204714298248, Val Acc: 50.0\n",
      "Epoch 26601/50000, Tr Loss: 0.3851, Tr Acc: 71.2121, Val Loss: 0.6912, Val Acc: 67.6471\n",
      "Val Loss: 0.7105798125267029, Val Acc: 50.0\n",
      "Epoch 26701/50000, Tr Loss: 0.4217, Tr Acc: 74.2424, Val Loss: 0.6913, Val Acc: 67.6471\n",
      "Val Loss: 0.7705250084400177, Val Acc: 50.0\n",
      "Epoch 26801/50000, Tr Loss: 0.3879, Tr Acc: 75.7576, Val Loss: 0.6914, Val Acc: 67.6471\n",
      "Val Loss: 0.7786555886268616, Val Acc: 50.0\n",
      "Epoch 26901/50000, Tr Loss: 0.3951, Tr Acc: 74.2424, Val Loss: 0.6911, Val Acc: 67.6471\n",
      "Val Loss: 0.7203008532524109, Val Acc: 50.0\n",
      "Epoch 27001/50000, Tr Loss: 0.3800, Tr Acc: 77.2727, Val Loss: 0.6913, Val Acc: 67.6471\n",
      "Val Loss: 0.7815976738929749, Val Acc: 50.0\n",
      "Epoch 27101/50000, Tr Loss: 0.3556, Tr Acc: 78.7879, Val Loss: 0.6914, Val Acc: 67.6471\n",
      "Val Loss: 0.6890275180339813, Val Acc: 50.0\n",
      "Epoch 27201/50000, Tr Loss: 0.4533, Tr Acc: 66.6667, Val Loss: 0.6913, Val Acc: 67.6471\n",
      "Val Loss: 0.7776861190795898, Val Acc: 50.0\n",
      "Epoch 27301/50000, Tr Loss: 0.4057, Tr Acc: 72.7273, Val Loss: 0.6915, Val Acc: 70.5882\n",
      "Val Loss: 0.7657406032085419, Val Acc: 50.0\n",
      "Epoch 27401/50000, Tr Loss: 0.4012, Tr Acc: 71.2121, Val Loss: 0.6915, Val Acc: 70.5882\n",
      "Val Loss: 0.7547775208950043, Val Acc: 50.0\n",
      "Epoch 27501/50000, Tr Loss: 0.4362, Tr Acc: 71.2121, Val Loss: 0.6916, Val Acc: 70.5882\n",
      "Val Loss: 0.7495479583740234, Val Acc: 50.0\n",
      "Epoch 27601/50000, Tr Loss: 0.3629, Tr Acc: 81.8182, Val Loss: 0.6916, Val Acc: 70.5882\n",
      "Val Loss: 0.7676705121994019, Val Acc: 50.0\n",
      "Epoch 27701/50000, Tr Loss: 0.3949, Tr Acc: 71.2121, Val Loss: 0.6917, Val Acc: 70.5882\n",
      "Val Loss: 0.7384868562221527, Val Acc: 50.0\n",
      "Epoch 27801/50000, Tr Loss: 0.4021, Tr Acc: 78.7879, Val Loss: 0.6918, Val Acc: 70.5882\n",
      "Val Loss: 0.7422103583812714, Val Acc: 50.0\n",
      "Epoch 27901/50000, Tr Loss: 0.4059, Tr Acc: 68.1818, Val Loss: 0.6918, Val Acc: 70.5882\n",
      "Val Loss: 0.7340580224990845, Val Acc: 50.0\n",
      "Epoch 28001/50000, Tr Loss: 0.4176, Tr Acc: 71.2121, Val Loss: 0.6918, Val Acc: 70.5882\n",
      "Val Loss: 0.7735754549503326, Val Acc: 50.0\n",
      "Epoch 28101/50000, Tr Loss: 0.4011, Tr Acc: 75.7576, Val Loss: 0.6920, Val Acc: 70.5882\n",
      "Val Loss: 0.7243114709854126, Val Acc: 50.0\n",
      "Epoch 28201/50000, Tr Loss: 0.3993, Tr Acc: 71.2121, Val Loss: 0.6917, Val Acc: 70.5882\n",
      "Val Loss: 0.7530142664909363, Val Acc: 52.27272727272727\n",
      "Epoch 28301/50000, Tr Loss: 0.3991, Tr Acc: 74.2424, Val Loss: 0.6918, Val Acc: 70.5882\n",
      "Val Loss: 0.7733242511749268, Val Acc: 52.27272727272727\n",
      "Epoch 28401/50000, Tr Loss: 0.4025, Tr Acc: 69.6970, Val Loss: 0.6918, Val Acc: 70.5882\n",
      "Val Loss: 0.7346361577510834, Val Acc: 52.27272727272727\n",
      "Epoch 28501/50000, Tr Loss: 0.3639, Tr Acc: 72.7273, Val Loss: 0.6921, Val Acc: 70.5882\n",
      "Val Loss: 0.7598458230495453, Val Acc: 52.27272727272727\n",
      "Epoch 28601/50000, Tr Loss: 0.3951, Tr Acc: 68.1818, Val Loss: 0.6919, Val Acc: 70.5882\n",
      "Val Loss: 0.729297012090683, Val Acc: 50.0\n",
      "Epoch 28701/50000, Tr Loss: 0.3929, Tr Acc: 69.6970, Val Loss: 0.6923, Val Acc: 70.5882\n",
      "Val Loss: 0.738242506980896, Val Acc: 50.0\n",
      "Epoch 28801/50000, Tr Loss: 0.3997, Tr Acc: 77.2727, Val Loss: 0.6919, Val Acc: 70.5882\n",
      "Val Loss: 0.7373500466346741, Val Acc: 50.0\n",
      "Epoch 28901/50000, Tr Loss: 0.3710, Tr Acc: 78.7879, Val Loss: 0.6921, Val Acc: 70.5882\n",
      "Val Loss: 0.7788379788398743, Val Acc: 50.0\n",
      "Epoch 29001/50000, Tr Loss: 0.3723, Tr Acc: 72.7273, Val Loss: 0.6922, Val Acc: 70.5882\n",
      "Val Loss: 0.7320504188537598, Val Acc: 50.0\n",
      "Epoch 29101/50000, Tr Loss: 0.3563, Tr Acc: 77.2727, Val Loss: 0.6923, Val Acc: 70.5882\n",
      "Val Loss: 0.7656708061695099, Val Acc: 50.0\n",
      "Epoch 29201/50000, Tr Loss: 0.3868, Tr Acc: 78.7879, Val Loss: 0.6923, Val Acc: 70.5882\n",
      "Val Loss: 0.7490654587745667, Val Acc: 50.0\n",
      "Epoch 29301/50000, Tr Loss: 0.3776, Tr Acc: 69.6970, Val Loss: 0.6924, Val Acc: 67.6471\n",
      "Val Loss: 0.7661540806293488, Val Acc: 50.0\n",
      "Epoch 29401/50000, Tr Loss: 0.4192, Tr Acc: 68.1818, Val Loss: 0.6923, Val Acc: 67.6471\n",
      "Val Loss: 0.726228803396225, Val Acc: 50.0\n",
      "Epoch 29501/50000, Tr Loss: 0.3839, Tr Acc: 72.7273, Val Loss: 0.6924, Val Acc: 67.6471\n",
      "Val Loss: 0.7701437175273895, Val Acc: 50.0\n",
      "Epoch 29601/50000, Tr Loss: 0.4088, Tr Acc: 77.2727, Val Loss: 0.6924, Val Acc: 67.6471\n",
      "Val Loss: 0.7997285425662994, Val Acc: 50.0\n",
      "Epoch 29701/50000, Tr Loss: 0.3821, Tr Acc: 78.7879, Val Loss: 0.6926, Val Acc: 67.6471\n",
      "Val Loss: 0.7243426442146301, Val Acc: 50.0\n",
      "Epoch 29801/50000, Tr Loss: 0.3812, Tr Acc: 74.2424, Val Loss: 0.6924, Val Acc: 67.6471\n",
      "Val Loss: 0.730998307466507, Val Acc: 50.0\n",
      "Epoch 29901/50000, Tr Loss: 0.3628, Tr Acc: 80.3030, Val Loss: 0.6925, Val Acc: 67.6471\n",
      "Val Loss: 0.7617331147193909, Val Acc: 50.0\n",
      "Epoch 30001/50000, Tr Loss: 0.3648, Tr Acc: 80.3030, Val Loss: 0.6925, Val Acc: 67.6471\n",
      "Val Loss: 0.7013171017169952, Val Acc: 50.0\n",
      "Epoch 30101/50000, Tr Loss: 0.3323, Tr Acc: 77.2727, Val Loss: 0.6926, Val Acc: 67.6471\n",
      "Val Loss: 0.7715599536895752, Val Acc: 50.0\n",
      "Epoch 30201/50000, Tr Loss: 0.3711, Tr Acc: 71.2121, Val Loss: 0.6928, Val Acc: 67.6471\n",
      "Val Loss: 0.778969794511795, Val Acc: 50.0\n",
      "Epoch 30301/50000, Tr Loss: 0.3823, Tr Acc: 77.2727, Val Loss: 0.6926, Val Acc: 67.6471\n",
      "Val Loss: 0.7512534856796265, Val Acc: 50.0\n",
      "Epoch 30401/50000, Tr Loss: 0.3767, Tr Acc: 72.7273, Val Loss: 0.6929, Val Acc: 67.6471\n",
      "Val Loss: 0.7578209042549133, Val Acc: 50.0\n",
      "Epoch 30501/50000, Tr Loss: 0.3719, Tr Acc: 74.2424, Val Loss: 0.6929, Val Acc: 67.6471\n",
      "Val Loss: 0.757358580827713, Val Acc: 50.0\n",
      "Epoch 30601/50000, Tr Loss: 0.3608, Tr Acc: 74.2424, Val Loss: 0.6929, Val Acc: 67.6471\n",
      "Val Loss: 0.7707033455371857, Val Acc: 50.0\n",
      "Epoch 30701/50000, Tr Loss: 0.3661, Tr Acc: 75.7576, Val Loss: 0.6928, Val Acc: 67.6471\n",
      "Val Loss: 0.7737046480178833, Val Acc: 50.0\n",
      "Epoch 30801/50000, Tr Loss: 0.4036, Tr Acc: 72.7273, Val Loss: 0.6930, Val Acc: 67.6471\n",
      "Val Loss: 0.7433595657348633, Val Acc: 50.0\n",
      "Epoch 30901/50000, Tr Loss: 0.3606, Tr Acc: 80.3030, Val Loss: 0.6931, Val Acc: 67.6471\n",
      "Val Loss: 0.7428523600101471, Val Acc: 50.0\n",
      "Epoch 31001/50000, Tr Loss: 0.4035, Tr Acc: 71.2121, Val Loss: 0.6931, Val Acc: 67.6471\n",
      "Val Loss: 0.7839030623435974, Val Acc: 50.0\n",
      "Epoch 31101/50000, Tr Loss: 0.3793, Tr Acc: 74.2424, Val Loss: 0.6930, Val Acc: 67.6471\n",
      "Val Loss: 0.7412599921226501, Val Acc: 50.0\n",
      "Epoch 31201/50000, Tr Loss: 0.3572, Tr Acc: 78.7879, Val Loss: 0.6931, Val Acc: 67.6471\n",
      "Val Loss: 0.7600567936897278, Val Acc: 50.0\n",
      "Epoch 31301/50000, Tr Loss: 0.3529, Tr Acc: 81.8182, Val Loss: 0.6932, Val Acc: 67.6471\n",
      "Val Loss: 0.728184849023819, Val Acc: 50.0\n",
      "Epoch 31401/50000, Tr Loss: 0.3633, Tr Acc: 81.8182, Val Loss: 0.6936, Val Acc: 67.6471\n",
      "Val Loss: 0.7447275519371033, Val Acc: 50.0\n",
      "Epoch 31501/50000, Tr Loss: 0.4048, Tr Acc: 75.7576, Val Loss: 0.6933, Val Acc: 67.6471\n",
      "Val Loss: 0.7247607409954071, Val Acc: 50.0\n",
      "Epoch 31601/50000, Tr Loss: 0.3633, Tr Acc: 86.3636, Val Loss: 0.6933, Val Acc: 67.6471\n",
      "Val Loss: 0.7780634462833405, Val Acc: 50.0\n",
      "Epoch 31701/50000, Tr Loss: 0.3693, Tr Acc: 77.2727, Val Loss: 0.6934, Val Acc: 67.6471\n",
      "Val Loss: 0.7597263753414154, Val Acc: 50.0\n",
      "Epoch 31801/50000, Tr Loss: 0.3476, Tr Acc: 78.7879, Val Loss: 0.6936, Val Acc: 67.6471\n",
      "Val Loss: 0.7753225266933441, Val Acc: 50.0\n",
      "Epoch 31901/50000, Tr Loss: 0.3692, Tr Acc: 74.2424, Val Loss: 0.6933, Val Acc: 67.6471\n",
      "Val Loss: 0.7894986867904663, Val Acc: 50.0\n",
      "Epoch 32001/50000, Tr Loss: 0.3810, Tr Acc: 77.2727, Val Loss: 0.6937, Val Acc: 67.6471\n",
      "Val Loss: 0.6995638310909271, Val Acc: 50.0\n",
      "Epoch 32101/50000, Tr Loss: 0.3441, Tr Acc: 78.7879, Val Loss: 0.6937, Val Acc: 67.6471\n",
      "Val Loss: 0.7213813066482544, Val Acc: 50.0\n",
      "Epoch 32201/50000, Tr Loss: 0.3870, Tr Acc: 66.6667, Val Loss: 0.6937, Val Acc: 67.6471\n",
      "Val Loss: 0.7656592428684235, Val Acc: 50.0\n",
      "Epoch 32301/50000, Tr Loss: 0.3424, Tr Acc: 84.8485, Val Loss: 0.6938, Val Acc: 67.6471\n",
      "Val Loss: 0.773773580789566, Val Acc: 50.0\n",
      "Epoch 32401/50000, Tr Loss: 0.3932, Tr Acc: 75.7576, Val Loss: 0.6939, Val Acc: 67.6471\n",
      "Val Loss: 0.7794947326183319, Val Acc: 50.0\n",
      "Epoch 32501/50000, Tr Loss: 0.3500, Tr Acc: 78.7879, Val Loss: 0.6937, Val Acc: 67.6471\n",
      "Val Loss: 0.7673670649528503, Val Acc: 50.0\n",
      "Epoch 32601/50000, Tr Loss: 0.3745, Tr Acc: 75.7576, Val Loss: 0.6938, Val Acc: 67.6471\n",
      "Val Loss: 0.762317568063736, Val Acc: 52.27272727272727\n",
      "Epoch 32701/50000, Tr Loss: 0.3574, Tr Acc: 72.7273, Val Loss: 0.6940, Val Acc: 67.6471\n",
      "Val Loss: 0.7497272491455078, Val Acc: 52.27272727272727\n",
      "Epoch 32801/50000, Tr Loss: 0.3482, Tr Acc: 75.7576, Val Loss: 0.6939, Val Acc: 67.6471\n",
      "Val Loss: 0.7314755320549011, Val Acc: 50.0\n",
      "Epoch 32901/50000, Tr Loss: 0.3772, Tr Acc: 77.2727, Val Loss: 0.6941, Val Acc: 67.6471\n",
      "Val Loss: 0.7855719029903412, Val Acc: 52.27272727272727\n",
      "Epoch 33001/50000, Tr Loss: 0.3386, Tr Acc: 83.3333, Val Loss: 0.6941, Val Acc: 67.6471\n",
      "Val Loss: 0.7424362599849701, Val Acc: 52.27272727272727\n",
      "Epoch 33101/50000, Tr Loss: 0.3251, Tr Acc: 83.3333, Val Loss: 0.6942, Val Acc: 67.6471\n",
      "Val Loss: 0.7894774377346039, Val Acc: 52.27272727272727\n",
      "Epoch 33201/50000, Tr Loss: 0.3337, Tr Acc: 89.3939, Val Loss: 0.6942, Val Acc: 67.6471\n",
      "Val Loss: 0.721998929977417, Val Acc: 52.27272727272727\n",
      "Epoch 33301/50000, Tr Loss: 0.3835, Tr Acc: 77.2727, Val Loss: 0.6945, Val Acc: 67.6471\n",
      "Val Loss: 0.7690580487251282, Val Acc: 52.27272727272727\n",
      "Epoch 33401/50000, Tr Loss: 0.3286, Tr Acc: 84.8485, Val Loss: 0.6943, Val Acc: 67.6471\n",
      "Val Loss: 0.7686280310153961, Val Acc: 52.27272727272727\n",
      "Epoch 33501/50000, Tr Loss: 0.3777, Tr Acc: 71.2121, Val Loss: 0.6947, Val Acc: 67.6471\n",
      "Val Loss: 0.767501711845398, Val Acc: 52.27272727272727\n",
      "Epoch 33601/50000, Tr Loss: 0.3732, Tr Acc: 72.7273, Val Loss: 0.6945, Val Acc: 67.6471\n",
      "Val Loss: 0.7548786997795105, Val Acc: 52.27272727272727\n",
      "Epoch 33701/50000, Tr Loss: 0.3931, Tr Acc: 78.7879, Val Loss: 0.6947, Val Acc: 67.6471\n",
      "Val Loss: 0.7015255689620972, Val Acc: 52.27272727272727\n",
      "Epoch 33801/50000, Tr Loss: 0.3492, Tr Acc: 81.8182, Val Loss: 0.6947, Val Acc: 67.6471\n",
      "Val Loss: 0.7794250845909119, Val Acc: 52.27272727272727\n",
      "Epoch 33901/50000, Tr Loss: 0.3839, Tr Acc: 81.8182, Val Loss: 0.6949, Val Acc: 67.6471\n",
      "Val Loss: 0.7788209319114685, Val Acc: 52.27272727272727\n",
      "Epoch 34001/50000, Tr Loss: 0.3574, Tr Acc: 77.2727, Val Loss: 0.6947, Val Acc: 67.6471\n",
      "Val Loss: 0.7552625834941864, Val Acc: 52.27272727272727\n",
      "Epoch 34101/50000, Tr Loss: 0.3642, Tr Acc: 71.2121, Val Loss: 0.6948, Val Acc: 67.6471\n",
      "Val Loss: 0.7658514976501465, Val Acc: 52.27272727272727\n",
      "Epoch 34201/50000, Tr Loss: 0.3670, Tr Acc: 78.7879, Val Loss: 0.6951, Val Acc: 67.6471\n",
      "Val Loss: 0.7516457736492157, Val Acc: 52.27272727272727\n",
      "Epoch 34301/50000, Tr Loss: 0.3456, Tr Acc: 81.8182, Val Loss: 0.6949, Val Acc: 67.6471\n",
      "Val Loss: 0.749546468257904, Val Acc: 52.27272727272727\n",
      "Epoch 34401/50000, Tr Loss: 0.3239, Tr Acc: 84.8485, Val Loss: 0.6950, Val Acc: 67.6471\n",
      "Val Loss: 0.7465169429779053, Val Acc: 52.27272727272727\n",
      "Epoch 34501/50000, Tr Loss: 0.3345, Tr Acc: 77.2727, Val Loss: 0.6951, Val Acc: 64.7059\n",
      "Val Loss: 0.7695673406124115, Val Acc: 52.27272727272727\n",
      "Epoch 34601/50000, Tr Loss: 0.3543, Tr Acc: 78.7879, Val Loss: 0.6951, Val Acc: 64.7059\n",
      "Val Loss: 0.7883501648902893, Val Acc: 52.27272727272727\n",
      "Epoch 34701/50000, Tr Loss: 0.3197, Tr Acc: 80.3030, Val Loss: 0.6952, Val Acc: 64.7059\n",
      "Val Loss: 0.7634584009647369, Val Acc: 52.27272727272727\n",
      "Epoch 34801/50000, Tr Loss: 0.3659, Tr Acc: 83.3333, Val Loss: 0.6952, Val Acc: 67.6471\n",
      "Val Loss: 0.7888225317001343, Val Acc: 52.27272727272727\n",
      "Epoch 34901/50000, Tr Loss: 0.3507, Tr Acc: 78.7879, Val Loss: 0.6954, Val Acc: 67.6471\n",
      "Val Loss: 0.7838262617588043, Val Acc: 52.27272727272727\n",
      "Epoch 35001/50000, Tr Loss: 0.3600, Tr Acc: 75.7576, Val Loss: 0.6952, Val Acc: 67.6471\n",
      "Val Loss: 0.6999810338020325, Val Acc: 52.27272727272727\n",
      "Epoch 35101/50000, Tr Loss: 0.3294, Tr Acc: 86.3636, Val Loss: 0.6954, Val Acc: 67.6471\n",
      "Val Loss: 0.7701652646064758, Val Acc: 52.27272727272727\n",
      "Epoch 35201/50000, Tr Loss: 0.3573, Tr Acc: 75.7576, Val Loss: 0.6955, Val Acc: 67.6471\n",
      "Val Loss: 0.7842089235782623, Val Acc: 52.27272727272727\n",
      "Epoch 35301/50000, Tr Loss: 0.3683, Tr Acc: 77.2727, Val Loss: 0.6956, Val Acc: 67.6471\n",
      "Val Loss: 0.7549031674861908, Val Acc: 52.27272727272727\n",
      "Epoch 35401/50000, Tr Loss: 0.3388, Tr Acc: 78.7879, Val Loss: 0.6955, Val Acc: 67.6471\n",
      "Val Loss: 0.7752605974674225, Val Acc: 52.27272727272727\n",
      "Epoch 35501/50000, Tr Loss: 0.3515, Tr Acc: 80.3030, Val Loss: 0.6959, Val Acc: 67.6471\n",
      "Val Loss: 0.7809861898422241, Val Acc: 52.27272727272727\n",
      "Epoch 35601/50000, Tr Loss: 0.3424, Tr Acc: 78.7879, Val Loss: 0.6958, Val Acc: 67.6471\n",
      "Val Loss: 0.7581603825092316, Val Acc: 52.27272727272727\n",
      "Epoch 35701/50000, Tr Loss: 0.3355, Tr Acc: 80.3030, Val Loss: 0.6963, Val Acc: 67.6471\n",
      "Val Loss: 0.7613368332386017, Val Acc: 52.27272727272727\n",
      "Epoch 35801/50000, Tr Loss: 0.3429, Tr Acc: 80.3030, Val Loss: 0.6960, Val Acc: 67.6471\n",
      "Val Loss: 0.7415117621421814, Val Acc: 52.27272727272727\n",
      "Epoch 35901/50000, Tr Loss: 0.3527, Tr Acc: 84.8485, Val Loss: 0.6962, Val Acc: 67.6471\n",
      "Val Loss: 0.7602330446243286, Val Acc: 52.27272727272727\n",
      "Epoch 36001/50000, Tr Loss: 0.3318, Tr Acc: 78.7879, Val Loss: 0.6960, Val Acc: 67.6471\n",
      "Val Loss: 0.7937010824680328, Val Acc: 52.27272727272727\n",
      "Epoch 36101/50000, Tr Loss: 0.3131, Tr Acc: 81.8182, Val Loss: 0.6961, Val Acc: 67.6471\n",
      "Val Loss: 0.7810908555984497, Val Acc: 52.27272727272727\n",
      "Epoch 36201/50000, Tr Loss: 0.3478, Tr Acc: 81.8182, Val Loss: 0.6961, Val Acc: 67.6471\n",
      "Val Loss: 0.7978824377059937, Val Acc: 52.27272727272727\n",
      "Epoch 36301/50000, Tr Loss: 0.3483, Tr Acc: 80.3030, Val Loss: 0.6962, Val Acc: 67.6471\n",
      "Val Loss: 0.7731381058692932, Val Acc: 52.27272727272727\n",
      "Epoch 36401/50000, Tr Loss: 0.3481, Tr Acc: 81.8182, Val Loss: 0.6961, Val Acc: 67.6471\n",
      "Val Loss: 0.7842342555522919, Val Acc: 52.27272727272727\n",
      "Epoch 36501/50000, Tr Loss: 0.3345, Tr Acc: 77.2727, Val Loss: 0.6964, Val Acc: 67.6471\n",
      "Val Loss: 0.7478183209896088, Val Acc: 52.27272727272727\n",
      "Epoch 36601/50000, Tr Loss: 0.3366, Tr Acc: 78.7879, Val Loss: 0.6966, Val Acc: 67.6471\n",
      "Val Loss: 0.7957425117492676, Val Acc: 52.27272727272727\n",
      "Epoch 36701/50000, Tr Loss: 0.3191, Tr Acc: 81.8182, Val Loss: 0.6964, Val Acc: 67.6471\n",
      "Val Loss: 0.7839716374874115, Val Acc: 52.27272727272727\n",
      "Epoch 36801/50000, Tr Loss: 0.3297, Tr Acc: 81.8182, Val Loss: 0.6967, Val Acc: 64.7059\n",
      "Val Loss: 0.8016275763511658, Val Acc: 52.27272727272727\n",
      "Epoch 36901/50000, Tr Loss: 0.3543, Tr Acc: 80.3030, Val Loss: 0.6969, Val Acc: 64.7059\n",
      "Val Loss: 0.8129765391349792, Val Acc: 50.0\n",
      "Epoch 37001/50000, Tr Loss: 0.3243, Tr Acc: 84.8485, Val Loss: 0.6968, Val Acc: 64.7059\n",
      "Val Loss: 0.7439651787281036, Val Acc: 50.0\n",
      "Epoch 37101/50000, Tr Loss: 0.3433, Tr Acc: 83.3333, Val Loss: 0.6969, Val Acc: 64.7059\n",
      "Val Loss: 0.744387149810791, Val Acc: 50.0\n",
      "Epoch 37201/50000, Tr Loss: 0.3314, Tr Acc: 78.7879, Val Loss: 0.6972, Val Acc: 64.7059\n",
      "Val Loss: 0.7880725860595703, Val Acc: 50.0\n",
      "Epoch 37301/50000, Tr Loss: 0.3460, Tr Acc: 78.7879, Val Loss: 0.6971, Val Acc: 64.7059\n",
      "Val Loss: 0.7706128656864166, Val Acc: 47.72727272727273\n",
      "Epoch 37401/50000, Tr Loss: 0.3297, Tr Acc: 83.3333, Val Loss: 0.6973, Val Acc: 64.7059\n",
      "Val Loss: 0.8052581548690796, Val Acc: 47.72727272727273\n",
      "Epoch 37501/50000, Tr Loss: 0.3204, Tr Acc: 80.3030, Val Loss: 0.6973, Val Acc: 64.7059\n",
      "Val Loss: 0.7717521488666534, Val Acc: 47.72727272727273\n",
      "Epoch 37601/50000, Tr Loss: 0.3697, Tr Acc: 75.7576, Val Loss: 0.6974, Val Acc: 64.7059\n",
      "Val Loss: 0.7642149329185486, Val Acc: 47.72727272727273\n",
      "Epoch 37701/50000, Tr Loss: 0.3719, Tr Acc: 77.2727, Val Loss: 0.6974, Val Acc: 64.7059\n",
      "Val Loss: 0.7508189082145691, Val Acc: 47.72727272727273\n",
      "Epoch 37801/50000, Tr Loss: 0.3440, Tr Acc: 78.7879, Val Loss: 0.6973, Val Acc: 64.7059\n",
      "Val Loss: 0.7781460583209991, Val Acc: 47.72727272727273\n",
      "Epoch 37901/50000, Tr Loss: 0.3157, Tr Acc: 83.3333, Val Loss: 0.6975, Val Acc: 64.7059\n",
      "Val Loss: 0.7119481563568115, Val Acc: 47.72727272727273\n",
      "Epoch 38001/50000, Tr Loss: 0.3284, Tr Acc: 80.3030, Val Loss: 0.6974, Val Acc: 64.7059\n",
      "Val Loss: 0.733775794506073, Val Acc: 47.72727272727273\n",
      "Epoch 38101/50000, Tr Loss: 0.3340, Tr Acc: 78.7879, Val Loss: 0.6975, Val Acc: 64.7059\n",
      "Val Loss: 0.798587828874588, Val Acc: 47.72727272727273\n",
      "Epoch 38201/50000, Tr Loss: 0.3396, Tr Acc: 80.3030, Val Loss: 0.6977, Val Acc: 64.7059\n",
      "Val Loss: 0.7352672517299652, Val Acc: 47.72727272727273\n",
      "Epoch 38301/50000, Tr Loss: 0.3221, Tr Acc: 83.3333, Val Loss: 0.6980, Val Acc: 64.7059\n",
      "Val Loss: 0.7265519797801971, Val Acc: 47.72727272727273\n",
      "Epoch 38401/50000, Tr Loss: 0.3071, Tr Acc: 83.3333, Val Loss: 0.6977, Val Acc: 64.7059\n",
      "Val Loss: 0.7725621163845062, Val Acc: 47.72727272727273\n",
      "Epoch 38501/50000, Tr Loss: 0.3089, Tr Acc: 83.3333, Val Loss: 0.6980, Val Acc: 64.7059\n",
      "Val Loss: 0.7793734669685364, Val Acc: 45.45454545454545\n",
      "Epoch 38601/50000, Tr Loss: 0.3346, Tr Acc: 80.3030, Val Loss: 0.6982, Val Acc: 64.7059\n",
      "Val Loss: 0.7570294439792633, Val Acc: 45.45454545454545\n",
      "Epoch 38701/50000, Tr Loss: 0.3186, Tr Acc: 81.8182, Val Loss: 0.6981, Val Acc: 64.7059\n",
      "Val Loss: 0.7298156321048737, Val Acc: 45.45454545454545\n",
      "Epoch 38801/50000, Tr Loss: 0.3200, Tr Acc: 78.7879, Val Loss: 0.6982, Val Acc: 64.7059\n",
      "Val Loss: 0.7639918625354767, Val Acc: 45.45454545454545\n",
      "Epoch 38901/50000, Tr Loss: 0.3431, Tr Acc: 72.7273, Val Loss: 0.6983, Val Acc: 64.7059\n",
      "Val Loss: 0.7243697643280029, Val Acc: 45.45454545454545\n",
      "Epoch 39001/50000, Tr Loss: 0.3269, Tr Acc: 80.3030, Val Loss: 0.6982, Val Acc: 64.7059\n",
      "Val Loss: 0.7513597309589386, Val Acc: 45.45454545454545\n",
      "Epoch 39101/50000, Tr Loss: 0.3269, Tr Acc: 86.3636, Val Loss: 0.6984, Val Acc: 64.7059\n",
      "Val Loss: 0.7314285039901733, Val Acc: 45.45454545454545\n",
      "Epoch 39201/50000, Tr Loss: 0.2936, Tr Acc: 81.8182, Val Loss: 0.6985, Val Acc: 64.7059\n",
      "Val Loss: 0.7418256402015686, Val Acc: 45.45454545454545\n",
      "Epoch 39301/50000, Tr Loss: 0.3213, Tr Acc: 81.8182, Val Loss: 0.6987, Val Acc: 64.7059\n",
      "Val Loss: 0.7991729974746704, Val Acc: 45.45454545454545\n",
      "Epoch 39401/50000, Tr Loss: 0.3076, Tr Acc: 89.3939, Val Loss: 0.6986, Val Acc: 64.7059\n",
      "Val Loss: 0.7806749641895294, Val Acc: 45.45454545454545\n",
      "Epoch 39501/50000, Tr Loss: 0.3436, Tr Acc: 81.8182, Val Loss: 0.6986, Val Acc: 64.7059\n",
      "Val Loss: 0.7717537581920624, Val Acc: 45.45454545454545\n",
      "Epoch 39601/50000, Tr Loss: 0.3148, Tr Acc: 81.8182, Val Loss: 0.6986, Val Acc: 64.7059\n",
      "Val Loss: 0.801898717880249, Val Acc: 45.45454545454545\n",
      "Epoch 39701/50000, Tr Loss: 0.2726, Tr Acc: 87.8788, Val Loss: 0.6989, Val Acc: 64.7059\n",
      "Val Loss: 0.767113596200943, Val Acc: 45.45454545454545\n",
      "Epoch 39801/50000, Tr Loss: 0.3164, Tr Acc: 86.3636, Val Loss: 0.6988, Val Acc: 64.7059\n",
      "Val Loss: 0.7814780473709106, Val Acc: 45.45454545454545\n",
      "Epoch 39901/50000, Tr Loss: 0.3224, Tr Acc: 83.3333, Val Loss: 0.6989, Val Acc: 61.7647\n",
      "Val Loss: 0.8134346306324005, Val Acc: 45.45454545454545\n",
      "Epoch 40001/50000, Tr Loss: 0.3525, Tr Acc: 80.3030, Val Loss: 0.6992, Val Acc: 61.7647\n",
      "Val Loss: 0.7500275373458862, Val Acc: 45.45454545454545\n",
      "Epoch 40101/50000, Tr Loss: 0.3398, Tr Acc: 78.7879, Val Loss: 0.6990, Val Acc: 61.7647\n",
      "Val Loss: 0.7822738885879517, Val Acc: 45.45454545454545\n",
      "Epoch 40201/50000, Tr Loss: 0.3150, Tr Acc: 78.7879, Val Loss: 0.6994, Val Acc: 61.7647\n",
      "Val Loss: 0.7718498110771179, Val Acc: 45.45454545454545\n",
      "Epoch 40301/50000, Tr Loss: 0.3297, Tr Acc: 84.8485, Val Loss: 0.6995, Val Acc: 61.7647\n",
      "Val Loss: 0.8036667704582214, Val Acc: 45.45454545454545\n",
      "Epoch 40401/50000, Tr Loss: 0.3136, Tr Acc: 89.3939, Val Loss: 0.6993, Val Acc: 61.7647\n",
      "Val Loss: 0.7715206146240234, Val Acc: 45.45454545454545\n",
      "Epoch 40501/50000, Tr Loss: 0.3060, Tr Acc: 86.3636, Val Loss: 0.6993, Val Acc: 61.7647\n",
      "Val Loss: 0.7733394503593445, Val Acc: 45.45454545454545\n",
      "Epoch 40601/50000, Tr Loss: 0.3304, Tr Acc: 84.8485, Val Loss: 0.6994, Val Acc: 61.7647\n",
      "Val Loss: 0.8128753006458282, Val Acc: 45.45454545454545\n",
      "Epoch 40701/50000, Tr Loss: 0.3059, Tr Acc: 83.3333, Val Loss: 0.6997, Val Acc: 61.7647\n",
      "Val Loss: 0.7932064831256866, Val Acc: 45.45454545454545\n",
      "Epoch 40801/50000, Tr Loss: 0.3257, Tr Acc: 78.7879, Val Loss: 0.6997, Val Acc: 61.7647\n",
      "Val Loss: 0.7449555993080139, Val Acc: 45.45454545454545\n",
      "Epoch 40901/50000, Tr Loss: 0.3133, Tr Acc: 80.3030, Val Loss: 0.7000, Val Acc: 61.7647\n",
      "Val Loss: 0.7659324109554291, Val Acc: 45.45454545454545\n",
      "Epoch 41001/50000, Tr Loss: 0.3225, Tr Acc: 83.3333, Val Loss: 0.6999, Val Acc: 61.7647\n",
      "Val Loss: 0.7982899844646454, Val Acc: 45.45454545454545\n",
      "Epoch 41101/50000, Tr Loss: 0.2926, Tr Acc: 89.3939, Val Loss: 0.7000, Val Acc: 61.7647\n",
      "Val Loss: 0.746350884437561, Val Acc: 43.18181818181818\n",
      "Epoch 41201/50000, Tr Loss: 0.3332, Tr Acc: 72.7273, Val Loss: 0.7002, Val Acc: 61.7647\n",
      "Val Loss: 0.7267841696739197, Val Acc: 43.18181818181818\n",
      "Epoch 41301/50000, Tr Loss: 0.3256, Tr Acc: 81.8182, Val Loss: 0.7005, Val Acc: 61.7647\n",
      "Val Loss: 0.800329178571701, Val Acc: 43.18181818181818\n",
      "Epoch 41401/50000, Tr Loss: 0.3741, Tr Acc: 83.3333, Val Loss: 0.7003, Val Acc: 61.7647\n",
      "Val Loss: 0.7588116824626923, Val Acc: 43.18181818181818\n",
      "Epoch 41501/50000, Tr Loss: 0.3122, Tr Acc: 83.3333, Val Loss: 0.7005, Val Acc: 61.7647\n",
      "Val Loss: 0.8052304089069366, Val Acc: 43.18181818181818\n",
      "Epoch 41601/50000, Tr Loss: 0.3303, Tr Acc: 87.8788, Val Loss: 0.7006, Val Acc: 61.7647\n",
      "Val Loss: 0.7382832467556, Val Acc: 43.18181818181818\n",
      "Epoch 41701/50000, Tr Loss: 0.3244, Tr Acc: 87.8788, Val Loss: 0.7005, Val Acc: 61.7647\n",
      "Val Loss: 0.7751220762729645, Val Acc: 43.18181818181818\n",
      "Epoch 41801/50000, Tr Loss: 0.3158, Tr Acc: 81.8182, Val Loss: 0.7003, Val Acc: 61.7647\n",
      "Val Loss: 0.7723134756088257, Val Acc: 43.18181818181818\n",
      "Epoch 41901/50000, Tr Loss: 0.3073, Tr Acc: 84.8485, Val Loss: 0.7009, Val Acc: 61.7647\n",
      "Val Loss: 0.8062348067760468, Val Acc: 43.18181818181818\n",
      "Epoch 42001/50000, Tr Loss: 0.3077, Tr Acc: 83.3333, Val Loss: 0.7007, Val Acc: 61.7647\n",
      "Val Loss: 0.7122816443443298, Val Acc: 43.18181818181818\n",
      "Epoch 42101/50000, Tr Loss: 0.2981, Tr Acc: 84.8485, Val Loss: 0.7009, Val Acc: 61.7647\n",
      "Val Loss: 0.7701870799064636, Val Acc: 43.18181818181818\n",
      "Epoch 42201/50000, Tr Loss: 0.3345, Tr Acc: 77.2727, Val Loss: 0.7011, Val Acc: 61.7647\n",
      "Val Loss: 0.7562800943851471, Val Acc: 43.18181818181818\n",
      "Epoch 42301/50000, Tr Loss: 0.3105, Tr Acc: 83.3333, Val Loss: 0.7011, Val Acc: 61.7647\n",
      "Val Loss: 0.8017099797725677, Val Acc: 43.18181818181818\n",
      "Epoch 42401/50000, Tr Loss: 0.3053, Tr Acc: 83.3333, Val Loss: 0.7011, Val Acc: 61.7647\n",
      "Val Loss: 0.8170695900917053, Val Acc: 43.18181818181818\n",
      "Epoch 42501/50000, Tr Loss: 0.3335, Tr Acc: 75.7576, Val Loss: 0.7012, Val Acc: 61.7647\n",
      "Val Loss: 0.7793140113353729, Val Acc: 43.18181818181818\n",
      "Epoch 42601/50000, Tr Loss: 0.3318, Tr Acc: 83.3333, Val Loss: 0.7011, Val Acc: 61.7647\n",
      "Val Loss: 0.7560887932777405, Val Acc: 43.18181818181818\n",
      "Epoch 42701/50000, Tr Loss: 0.2985, Tr Acc: 86.3636, Val Loss: 0.7014, Val Acc: 61.7647\n",
      "Val Loss: 0.7805131375789642, Val Acc: 43.18181818181818\n",
      "Epoch 42801/50000, Tr Loss: 0.3275, Tr Acc: 77.2727, Val Loss: 0.7016, Val Acc: 61.7647\n",
      "Val Loss: 0.7882751524448395, Val Acc: 43.18181818181818\n",
      "Epoch 42901/50000, Tr Loss: 0.3251, Tr Acc: 77.2727, Val Loss: 0.7016, Val Acc: 61.7647\n",
      "Val Loss: 0.7769240736961365, Val Acc: 43.18181818181818\n",
      "Epoch 43001/50000, Tr Loss: 0.2874, Tr Acc: 83.3333, Val Loss: 0.7017, Val Acc: 61.7647\n",
      "Val Loss: 0.754971444606781, Val Acc: 43.18181818181818\n",
      "Epoch 43101/50000, Tr Loss: 0.3089, Tr Acc: 87.8788, Val Loss: 0.7018, Val Acc: 61.7647\n",
      "Val Loss: 0.7799495756626129, Val Acc: 43.18181818181818\n",
      "Epoch 43201/50000, Tr Loss: 0.3020, Tr Acc: 87.8788, Val Loss: 0.7020, Val Acc: 61.7647\n",
      "Val Loss: 0.7748781144618988, Val Acc: 43.18181818181818\n",
      "Epoch 43301/50000, Tr Loss: 0.2852, Tr Acc: 87.8788, Val Loss: 0.7021, Val Acc: 61.7647\n",
      "Val Loss: 0.8094201385974884, Val Acc: 43.18181818181818\n",
      "Epoch 43401/50000, Tr Loss: 0.2925, Tr Acc: 84.8485, Val Loss: 0.7021, Val Acc: 61.7647\n",
      "Val Loss: 0.7846699655056, Val Acc: 43.18181818181818\n",
      "Epoch 43501/50000, Tr Loss: 0.2812, Tr Acc: 83.3333, Val Loss: 0.7023, Val Acc: 61.7647\n",
      "Val Loss: 0.7812240719795227, Val Acc: 43.18181818181818\n",
      "Epoch 43601/50000, Tr Loss: 0.2889, Tr Acc: 87.8788, Val Loss: 0.7022, Val Acc: 61.7647\n",
      "Val Loss: 0.7697411477565765, Val Acc: 43.18181818181818\n",
      "Epoch 43701/50000, Tr Loss: 0.3122, Tr Acc: 84.8485, Val Loss: 0.7020, Val Acc: 61.7647\n",
      "Val Loss: 0.7499459385871887, Val Acc: 43.18181818181818\n",
      "Epoch 43801/50000, Tr Loss: 0.3088, Tr Acc: 83.3333, Val Loss: 0.7025, Val Acc: 61.7647\n",
      "Val Loss: 0.7637138068675995, Val Acc: 43.18181818181818\n",
      "Epoch 43901/50000, Tr Loss: 0.3116, Tr Acc: 89.3939, Val Loss: 0.7025, Val Acc: 61.7647\n",
      "Val Loss: 0.7828424572944641, Val Acc: 43.18181818181818\n",
      "Epoch 44001/50000, Tr Loss: 0.3058, Tr Acc: 83.3333, Val Loss: 0.7024, Val Acc: 61.7647\n",
      "Val Loss: 0.7795031368732452, Val Acc: 43.18181818181818\n",
      "Epoch 44101/50000, Tr Loss: 0.3076, Tr Acc: 81.8182, Val Loss: 0.7026, Val Acc: 61.7647\n",
      "Val Loss: 0.7670277059078217, Val Acc: 43.18181818181818\n",
      "Epoch 44201/50000, Tr Loss: 0.2959, Tr Acc: 83.3333, Val Loss: 0.7028, Val Acc: 61.7647\n",
      "Val Loss: 0.7976105809211731, Val Acc: 43.18181818181818\n",
      "Epoch 44301/50000, Tr Loss: 0.2971, Tr Acc: 84.8485, Val Loss: 0.7029, Val Acc: 61.7647\n",
      "Val Loss: 0.7934368252754211, Val Acc: 43.18181818181818\n",
      "Epoch 44401/50000, Tr Loss: 0.2747, Tr Acc: 92.4242, Val Loss: 0.7027, Val Acc: 61.7647\n",
      "Val Loss: 0.7676420211791992, Val Acc: 43.18181818181818\n",
      "Epoch 44501/50000, Tr Loss: 0.2927, Tr Acc: 84.8485, Val Loss: 0.7031, Val Acc: 61.7647\n",
      "Val Loss: 0.765826404094696, Val Acc: 43.18181818181818\n",
      "Epoch 44601/50000, Tr Loss: 0.3221, Tr Acc: 78.7879, Val Loss: 0.7030, Val Acc: 61.7647\n",
      "Val Loss: 0.7345671653747559, Val Acc: 43.18181818181818\n",
      "Epoch 44701/50000, Tr Loss: 0.2981, Tr Acc: 87.8788, Val Loss: 0.7033, Val Acc: 61.7647\n",
      "Val Loss: 0.816834419965744, Val Acc: 43.18181818181818\n",
      "Epoch 44801/50000, Tr Loss: 0.3174, Tr Acc: 78.7879, Val Loss: 0.7032, Val Acc: 61.7647\n",
      "Val Loss: 0.7520636916160583, Val Acc: 45.45454545454545\n",
      "Epoch 44901/50000, Tr Loss: 0.3191, Tr Acc: 83.3333, Val Loss: 0.7033, Val Acc: 61.7647\n",
      "Val Loss: 0.8112416565418243, Val Acc: 45.45454545454545\n",
      "Epoch 45001/50000, Tr Loss: 0.2709, Tr Acc: 84.8485, Val Loss: 0.7035, Val Acc: 61.7647\n",
      "Val Loss: 0.8082850277423859, Val Acc: 45.45454545454545\n",
      "Epoch 45101/50000, Tr Loss: 0.3035, Tr Acc: 81.8182, Val Loss: 0.7037, Val Acc: 61.7647\n",
      "Val Loss: 0.7966766655445099, Val Acc: 43.18181818181818\n",
      "Epoch 45201/50000, Tr Loss: 0.3281, Tr Acc: 72.7273, Val Loss: 0.7037, Val Acc: 61.7647\n",
      "Val Loss: 0.7837924361228943, Val Acc: 45.45454545454545\n",
      "Epoch 45301/50000, Tr Loss: 0.2776, Tr Acc: 87.8788, Val Loss: 0.7039, Val Acc: 61.7647\n",
      "Val Loss: 0.7889372110366821, Val Acc: 43.18181818181818\n",
      "Epoch 45401/50000, Tr Loss: 0.2846, Tr Acc: 86.3636, Val Loss: 0.7039, Val Acc: 61.7647\n",
      "Val Loss: 0.8650085926055908, Val Acc: 43.18181818181818\n",
      "Epoch 45501/50000, Tr Loss: 0.3018, Tr Acc: 84.8485, Val Loss: 0.7039, Val Acc: 61.7647\n",
      "Val Loss: 0.8218702971935272, Val Acc: 40.909090909090914\n",
      "Epoch 45601/50000, Tr Loss: 0.3417, Tr Acc: 78.7879, Val Loss: 0.7041, Val Acc: 61.7647\n",
      "Val Loss: 0.7513822019100189, Val Acc: 43.18181818181818\n",
      "Epoch 45701/50000, Tr Loss: 0.2902, Tr Acc: 83.3333, Val Loss: 0.7045, Val Acc: 61.7647\n",
      "Val Loss: 0.762298971414566, Val Acc: 43.18181818181818\n",
      "Epoch 45801/50000, Tr Loss: 0.2672, Tr Acc: 87.8788, Val Loss: 0.7043, Val Acc: 61.7647\n",
      "Val Loss: 0.7928100824356079, Val Acc: 40.909090909090914\n",
      "Epoch 45901/50000, Tr Loss: 0.2814, Tr Acc: 86.3636, Val Loss: 0.7044, Val Acc: 61.7647\n",
      "Val Loss: 0.8090083301067352, Val Acc: 40.909090909090914\n",
      "Epoch 46001/50000, Tr Loss: 0.3051, Tr Acc: 84.8485, Val Loss: 0.7048, Val Acc: 61.7647\n",
      "Val Loss: 0.7761098444461823, Val Acc: 40.909090909090914\n",
      "Epoch 46101/50000, Tr Loss: 0.3095, Tr Acc: 80.3030, Val Loss: 0.7046, Val Acc: 61.7647\n",
      "Val Loss: 0.7433049380779266, Val Acc: 40.909090909090914\n",
      "Epoch 46201/50000, Tr Loss: 0.2918, Tr Acc: 84.8485, Val Loss: 0.7049, Val Acc: 61.7647\n",
      "Val Loss: 0.7914508283138275, Val Acc: 40.909090909090914\n",
      "Epoch 46301/50000, Tr Loss: 0.2884, Tr Acc: 86.3636, Val Loss: 0.7050, Val Acc: 61.7647\n",
      "Val Loss: 0.7312885820865631, Val Acc: 40.909090909090914\n",
      "Epoch 46401/50000, Tr Loss: 0.2745, Tr Acc: 86.3636, Val Loss: 0.7048, Val Acc: 61.7647\n",
      "Val Loss: 0.7855287492275238, Val Acc: 40.909090909090914\n",
      "Epoch 46501/50000, Tr Loss: 0.2906, Tr Acc: 84.8485, Val Loss: 0.7049, Val Acc: 61.7647\n",
      "Val Loss: 0.8022235333919525, Val Acc: 40.909090909090914\n",
      "Epoch 46601/50000, Tr Loss: 0.2836, Tr Acc: 87.8788, Val Loss: 0.7052, Val Acc: 61.7647\n",
      "Val Loss: 0.7640046775341034, Val Acc: 40.909090909090914\n",
      "Epoch 46701/50000, Tr Loss: 0.3178, Tr Acc: 80.3030, Val Loss: 0.7053, Val Acc: 61.7647\n",
      "Val Loss: 0.7768436074256897, Val Acc: 40.909090909090914\n",
      "Epoch 46801/50000, Tr Loss: 0.2795, Tr Acc: 89.3939, Val Loss: 0.7054, Val Acc: 61.7647\n",
      "Val Loss: 0.8157995343208313, Val Acc: 40.909090909090914\n",
      "Epoch 46901/50000, Tr Loss: 0.3081, Tr Acc: 87.8788, Val Loss: 0.7053, Val Acc: 61.7647\n",
      "Val Loss: 0.7645033299922943, Val Acc: 40.909090909090914\n",
      "Epoch 47001/50000, Tr Loss: 0.2812, Tr Acc: 86.3636, Val Loss: 0.7056, Val Acc: 61.7647\n",
      "Val Loss: 0.7965871691703796, Val Acc: 40.909090909090914\n",
      "Epoch 47101/50000, Tr Loss: 0.2859, Tr Acc: 84.8485, Val Loss: 0.7057, Val Acc: 61.7647\n",
      "Val Loss: 0.8053602278232574, Val Acc: 40.909090909090914\n",
      "Epoch 47201/50000, Tr Loss: 0.2713, Tr Acc: 86.3636, Val Loss: 0.7058, Val Acc: 61.7647\n",
      "Val Loss: 0.7882120311260223, Val Acc: 40.909090909090914\n",
      "Epoch 47301/50000, Tr Loss: 0.2921, Tr Acc: 87.8788, Val Loss: 0.7059, Val Acc: 61.7647\n",
      "Val Loss: 0.7900728285312653, Val Acc: 40.909090909090914\n",
      "Epoch 47401/50000, Tr Loss: 0.2781, Tr Acc: 87.8788, Val Loss: 0.7063, Val Acc: 61.7647\n",
      "Val Loss: 0.7468219995498657, Val Acc: 40.909090909090914\n",
      "Epoch 47501/50000, Tr Loss: 0.2804, Tr Acc: 89.3939, Val Loss: 0.7062, Val Acc: 61.7647\n",
      "Val Loss: 0.8156548142433167, Val Acc: 40.909090909090914\n",
      "Epoch 47601/50000, Tr Loss: 0.2842, Tr Acc: 86.3636, Val Loss: 0.7063, Val Acc: 61.7647\n",
      "Val Loss: 0.8002131283283234, Val Acc: 40.909090909090914\n",
      "Epoch 47701/50000, Tr Loss: 0.2998, Tr Acc: 80.3030, Val Loss: 0.7065, Val Acc: 61.7647\n",
      "Val Loss: 0.7922298610210419, Val Acc: 40.909090909090914\n",
      "Epoch 47801/50000, Tr Loss: 0.3002, Tr Acc: 81.8182, Val Loss: 0.7065, Val Acc: 61.7647\n",
      "Val Loss: 0.7814935147762299, Val Acc: 40.909090909090914\n",
      "Epoch 47901/50000, Tr Loss: 0.2631, Tr Acc: 86.3636, Val Loss: 0.7068, Val Acc: 61.7647\n",
      "Val Loss: 0.8152629137039185, Val Acc: 40.909090909090914\n",
      "Epoch 48001/50000, Tr Loss: 0.2614, Tr Acc: 93.9394, Val Loss: 0.7071, Val Acc: 61.7647\n",
      "Val Loss: 0.8021128475666046, Val Acc: 40.909090909090914\n",
      "Epoch 48101/50000, Tr Loss: 0.2940, Tr Acc: 87.8788, Val Loss: 0.7068, Val Acc: 61.7647\n",
      "Val Loss: 0.7604972422122955, Val Acc: 40.909090909090914\n",
      "Epoch 48201/50000, Tr Loss: 0.2744, Tr Acc: 90.9091, Val Loss: 0.7069, Val Acc: 61.7647\n",
      "Val Loss: 0.8177123069763184, Val Acc: 40.909090909090914\n",
      "Epoch 48301/50000, Tr Loss: 0.2818, Tr Acc: 92.4242, Val Loss: 0.7069, Val Acc: 61.7647\n",
      "Val Loss: 0.8073741495609283, Val Acc: 40.909090909090914\n",
      "Epoch 48401/50000, Tr Loss: 0.2953, Tr Acc: 84.8485, Val Loss: 0.7071, Val Acc: 61.7647\n",
      "Val Loss: 0.8023980557918549, Val Acc: 40.909090909090914\n",
      "Epoch 48501/50000, Tr Loss: 0.2842, Tr Acc: 90.9091, Val Loss: 0.7070, Val Acc: 61.7647\n",
      "Val Loss: 0.8002692461013794, Val Acc: 40.909090909090914\n",
      "Epoch 48601/50000, Tr Loss: 0.2751, Tr Acc: 87.8788, Val Loss: 0.7074, Val Acc: 61.7647\n",
      "Val Loss: 0.8276135921478271, Val Acc: 40.909090909090914\n",
      "Epoch 48701/50000, Tr Loss: 0.2821, Tr Acc: 83.3333, Val Loss: 0.7074, Val Acc: 61.7647\n",
      "Val Loss: 0.7761005461215973, Val Acc: 40.909090909090914\n",
      "Epoch 48801/50000, Tr Loss: 0.2757, Tr Acc: 78.7879, Val Loss: 0.7074, Val Acc: 61.7647\n",
      "Val Loss: 0.7899556159973145, Val Acc: 40.909090909090914\n",
      "Epoch 48901/50000, Tr Loss: 0.3055, Tr Acc: 81.8182, Val Loss: 0.7076, Val Acc: 61.7647\n",
      "Val Loss: 0.7413407564163208, Val Acc: 38.63636363636363\n",
      "Epoch 49001/50000, Tr Loss: 0.2905, Tr Acc: 87.8788, Val Loss: 0.7078, Val Acc: 61.7647\n",
      "Val Loss: 0.7971408069133759, Val Acc: 38.63636363636363\n",
      "Epoch 49101/50000, Tr Loss: 0.2674, Tr Acc: 89.3939, Val Loss: 0.7077, Val Acc: 61.7647\n",
      "Val Loss: 0.7647842168807983, Val Acc: 38.63636363636363\n",
      "Epoch 49201/50000, Tr Loss: 0.2624, Tr Acc: 86.3636, Val Loss: 0.7081, Val Acc: 61.7647\n",
      "Val Loss: 0.8260471224784851, Val Acc: 38.63636363636363\n",
      "Epoch 49301/50000, Tr Loss: 0.3095, Tr Acc: 90.9091, Val Loss: 0.7082, Val Acc: 61.7647\n",
      "Val Loss: 0.836001068353653, Val Acc: 38.63636363636363\n",
      "Epoch 49401/50000, Tr Loss: 0.2466, Tr Acc: 92.4242, Val Loss: 0.7083, Val Acc: 61.7647\n",
      "Val Loss: 0.7956444025039673, Val Acc: 38.63636363636363\n",
      "Epoch 49501/50000, Tr Loss: 0.2731, Tr Acc: 84.8485, Val Loss: 0.7087, Val Acc: 61.7647\n",
      "Val Loss: 0.7983072102069855, Val Acc: 38.63636363636363\n",
      "Epoch 49601/50000, Tr Loss: 0.2816, Tr Acc: 92.4242, Val Loss: 0.7087, Val Acc: 61.7647\n",
      "Val Loss: 0.8360006809234619, Val Acc: 38.63636363636363\n",
      "Epoch 49701/50000, Tr Loss: 0.2670, Tr Acc: 90.9091, Val Loss: 0.7088, Val Acc: 58.8235\n",
      "Val Loss: 0.8086656332015991, Val Acc: 38.63636363636363\n",
      "Epoch 49801/50000, Tr Loss: 0.2649, Tr Acc: 87.8788, Val Loss: 0.7090, Val Acc: 58.8235\n",
      "Val Loss: 0.7907059192657471, Val Acc: 38.63636363636363\n",
      "Epoch 49901/50000, Tr Loss: 0.2803, Tr Acc: 86.3636, Val Loss: 0.7090, Val Acc: 61.7647\n",
      "Val Loss: 0.8343126177787781, Val Acc: 38.63636363636363\n",
      "Fold 2\n",
      "Epoch 1/50000, Tr Loss: 0.4573, Tr Acc: 64.1791, Val Loss: 0.2666, Val Acc: 96.9697\n",
      "Val Loss: 0.7415837347507477, Val Acc: 38.63636363636363\n",
      "Epoch 101/50000, Tr Loss: 0.4574, Tr Acc: 70.1493, Val Loss: 0.2723, Val Acc: 96.9697\n",
      "Val Loss: 0.7952437996864319, Val Acc: 38.63636363636363\n",
      "Epoch 201/50000, Tr Loss: 0.4688, Tr Acc: 65.6716, Val Loss: 0.2726, Val Acc: 96.9697\n",
      "Val Loss: 0.8383539319038391, Val Acc: 38.63636363636363\n",
      "Epoch 301/50000, Tr Loss: 0.4622, Tr Acc: 70.1493, Val Loss: 0.2731, Val Acc: 96.9697\n",
      "Val Loss: 0.7770201563835144, Val Acc: 38.63636363636363\n",
      "Epoch 401/50000, Tr Loss: 0.4303, Tr Acc: 74.6269, Val Loss: 0.2745, Val Acc: 96.9697\n",
      "Val Loss: 0.8007303178310394, Val Acc: 38.63636363636363\n",
      "Epoch 501/50000, Tr Loss: 0.4892, Tr Acc: 70.1493, Val Loss: 0.2749, Val Acc: 96.9697\n",
      "Val Loss: 0.7731280326843262, Val Acc: 40.909090909090914\n",
      "Epoch 601/50000, Tr Loss: 0.4258, Tr Acc: 74.6269, Val Loss: 0.2759, Val Acc: 96.9697\n",
      "Val Loss: 0.7533190250396729, Val Acc: 40.909090909090914\n",
      "Epoch 701/50000, Tr Loss: 0.4520, Tr Acc: 67.1642, Val Loss: 0.2767, Val Acc: 96.9697\n",
      "Val Loss: 0.7941196858882904, Val Acc: 40.909090909090914\n",
      "Epoch 801/50000, Tr Loss: 0.4213, Tr Acc: 74.6269, Val Loss: 0.2773, Val Acc: 96.9697\n",
      "Val Loss: 0.7831736505031586, Val Acc: 40.909090909090914\n",
      "Epoch 901/50000, Tr Loss: 0.4361, Tr Acc: 70.1493, Val Loss: 0.2783, Val Acc: 96.9697\n",
      "Val Loss: 0.8221990466117859, Val Acc: 40.909090909090914\n",
      "Epoch 1001/50000, Tr Loss: 0.4430, Tr Acc: 70.1493, Val Loss: 0.2789, Val Acc: 96.9697\n",
      "Val Loss: 0.8201847970485687, Val Acc: 40.909090909090914\n",
      "Epoch 1101/50000, Tr Loss: 0.4294, Tr Acc: 74.6269, Val Loss: 0.2798, Val Acc: 96.9697\n",
      "Val Loss: 0.7905541658401489, Val Acc: 40.909090909090914\n",
      "Epoch 1201/50000, Tr Loss: 0.4435, Tr Acc: 67.1642, Val Loss: 0.2806, Val Acc: 96.9697\n",
      "Val Loss: 0.8103125393390656, Val Acc: 40.909090909090914\n",
      "Epoch 1301/50000, Tr Loss: 0.4357, Tr Acc: 71.6418, Val Loss: 0.2814, Val Acc: 96.9697\n",
      "Val Loss: 0.7190262675285339, Val Acc: 40.909090909090914\n",
      "Epoch 1401/50000, Tr Loss: 0.4586, Tr Acc: 64.1791, Val Loss: 0.2827, Val Acc: 96.9697\n",
      "Val Loss: 0.8507446944713593, Val Acc: 40.909090909090914\n",
      "Epoch 1501/50000, Tr Loss: 0.4320, Tr Acc: 70.1493, Val Loss: 0.2839, Val Acc: 96.9697\n",
      "Val Loss: 0.7950117886066437, Val Acc: 40.909090909090914\n",
      "Epoch 1601/50000, Tr Loss: 0.4186, Tr Acc: 79.1045, Val Loss: 0.2847, Val Acc: 96.9697\n",
      "Val Loss: 0.8333548307418823, Val Acc: 40.909090909090914\n",
      "Epoch 1701/50000, Tr Loss: 0.4192, Tr Acc: 70.1493, Val Loss: 0.2850, Val Acc: 96.9697\n",
      "Val Loss: 0.8086584508419037, Val Acc: 40.909090909090914\n",
      "Epoch 1801/50000, Tr Loss: 0.4008, Tr Acc: 71.6418, Val Loss: 0.2862, Val Acc: 96.9697\n",
      "Val Loss: 0.8130775690078735, Val Acc: 40.909090909090914\n",
      "Epoch 1901/50000, Tr Loss: 0.4478, Tr Acc: 64.1791, Val Loss: 0.2870, Val Acc: 96.9697\n",
      "Val Loss: 0.7680596709251404, Val Acc: 40.909090909090914\n",
      "Epoch 2001/50000, Tr Loss: 0.4132, Tr Acc: 79.1045, Val Loss: 0.2877, Val Acc: 93.9394\n",
      "Val Loss: 0.8083584308624268, Val Acc: 40.909090909090914\n",
      "Epoch 2101/50000, Tr Loss: 0.4371, Tr Acc: 79.1045, Val Loss: 0.2884, Val Acc: 93.9394\n",
      "Val Loss: 0.7746443450450897, Val Acc: 40.909090909090914\n",
      "Epoch 2201/50000, Tr Loss: 0.4243, Tr Acc: 68.6567, Val Loss: 0.2894, Val Acc: 93.9394\n",
      "Val Loss: 0.7730028927326202, Val Acc: 40.909090909090914\n",
      "Epoch 2301/50000, Tr Loss: 0.4408, Tr Acc: 64.1791, Val Loss: 0.2901, Val Acc: 93.9394\n",
      "Val Loss: 0.8151263296604156, Val Acc: 40.909090909090914\n",
      "Epoch 2401/50000, Tr Loss: 0.4081, Tr Acc: 71.6418, Val Loss: 0.2908, Val Acc: 93.9394\n",
      "Val Loss: 0.7571569681167603, Val Acc: 40.909090909090914\n",
      "Epoch 2501/50000, Tr Loss: 0.4236, Tr Acc: 76.1194, Val Loss: 0.2921, Val Acc: 93.9394\n",
      "Val Loss: 0.8036338090896606, Val Acc: 40.909090909090914\n",
      "Epoch 2601/50000, Tr Loss: 0.4363, Tr Acc: 65.6716, Val Loss: 0.2927, Val Acc: 93.9394\n",
      "Val Loss: 0.8010281622409821, Val Acc: 40.909090909090914\n",
      "Epoch 2701/50000, Tr Loss: 0.4282, Tr Acc: 68.6567, Val Loss: 0.2937, Val Acc: 93.9394\n",
      "Val Loss: 0.8554717898368835, Val Acc: 40.909090909090914\n",
      "Epoch 2801/50000, Tr Loss: 0.4148, Tr Acc: 74.6269, Val Loss: 0.2946, Val Acc: 93.9394\n",
      "Val Loss: 0.803022712469101, Val Acc: 40.909090909090914\n",
      "Epoch 2901/50000, Tr Loss: 0.3593, Tr Acc: 79.1045, Val Loss: 0.2951, Val Acc: 93.9394\n",
      "Val Loss: 0.8420714139938354, Val Acc: 40.909090909090914\n",
      "Epoch 3001/50000, Tr Loss: 0.4249, Tr Acc: 71.6418, Val Loss: 0.2957, Val Acc: 93.9394\n",
      "Val Loss: 0.781940221786499, Val Acc: 40.909090909090914\n",
      "Epoch 3101/50000, Tr Loss: 0.3944, Tr Acc: 76.1194, Val Loss: 0.2969, Val Acc: 93.9394\n",
      "Val Loss: 0.8281000852584839, Val Acc: 40.909090909090914\n",
      "Epoch 3201/50000, Tr Loss: 0.3894, Tr Acc: 71.6418, Val Loss: 0.2982, Val Acc: 93.9394\n",
      "Val Loss: 0.8016399145126343, Val Acc: 40.909090909090914\n",
      "Epoch 3301/50000, Tr Loss: 0.4213, Tr Acc: 73.1343, Val Loss: 0.2995, Val Acc: 93.9394\n",
      "Val Loss: 0.8075809180736542, Val Acc: 43.18181818181818\n",
      "Epoch 3401/50000, Tr Loss: 0.4509, Tr Acc: 70.1493, Val Loss: 0.2998, Val Acc: 93.9394\n",
      "Val Loss: 0.7846582233905792, Val Acc: 43.18181818181818\n",
      "Epoch 3501/50000, Tr Loss: 0.4150, Tr Acc: 73.1343, Val Loss: 0.3011, Val Acc: 93.9394\n",
      "Val Loss: 0.803626298904419, Val Acc: 43.18181818181818\n",
      "Epoch 3601/50000, Tr Loss: 0.3923, Tr Acc: 73.1343, Val Loss: 0.3021, Val Acc: 93.9394\n",
      "Val Loss: 0.8038716316223145, Val Acc: 43.18181818181818\n",
      "Epoch 3701/50000, Tr Loss: 0.4300, Tr Acc: 70.1493, Val Loss: 0.3029, Val Acc: 93.9394\n",
      "Val Loss: 0.833489328622818, Val Acc: 45.45454545454545\n",
      "Epoch 3801/50000, Tr Loss: 0.4360, Tr Acc: 68.6567, Val Loss: 0.3041, Val Acc: 93.9394\n",
      "Val Loss: 0.7638542950153351, Val Acc: 45.45454545454545\n",
      "Epoch 3901/50000, Tr Loss: 0.4501, Tr Acc: 68.6567, Val Loss: 0.3049, Val Acc: 93.9394\n",
      "Val Loss: 0.7905156314373016, Val Acc: 45.45454545454545\n",
      "Epoch 4001/50000, Tr Loss: 0.4057, Tr Acc: 74.6269, Val Loss: 0.3058, Val Acc: 93.9394\n",
      "Val Loss: 0.7958483397960663, Val Acc: 45.45454545454545\n",
      "Epoch 4101/50000, Tr Loss: 0.4008, Tr Acc: 70.1493, Val Loss: 0.3071, Val Acc: 93.9394\n",
      "Val Loss: 0.8401519656181335, Val Acc: 45.45454545454545\n",
      "Epoch 4201/50000, Tr Loss: 0.4072, Tr Acc: 67.1642, Val Loss: 0.3081, Val Acc: 93.9394\n",
      "Val Loss: 0.8174540996551514, Val Acc: 45.45454545454545\n",
      "Epoch 4301/50000, Tr Loss: 0.4231, Tr Acc: 71.6418, Val Loss: 0.3087, Val Acc: 93.9394\n",
      "Val Loss: 0.7882096469402313, Val Acc: 45.45454545454545\n",
      "Epoch 4401/50000, Tr Loss: 0.3916, Tr Acc: 65.6716, Val Loss: 0.3105, Val Acc: 93.9394\n",
      "Val Loss: 0.8119646608829498, Val Acc: 45.45454545454545\n",
      "Epoch 4501/50000, Tr Loss: 0.3919, Tr Acc: 70.1493, Val Loss: 0.3109, Val Acc: 93.9394\n",
      "Val Loss: 0.8278228938579559, Val Acc: 45.45454545454545\n",
      "Epoch 4601/50000, Tr Loss: 0.4055, Tr Acc: 73.1343, Val Loss: 0.3114, Val Acc: 93.9394\n",
      "Val Loss: 0.7921896874904633, Val Acc: 45.45454545454545\n",
      "Epoch 4701/50000, Tr Loss: 0.4180, Tr Acc: 70.1493, Val Loss: 0.3129, Val Acc: 93.9394\n",
      "Val Loss: 0.8389792144298553, Val Acc: 45.45454545454545\n",
      "Epoch 4801/50000, Tr Loss: 0.4205, Tr Acc: 68.6567, Val Loss: 0.3135, Val Acc: 93.9394\n",
      "Val Loss: 0.7989509999752045, Val Acc: 45.45454545454545\n",
      "Epoch 4901/50000, Tr Loss: 0.3907, Tr Acc: 77.6119, Val Loss: 0.3145, Val Acc: 93.9394\n",
      "Val Loss: 0.762309193611145, Val Acc: 45.45454545454545\n",
      "Epoch 5001/50000, Tr Loss: 0.3928, Tr Acc: 73.1343, Val Loss: 0.3153, Val Acc: 93.9394\n",
      "Val Loss: 0.7977850437164307, Val Acc: 45.45454545454545\n",
      "Epoch 5101/50000, Tr Loss: 0.3972, Tr Acc: 74.6269, Val Loss: 0.3162, Val Acc: 93.9394\n",
      "Val Loss: 0.7850243747234344, Val Acc: 45.45454545454545\n",
      "Epoch 5201/50000, Tr Loss: 0.3879, Tr Acc: 71.6418, Val Loss: 0.3171, Val Acc: 93.9394\n",
      "Val Loss: 0.808772623538971, Val Acc: 45.45454545454545\n",
      "Epoch 5301/50000, Tr Loss: 0.4013, Tr Acc: 76.1194, Val Loss: 0.3185, Val Acc: 93.9394\n",
      "Val Loss: 0.8129651844501495, Val Acc: 45.45454545454545\n",
      "Epoch 5401/50000, Tr Loss: 0.4214, Tr Acc: 67.1642, Val Loss: 0.3192, Val Acc: 93.9394\n",
      "Val Loss: 0.7757610380649567, Val Acc: 45.45454545454545\n",
      "Epoch 5501/50000, Tr Loss: 0.3980, Tr Acc: 77.6119, Val Loss: 0.3203, Val Acc: 93.9394\n",
      "Val Loss: 0.8386658430099487, Val Acc: 45.45454545454545\n",
      "Epoch 5601/50000, Tr Loss: 0.4045, Tr Acc: 68.6567, Val Loss: 0.3214, Val Acc: 93.9394\n",
      "Val Loss: 0.8161223530769348, Val Acc: 45.45454545454545\n",
      "Epoch 5701/50000, Tr Loss: 0.4033, Tr Acc: 74.6269, Val Loss: 0.3221, Val Acc: 93.9394\n",
      "Val Loss: 0.798515647649765, Val Acc: 45.45454545454545\n",
      "Epoch 5801/50000, Tr Loss: 0.4154, Tr Acc: 70.1493, Val Loss: 0.3231, Val Acc: 93.9394\n",
      "Val Loss: 0.76600581407547, Val Acc: 45.45454545454545\n",
      "Epoch 5901/50000, Tr Loss: 0.3856, Tr Acc: 74.6269, Val Loss: 0.3244, Val Acc: 93.9394\n",
      "Val Loss: 0.8552826046943665, Val Acc: 45.45454545454545\n",
      "Epoch 6001/50000, Tr Loss: 0.3710, Tr Acc: 82.0896, Val Loss: 0.3254, Val Acc: 93.9394\n",
      "Val Loss: 0.7543787062168121, Val Acc: 45.45454545454545\n",
      "Epoch 6101/50000, Tr Loss: 0.3671, Tr Acc: 77.6119, Val Loss: 0.3261, Val Acc: 93.9394\n",
      "Val Loss: 0.7921478748321533, Val Acc: 45.45454545454545\n",
      "Epoch 6201/50000, Tr Loss: 0.4088, Tr Acc: 77.6119, Val Loss: 0.3274, Val Acc: 93.9394\n",
      "Val Loss: 0.7726710736751556, Val Acc: 45.45454545454545\n",
      "Epoch 6301/50000, Tr Loss: 0.3801, Tr Acc: 70.1493, Val Loss: 0.3277, Val Acc: 93.9394\n",
      "Val Loss: 0.7923913300037384, Val Acc: 45.45454545454545\n",
      "Epoch 6401/50000, Tr Loss: 0.3757, Tr Acc: 74.6269, Val Loss: 0.3286, Val Acc: 93.9394\n",
      "Val Loss: 0.7997661530971527, Val Acc: 45.45454545454545\n",
      "Epoch 6501/50000, Tr Loss: 0.3976, Tr Acc: 68.6567, Val Loss: 0.3295, Val Acc: 93.9394\n",
      "Val Loss: 0.750370979309082, Val Acc: 45.45454545454545\n",
      "Epoch 6601/50000, Tr Loss: 0.3888, Tr Acc: 74.6269, Val Loss: 0.3308, Val Acc: 93.9394\n",
      "Val Loss: 0.8286272287368774, Val Acc: 45.45454545454545\n",
      "Epoch 6701/50000, Tr Loss: 0.3646, Tr Acc: 74.6269, Val Loss: 0.3318, Val Acc: 93.9394\n",
      "Val Loss: 0.797153651714325, Val Acc: 45.45454545454545\n",
      "Epoch 6801/50000, Tr Loss: 0.3616, Tr Acc: 77.6119, Val Loss: 0.3332, Val Acc: 93.9394\n",
      "Val Loss: 0.7900463044643402, Val Acc: 45.45454545454545\n",
      "Epoch 6901/50000, Tr Loss: 0.4049, Tr Acc: 74.6269, Val Loss: 0.3335, Val Acc: 93.9394\n",
      "Val Loss: 0.8047491014003754, Val Acc: 45.45454545454545\n",
      "Epoch 7001/50000, Tr Loss: 0.3599, Tr Acc: 76.1194, Val Loss: 0.3350, Val Acc: 93.9394\n",
      "Val Loss: 0.8312000334262848, Val Acc: 45.45454545454545\n",
      "Epoch 7101/50000, Tr Loss: 0.3667, Tr Acc: 73.1343, Val Loss: 0.3365, Val Acc: 90.9091\n",
      "Val Loss: 0.7629207968711853, Val Acc: 45.45454545454545\n",
      "Epoch 7201/50000, Tr Loss: 0.3696, Tr Acc: 73.1343, Val Loss: 0.3366, Val Acc: 90.9091\n",
      "Val Loss: 0.7634795010089874, Val Acc: 45.45454545454545\n",
      "Epoch 7301/50000, Tr Loss: 0.3829, Tr Acc: 77.6119, Val Loss: 0.3377, Val Acc: 90.9091\n",
      "Val Loss: 0.7579548954963684, Val Acc: 45.45454545454545\n",
      "Epoch 7401/50000, Tr Loss: 0.3461, Tr Acc: 85.0746, Val Loss: 0.3388, Val Acc: 90.9091\n",
      "Val Loss: 0.8220423758029938, Val Acc: 45.45454545454545\n",
      "Epoch 7501/50000, Tr Loss: 0.3856, Tr Acc: 77.6119, Val Loss: 0.3401, Val Acc: 90.9091\n",
      "Val Loss: 0.809440940618515, Val Acc: 45.45454545454545\n",
      "Epoch 7601/50000, Tr Loss: 0.3864, Tr Acc: 74.6269, Val Loss: 0.3407, Val Acc: 90.9091\n",
      "Val Loss: 0.8811950385570526, Val Acc: 45.45454545454545\n",
      "Epoch 7701/50000, Tr Loss: 0.4070, Tr Acc: 71.6418, Val Loss: 0.3421, Val Acc: 90.9091\n",
      "Val Loss: 0.8012909293174744, Val Acc: 45.45454545454545\n",
      "Epoch 7801/50000, Tr Loss: 0.3532, Tr Acc: 79.1045, Val Loss: 0.3433, Val Acc: 90.9091\n",
      "Val Loss: 0.7965430319309235, Val Acc: 43.18181818181818\n",
      "Epoch 7901/50000, Tr Loss: 0.3563, Tr Acc: 74.6269, Val Loss: 0.3433, Val Acc: 90.9091\n",
      "Val Loss: 0.7734602391719818, Val Acc: 43.18181818181818\n",
      "Epoch 8001/50000, Tr Loss: 0.3561, Tr Acc: 77.6119, Val Loss: 0.3443, Val Acc: 90.9091\n",
      "Val Loss: 0.7073365449905396, Val Acc: 43.18181818181818\n",
      "Epoch 8101/50000, Tr Loss: 0.3478, Tr Acc: 70.1493, Val Loss: 0.3452, Val Acc: 90.9091\n",
      "Val Loss: 0.7601184546947479, Val Acc: 43.18181818181818\n",
      "Epoch 8201/50000, Tr Loss: 0.3619, Tr Acc: 80.5970, Val Loss: 0.3463, Val Acc: 90.9091\n",
      "Val Loss: 0.7898249924182892, Val Acc: 43.18181818181818\n",
      "Epoch 8301/50000, Tr Loss: 0.3973, Tr Acc: 71.6418, Val Loss: 0.3477, Val Acc: 90.9091\n",
      "Val Loss: 0.7984083294868469, Val Acc: 43.18181818181818\n",
      "Epoch 8401/50000, Tr Loss: 0.3601, Tr Acc: 74.6269, Val Loss: 0.3482, Val Acc: 90.9091\n",
      "Val Loss: 0.7848112881183624, Val Acc: 43.18181818181818\n",
      "Epoch 8501/50000, Tr Loss: 0.3878, Tr Acc: 70.1493, Val Loss: 0.3498, Val Acc: 90.9091\n",
      "Val Loss: 0.8078012764453888, Val Acc: 43.18181818181818\n",
      "Epoch 8601/50000, Tr Loss: 0.3517, Tr Acc: 80.5970, Val Loss: 0.3507, Val Acc: 90.9091\n",
      "Val Loss: 0.77569180727005, Val Acc: 43.18181818181818\n",
      "Epoch 8701/50000, Tr Loss: 0.3583, Tr Acc: 83.5821, Val Loss: 0.3516, Val Acc: 90.9091\n",
      "Val Loss: 0.7893498539924622, Val Acc: 45.45454545454545\n",
      "Epoch 8801/50000, Tr Loss: 0.3586, Tr Acc: 79.1045, Val Loss: 0.3531, Val Acc: 90.9091\n",
      "Val Loss: 0.759235143661499, Val Acc: 45.45454545454545\n",
      "Epoch 8901/50000, Tr Loss: 0.3450, Tr Acc: 80.5970, Val Loss: 0.3541, Val Acc: 90.9091\n",
      "Val Loss: 0.8142358660697937, Val Acc: 45.45454545454545\n",
      "Epoch 9001/50000, Tr Loss: 0.3705, Tr Acc: 71.6418, Val Loss: 0.3551, Val Acc: 90.9091\n",
      "Val Loss: 0.7589883208274841, Val Acc: 45.45454545454545\n",
      "Epoch 9101/50000, Tr Loss: 0.3505, Tr Acc: 76.1194, Val Loss: 0.3556, Val Acc: 90.9091\n",
      "Val Loss: 0.8094989955425262, Val Acc: 45.45454545454545\n",
      "Epoch 9201/50000, Tr Loss: 0.3393, Tr Acc: 77.6119, Val Loss: 0.3567, Val Acc: 90.9091\n",
      "Val Loss: 0.7856080532073975, Val Acc: 45.45454545454545\n",
      "Epoch 9301/50000, Tr Loss: 0.3621, Tr Acc: 76.1194, Val Loss: 0.3583, Val Acc: 90.9091\n",
      "Val Loss: 0.8357613682746887, Val Acc: 45.45454545454545\n",
      "Epoch 9401/50000, Tr Loss: 0.3585, Tr Acc: 76.1194, Val Loss: 0.3591, Val Acc: 90.9091\n",
      "Val Loss: 0.8071911334991455, Val Acc: 45.45454545454545\n",
      "Epoch 9501/50000, Tr Loss: 0.3423, Tr Acc: 79.1045, Val Loss: 0.3603, Val Acc: 90.9091\n",
      "Val Loss: 0.7636945247650146, Val Acc: 45.45454545454545\n",
      "Epoch 9601/50000, Tr Loss: 0.3392, Tr Acc: 79.1045, Val Loss: 0.3618, Val Acc: 90.9091\n",
      "Val Loss: 0.8088443875312805, Val Acc: 45.45454545454545\n",
      "Epoch 9701/50000, Tr Loss: 0.3463, Tr Acc: 82.0896, Val Loss: 0.3622, Val Acc: 90.9091\n",
      "Val Loss: 0.770136296749115, Val Acc: 45.45454545454545\n",
      "Epoch 9801/50000, Tr Loss: 0.3616, Tr Acc: 77.6119, Val Loss: 0.3636, Val Acc: 90.9091\n",
      "Val Loss: 0.7917816936969757, Val Acc: 45.45454545454545\n",
      "Epoch 9901/50000, Tr Loss: 0.3529, Tr Acc: 71.6418, Val Loss: 0.3642, Val Acc: 90.9091\n",
      "Val Loss: 0.7761516273021698, Val Acc: 45.45454545454545\n",
      "Epoch 10001/50000, Tr Loss: 0.3510, Tr Acc: 79.1045, Val Loss: 0.3657, Val Acc: 90.9091\n",
      "Val Loss: 0.7484830021858215, Val Acc: 45.45454545454545\n",
      "Epoch 10101/50000, Tr Loss: 0.3627, Tr Acc: 79.1045, Val Loss: 0.3660, Val Acc: 90.9091\n",
      "Val Loss: 0.7627863883972168, Val Acc: 45.45454545454545\n",
      "Epoch 10201/50000, Tr Loss: 0.3502, Tr Acc: 80.5970, Val Loss: 0.3670, Val Acc: 90.9091\n",
      "Val Loss: 0.7733289897441864, Val Acc: 45.45454545454545\n",
      "Epoch 10301/50000, Tr Loss: 0.3416, Tr Acc: 79.1045, Val Loss: 0.3679, Val Acc: 90.9091\n",
      "Val Loss: 0.7985661923885345, Val Acc: 47.72727272727273\n",
      "Epoch 10401/50000, Tr Loss: 0.3625, Tr Acc: 70.1493, Val Loss: 0.3696, Val Acc: 90.9091\n",
      "Val Loss: 0.8642643094062805, Val Acc: 47.72727272727273\n",
      "Epoch 10501/50000, Tr Loss: 0.3292, Tr Acc: 82.0896, Val Loss: 0.3698, Val Acc: 90.9091\n",
      "Val Loss: 0.8033628463745117, Val Acc: 47.72727272727273\n",
      "Epoch 10601/50000, Tr Loss: 0.3577, Tr Acc: 76.1194, Val Loss: 0.3709, Val Acc: 90.9091\n",
      "Val Loss: 0.8296710252761841, Val Acc: 47.72727272727273\n",
      "Epoch 10701/50000, Tr Loss: 0.3732, Tr Acc: 70.1493, Val Loss: 0.3717, Val Acc: 90.9091\n",
      "Val Loss: 0.7833595275878906, Val Acc: 47.72727272727273\n",
      "Epoch 10801/50000, Tr Loss: 0.3675, Tr Acc: 77.6119, Val Loss: 0.3726, Val Acc: 90.9091\n",
      "Val Loss: 0.8044276535511017, Val Acc: 45.45454545454545\n",
      "Epoch 10901/50000, Tr Loss: 0.3759, Tr Acc: 79.1045, Val Loss: 0.3735, Val Acc: 90.9091\n",
      "Val Loss: 0.7838111221790314, Val Acc: 47.72727272727273\n",
      "Epoch 11001/50000, Tr Loss: 0.3131, Tr Acc: 86.5672, Val Loss: 0.3744, Val Acc: 90.9091\n",
      "Val Loss: 0.802751362323761, Val Acc: 47.72727272727273\n",
      "Epoch 11101/50000, Tr Loss: 0.3564, Tr Acc: 76.1194, Val Loss: 0.3754, Val Acc: 90.9091\n",
      "Val Loss: 0.7850720584392548, Val Acc: 47.72727272727273\n",
      "Epoch 11201/50000, Tr Loss: 0.3367, Tr Acc: 79.1045, Val Loss: 0.3770, Val Acc: 90.9091\n",
      "Val Loss: 0.804383784532547, Val Acc: 45.45454545454545\n",
      "Epoch 11301/50000, Tr Loss: 0.3463, Tr Acc: 82.0896, Val Loss: 0.3775, Val Acc: 90.9091\n",
      "Val Loss: 0.8442471325397491, Val Acc: 47.72727272727273\n",
      "Epoch 11401/50000, Tr Loss: 0.3460, Tr Acc: 82.0896, Val Loss: 0.3788, Val Acc: 90.9091\n",
      "Val Loss: 0.829526424407959, Val Acc: 47.72727272727273\n",
      "Epoch 11501/50000, Tr Loss: 0.3357, Tr Acc: 79.1045, Val Loss: 0.3794, Val Acc: 90.9091\n",
      "Val Loss: 0.7597329020500183, Val Acc: 47.72727272727273\n",
      "Epoch 11601/50000, Tr Loss: 0.3006, Tr Acc: 89.5522, Val Loss: 0.3807, Val Acc: 90.9091\n",
      "Val Loss: 0.8220254182815552, Val Acc: 47.72727272727273\n",
      "Epoch 11701/50000, Tr Loss: 0.3477, Tr Acc: 74.6269, Val Loss: 0.3814, Val Acc: 90.9091\n",
      "Val Loss: 0.7708489894866943, Val Acc: 47.72727272727273\n",
      "Epoch 11801/50000, Tr Loss: 0.3360, Tr Acc: 80.5970, Val Loss: 0.3833, Val Acc: 90.9091\n",
      "Val Loss: 0.8344279229640961, Val Acc: 45.45454545454545\n",
      "Epoch 11901/50000, Tr Loss: 0.3478, Tr Acc: 79.1045, Val Loss: 0.3846, Val Acc: 90.9091\n",
      "Val Loss: 0.8120537400245667, Val Acc: 47.72727272727273\n",
      "Epoch 12001/50000, Tr Loss: 0.3325, Tr Acc: 80.5970, Val Loss: 0.3847, Val Acc: 90.9091\n",
      "Val Loss: 0.7784501016139984, Val Acc: 47.72727272727273\n",
      "Epoch 12101/50000, Tr Loss: 0.3313, Tr Acc: 77.6119, Val Loss: 0.3865, Val Acc: 90.9091\n",
      "Val Loss: 0.7850208580493927, Val Acc: 47.72727272727273\n",
      "Epoch 12201/50000, Tr Loss: 0.3322, Tr Acc: 86.5672, Val Loss: 0.3875, Val Acc: 90.9091\n",
      "Val Loss: 0.7877236902713776, Val Acc: 47.72727272727273\n",
      "Epoch 12301/50000, Tr Loss: 0.3487, Tr Acc: 77.6119, Val Loss: 0.3885, Val Acc: 90.9091\n",
      "Val Loss: 0.7759130597114563, Val Acc: 47.72727272727273\n",
      "Epoch 12401/50000, Tr Loss: 0.3147, Tr Acc: 79.1045, Val Loss: 0.3887, Val Acc: 90.9091\n",
      "Val Loss: 0.7791081964969635, Val Acc: 47.72727272727273\n",
      "Epoch 12501/50000, Tr Loss: 0.3491, Tr Acc: 82.0896, Val Loss: 0.3899, Val Acc: 90.9091\n",
      "Val Loss: 0.7850256562232971, Val Acc: 47.72727272727273\n",
      "Epoch 12601/50000, Tr Loss: 0.3446, Tr Acc: 79.1045, Val Loss: 0.3909, Val Acc: 90.9091\n",
      "Val Loss: 0.7781405448913574, Val Acc: 47.72727272727273\n",
      "Epoch 12701/50000, Tr Loss: 0.3246, Tr Acc: 79.1045, Val Loss: 0.3918, Val Acc: 90.9091\n",
      "Val Loss: 0.8127888441085815, Val Acc: 47.72727272727273\n",
      "Epoch 12801/50000, Tr Loss: 0.3198, Tr Acc: 80.5970, Val Loss: 0.3931, Val Acc: 90.9091\n",
      "Val Loss: 0.8206758201122284, Val Acc: 47.72727272727273\n",
      "Epoch 12901/50000, Tr Loss: 0.3439, Tr Acc: 74.6269, Val Loss: 0.3937, Val Acc: 90.9091\n",
      "Val Loss: 0.7980689406394958, Val Acc: 47.72727272727273\n",
      "Epoch 13001/50000, Tr Loss: 0.3529, Tr Acc: 77.6119, Val Loss: 0.3945, Val Acc: 90.9091\n",
      "Val Loss: 0.7577501237392426, Val Acc: 47.72727272727273\n",
      "Epoch 13101/50000, Tr Loss: 0.3423, Tr Acc: 77.6119, Val Loss: 0.3957, Val Acc: 90.9091\n",
      "Val Loss: 0.7599287331104279, Val Acc: 47.72727272727273\n",
      "Epoch 13201/50000, Tr Loss: 0.3321, Tr Acc: 79.1045, Val Loss: 0.3968, Val Acc: 90.9091\n",
      "Val Loss: 0.8211372196674347, Val Acc: 47.72727272727273\n",
      "Epoch 13301/50000, Tr Loss: 0.3420, Tr Acc: 73.1343, Val Loss: 0.3971, Val Acc: 90.9091\n",
      "Val Loss: 0.7918318510055542, Val Acc: 47.72727272727273\n",
      "Epoch 13401/50000, Tr Loss: 0.3204, Tr Acc: 85.0746, Val Loss: 0.3985, Val Acc: 90.9091\n",
      "Val Loss: 0.8036227226257324, Val Acc: 47.72727272727273\n",
      "Epoch 13501/50000, Tr Loss: 0.3515, Tr Acc: 73.1343, Val Loss: 0.3997, Val Acc: 90.9091\n",
      "Val Loss: 0.7826624512672424, Val Acc: 47.72727272727273\n",
      "Epoch 13601/50000, Tr Loss: 0.3258, Tr Acc: 77.6119, Val Loss: 0.4011, Val Acc: 90.9091\n",
      "Val Loss: 0.7806354463100433, Val Acc: 47.72727272727273\n",
      "Epoch 13701/50000, Tr Loss: 0.3152, Tr Acc: 82.0896, Val Loss: 0.4020, Val Acc: 90.9091\n",
      "Val Loss: 0.8359617590904236, Val Acc: 47.72727272727273\n",
      "Epoch 13801/50000, Tr Loss: 0.3345, Tr Acc: 85.0746, Val Loss: 0.4027, Val Acc: 90.9091\n",
      "Val Loss: 0.7746152281761169, Val Acc: 47.72727272727273\n",
      "Epoch 13901/50000, Tr Loss: 0.3204, Tr Acc: 79.1045, Val Loss: 0.4039, Val Acc: 90.9091\n",
      "Val Loss: 0.8189607560634613, Val Acc: 47.72727272727273\n",
      "Epoch 14001/50000, Tr Loss: 0.3344, Tr Acc: 80.5970, Val Loss: 0.4052, Val Acc: 90.9091\n",
      "Val Loss: 0.8091046214103699, Val Acc: 47.72727272727273\n",
      "Epoch 14101/50000, Tr Loss: 0.3232, Tr Acc: 82.0896, Val Loss: 0.4063, Val Acc: 90.9091\n",
      "Val Loss: 0.7816548049449921, Val Acc: 47.72727272727273\n",
      "Epoch 14201/50000, Tr Loss: 0.3524, Tr Acc: 73.1343, Val Loss: 0.4074, Val Acc: 90.9091\n",
      "Val Loss: 0.7687031328678131, Val Acc: 47.72727272727273\n",
      "Epoch 14301/50000, Tr Loss: 0.3177, Tr Acc: 79.1045, Val Loss: 0.4079, Val Acc: 90.9091\n",
      "Val Loss: 0.7961426675319672, Val Acc: 47.72727272727273\n",
      "Epoch 14401/50000, Tr Loss: 0.3470, Tr Acc: 77.6119, Val Loss: 0.4093, Val Acc: 90.9091\n",
      "Val Loss: 0.8171465396881104, Val Acc: 47.72727272727273\n",
      "Epoch 14501/50000, Tr Loss: 0.3253, Tr Acc: 80.5970, Val Loss: 0.4100, Val Acc: 90.9091\n",
      "Val Loss: 0.7968826591968536, Val Acc: 47.72727272727273\n",
      "Epoch 14601/50000, Tr Loss: 0.3119, Tr Acc: 80.5970, Val Loss: 0.4109, Val Acc: 90.9091\n",
      "Val Loss: 0.8271050155162811, Val Acc: 47.72727272727273\n",
      "Epoch 14701/50000, Tr Loss: 0.3281, Tr Acc: 86.5672, Val Loss: 0.4112, Val Acc: 90.9091\n",
      "Val Loss: 0.7779774963855743, Val Acc: 47.72727272727273\n",
      "Epoch 14801/50000, Tr Loss: 0.3108, Tr Acc: 83.5821, Val Loss: 0.4124, Val Acc: 90.9091\n",
      "Val Loss: 0.7305859327316284, Val Acc: 47.72727272727273\n",
      "Epoch 14901/50000, Tr Loss: 0.3233, Tr Acc: 80.5970, Val Loss: 0.4141, Val Acc: 90.9091\n",
      "Val Loss: 0.8090631365776062, Val Acc: 47.72727272727273\n",
      "Epoch 15001/50000, Tr Loss: 0.3302, Tr Acc: 85.0746, Val Loss: 0.4152, Val Acc: 90.9091\n",
      "Val Loss: 0.7877876162528992, Val Acc: 47.72727272727273\n",
      "Epoch 15101/50000, Tr Loss: 0.3233, Tr Acc: 77.6119, Val Loss: 0.4153, Val Acc: 90.9091\n",
      "Val Loss: 0.7703630924224854, Val Acc: 47.72727272727273\n",
      "Epoch 15201/50000, Tr Loss: 0.3504, Tr Acc: 74.6269, Val Loss: 0.4163, Val Acc: 90.9091\n",
      "Val Loss: 0.774215430021286, Val Acc: 47.72727272727273\n",
      "Epoch 15301/50000, Tr Loss: 0.3479, Tr Acc: 74.6269, Val Loss: 0.4175, Val Acc: 90.9091\n",
      "Val Loss: 0.7983364760875702, Val Acc: 47.72727272727273\n",
      "Epoch 15401/50000, Tr Loss: 0.3567, Tr Acc: 76.1194, Val Loss: 0.4191, Val Acc: 90.9091\n",
      "Val Loss: 0.8242950141429901, Val Acc: 47.72727272727273\n",
      "Epoch 15501/50000, Tr Loss: 0.2992, Tr Acc: 83.5821, Val Loss: 0.4199, Val Acc: 90.9091\n",
      "Val Loss: 0.7744664251804352, Val Acc: 47.72727272727273\n",
      "Epoch 15601/50000, Tr Loss: 0.3384, Tr Acc: 76.1194, Val Loss: 0.4205, Val Acc: 90.9091\n",
      "Val Loss: 0.8158639073371887, Val Acc: 47.72727272727273\n",
      "Epoch 15701/50000, Tr Loss: 0.2994, Tr Acc: 85.0746, Val Loss: 0.4214, Val Acc: 90.9091\n",
      "Val Loss: 0.7739925980567932, Val Acc: 47.72727272727273\n",
      "Epoch 15801/50000, Tr Loss: 0.3372, Tr Acc: 79.1045, Val Loss: 0.4220, Val Acc: 90.9091\n",
      "Val Loss: 0.7892427444458008, Val Acc: 47.72727272727273\n",
      "Epoch 15901/50000, Tr Loss: 0.3098, Tr Acc: 82.0896, Val Loss: 0.4229, Val Acc: 90.9091\n",
      "Val Loss: 0.777951568365097, Val Acc: 47.72727272727273\n",
      "Epoch 16001/50000, Tr Loss: 0.3097, Tr Acc: 80.5970, Val Loss: 0.4239, Val Acc: 90.9091\n",
      "Val Loss: 0.7951823770999908, Val Acc: 47.72727272727273\n",
      "Epoch 16101/50000, Tr Loss: 0.3059, Tr Acc: 80.5970, Val Loss: 0.4257, Val Acc: 90.9091\n",
      "Val Loss: 0.8179864287376404, Val Acc: 45.45454545454545\n",
      "Epoch 16201/50000, Tr Loss: 0.3265, Tr Acc: 79.1045, Val Loss: 0.4267, Val Acc: 90.9091\n",
      "Val Loss: 0.7545522153377533, Val Acc: 47.72727272727273\n",
      "Epoch 16301/50000, Tr Loss: 0.3138, Tr Acc: 79.1045, Val Loss: 0.4270, Val Acc: 90.9091\n",
      "Val Loss: 0.7866373360157013, Val Acc: 47.72727272727273\n",
      "Epoch 16401/50000, Tr Loss: 0.3035, Tr Acc: 82.0896, Val Loss: 0.4277, Val Acc: 90.9091\n",
      "Val Loss: 0.7728956043720245, Val Acc: 47.72727272727273\n",
      "Epoch 16501/50000, Tr Loss: 0.3333, Tr Acc: 74.6269, Val Loss: 0.4285, Val Acc: 90.9091\n",
      "Val Loss: 0.7731615304946899, Val Acc: 47.72727272727273\n",
      "Epoch 16601/50000, Tr Loss: 0.3165, Tr Acc: 83.5821, Val Loss: 0.4298, Val Acc: 90.9091\n",
      "Val Loss: 0.8458793461322784, Val Acc: 45.45454545454545\n",
      "Epoch 16701/50000, Tr Loss: 0.3459, Tr Acc: 73.1343, Val Loss: 0.4307, Val Acc: 90.9091\n",
      "Val Loss: 0.8168711960315704, Val Acc: 45.45454545454545\n",
      "Epoch 16801/50000, Tr Loss: 0.3268, Tr Acc: 82.0896, Val Loss: 0.4314, Val Acc: 87.8788\n",
      "Val Loss: 0.8109903931617737, Val Acc: 47.72727272727273\n",
      "Epoch 16901/50000, Tr Loss: 0.3027, Tr Acc: 83.5821, Val Loss: 0.4323, Val Acc: 87.8788\n",
      "Val Loss: 0.8292747139930725, Val Acc: 47.72727272727273\n",
      "Epoch 17001/50000, Tr Loss: 0.2916, Tr Acc: 89.5522, Val Loss: 0.4337, Val Acc: 87.8788\n",
      "Val Loss: 0.810573548078537, Val Acc: 45.45454545454545\n",
      "Epoch 17101/50000, Tr Loss: 0.3276, Tr Acc: 76.1194, Val Loss: 0.4346, Val Acc: 87.8788\n",
      "Val Loss: 0.7813321650028229, Val Acc: 45.45454545454545\n",
      "Epoch 17201/50000, Tr Loss: 0.3200, Tr Acc: 82.0896, Val Loss: 0.4353, Val Acc: 87.8788\n",
      "Val Loss: 0.8200744390487671, Val Acc: 45.45454545454545\n",
      "Epoch 17301/50000, Tr Loss: 0.3359, Tr Acc: 77.6119, Val Loss: 0.4361, Val Acc: 87.8788\n",
      "Val Loss: 0.7835380136966705, Val Acc: 45.45454545454545\n",
      "Epoch 17401/50000, Tr Loss: 0.2811, Tr Acc: 88.0597, Val Loss: 0.4367, Val Acc: 87.8788\n",
      "Val Loss: 0.7867327034473419, Val Acc: 45.45454545454545\n",
      "Epoch 17501/50000, Tr Loss: 0.2783, Tr Acc: 80.5970, Val Loss: 0.4380, Val Acc: 87.8788\n",
      "Val Loss: 0.8360334932804108, Val Acc: 45.45454545454545\n",
      "Epoch 17601/50000, Tr Loss: 0.3099, Tr Acc: 85.0746, Val Loss: 0.4383, Val Acc: 87.8788\n",
      "Val Loss: 0.8309895992279053, Val Acc: 45.45454545454545\n",
      "Epoch 17701/50000, Tr Loss: 0.2985, Tr Acc: 83.5821, Val Loss: 0.4398, Val Acc: 87.8788\n",
      "Val Loss: 0.7854425609111786, Val Acc: 45.45454545454545\n",
      "Epoch 17801/50000, Tr Loss: 0.3124, Tr Acc: 80.5970, Val Loss: 0.4412, Val Acc: 87.8788\n",
      "Val Loss: 0.7528354227542877, Val Acc: 45.45454545454545\n",
      "Epoch 17901/50000, Tr Loss: 0.3197, Tr Acc: 77.6119, Val Loss: 0.4416, Val Acc: 87.8788\n",
      "Val Loss: 0.7734066545963287, Val Acc: 45.45454545454545\n",
      "Epoch 18001/50000, Tr Loss: 0.2953, Tr Acc: 89.5522, Val Loss: 0.4429, Val Acc: 87.8788\n",
      "Val Loss: 0.7636241018772125, Val Acc: 45.45454545454545\n",
      "Epoch 18101/50000, Tr Loss: 0.2911, Tr Acc: 82.0896, Val Loss: 0.4430, Val Acc: 87.8788\n",
      "Val Loss: 0.7504486441612244, Val Acc: 45.45454545454545\n",
      "Epoch 18201/50000, Tr Loss: 0.2979, Tr Acc: 89.5522, Val Loss: 0.4449, Val Acc: 87.8788\n",
      "Val Loss: 0.8398908078670502, Val Acc: 45.45454545454545\n",
      "Epoch 18301/50000, Tr Loss: 0.2826, Tr Acc: 85.0746, Val Loss: 0.4451, Val Acc: 87.8788\n",
      "Val Loss: 0.842179149389267, Val Acc: 47.72727272727273\n",
      "Epoch 18401/50000, Tr Loss: 0.2848, Tr Acc: 83.5821, Val Loss: 0.4466, Val Acc: 87.8788\n",
      "Val Loss: 0.7939459979534149, Val Acc: 47.72727272727273\n",
      "Epoch 18501/50000, Tr Loss: 0.3215, Tr Acc: 85.0746, Val Loss: 0.4472, Val Acc: 87.8788\n",
      "Val Loss: 0.854733794927597, Val Acc: 47.72727272727273\n",
      "Epoch 18601/50000, Tr Loss: 0.3209, Tr Acc: 79.1045, Val Loss: 0.4488, Val Acc: 87.8788\n",
      "Val Loss: 0.7763867378234863, Val Acc: 45.45454545454545\n",
      "Epoch 18701/50000, Tr Loss: 0.2911, Tr Acc: 91.0448, Val Loss: 0.4492, Val Acc: 87.8788\n",
      "Val Loss: 0.8444616794586182, Val Acc: 45.45454545454545\n",
      "Epoch 18801/50000, Tr Loss: 0.3041, Tr Acc: 89.5522, Val Loss: 0.4508, Val Acc: 87.8788\n",
      "Val Loss: 0.806014895439148, Val Acc: 45.45454545454545\n",
      "Epoch 18901/50000, Tr Loss: 0.3070, Tr Acc: 86.5672, Val Loss: 0.4509, Val Acc: 87.8788\n",
      "Val Loss: 0.8288081288337708, Val Acc: 45.45454545454545\n",
      "Epoch 19001/50000, Tr Loss: 0.3021, Tr Acc: 82.0896, Val Loss: 0.4515, Val Acc: 87.8788\n",
      "Val Loss: 0.8251976668834686, Val Acc: 43.18181818181818\n",
      "Epoch 19101/50000, Tr Loss: 0.3087, Tr Acc: 83.5821, Val Loss: 0.4529, Val Acc: 87.8788\n",
      "Val Loss: 0.7708065509796143, Val Acc: 43.18181818181818\n",
      "Epoch 19201/50000, Tr Loss: 0.3003, Tr Acc: 89.5522, Val Loss: 0.4540, Val Acc: 87.8788\n",
      "Val Loss: 0.7756263017654419, Val Acc: 43.18181818181818\n",
      "Epoch 19301/50000, Tr Loss: 0.2940, Tr Acc: 83.5821, Val Loss: 0.4545, Val Acc: 87.8788\n",
      "Val Loss: 0.8094315528869629, Val Acc: 43.18181818181818\n",
      "Epoch 19401/50000, Tr Loss: 0.3058, Tr Acc: 83.5821, Val Loss: 0.4555, Val Acc: 87.8788\n",
      "Val Loss: 0.7704012095928192, Val Acc: 43.18181818181818\n",
      "Epoch 19501/50000, Tr Loss: 0.2941, Tr Acc: 85.0746, Val Loss: 0.4570, Val Acc: 87.8788\n",
      "Val Loss: 0.7608300447463989, Val Acc: 43.18181818181818\n",
      "Epoch 19601/50000, Tr Loss: 0.3066, Tr Acc: 82.0896, Val Loss: 0.4575, Val Acc: 87.8788\n",
      "Val Loss: 0.7732000052928925, Val Acc: 43.18181818181818\n",
      "Epoch 19701/50000, Tr Loss: 0.2960, Tr Acc: 83.5821, Val Loss: 0.4585, Val Acc: 87.8788\n",
      "Val Loss: 0.7485317885875702, Val Acc: 43.18181818181818\n",
      "Epoch 19801/50000, Tr Loss: 0.2788, Tr Acc: 83.5821, Val Loss: 0.4594, Val Acc: 87.8788\n",
      "Val Loss: 0.7971695959568024, Val Acc: 43.18181818181818\n",
      "Epoch 19901/50000, Tr Loss: 0.2940, Tr Acc: 88.0597, Val Loss: 0.4606, Val Acc: 87.8788\n",
      "Val Loss: 0.8111905455589294, Val Acc: 43.18181818181818\n",
      "Epoch 20001/50000, Tr Loss: 0.2944, Tr Acc: 82.0896, Val Loss: 0.4616, Val Acc: 87.8788\n",
      "Val Loss: 0.82502481341362, Val Acc: 43.18181818181818\n",
      "Epoch 20101/50000, Tr Loss: 0.2719, Tr Acc: 91.0448, Val Loss: 0.4619, Val Acc: 87.8788\n",
      "Val Loss: 0.7513432204723358, Val Acc: 43.18181818181818\n",
      "Epoch 20201/50000, Tr Loss: 0.2915, Tr Acc: 86.5672, Val Loss: 0.4620, Val Acc: 87.8788\n",
      "Val Loss: 0.7666424810886383, Val Acc: 43.18181818181818\n",
      "Epoch 20301/50000, Tr Loss: 0.3171, Tr Acc: 80.5970, Val Loss: 0.4637, Val Acc: 87.8788\n",
      "Val Loss: 0.7974931001663208, Val Acc: 43.18181818181818\n",
      "Epoch 20401/50000, Tr Loss: 0.2950, Tr Acc: 83.5821, Val Loss: 0.4647, Val Acc: 87.8788\n",
      "Val Loss: 0.8692869544029236, Val Acc: 43.18181818181818\n",
      "Epoch 20501/50000, Tr Loss: 0.2943, Tr Acc: 83.5821, Val Loss: 0.4659, Val Acc: 87.8788\n",
      "Val Loss: 0.7382749021053314, Val Acc: 43.18181818181818\n",
      "Epoch 20601/50000, Tr Loss: 0.2950, Tr Acc: 86.5672, Val Loss: 0.4669, Val Acc: 87.8788\n",
      "Val Loss: 0.7729699611663818, Val Acc: 43.18181818181818\n",
      "Epoch 20701/50000, Tr Loss: 0.2780, Tr Acc: 86.5672, Val Loss: 0.4681, Val Acc: 84.8485\n",
      "Val Loss: 0.7971175312995911, Val Acc: 43.18181818181818\n",
      "Epoch 20801/50000, Tr Loss: 0.2783, Tr Acc: 85.0746, Val Loss: 0.4697, Val Acc: 84.8485\n",
      "Val Loss: 0.8074910342693329, Val Acc: 43.18181818181818\n",
      "Epoch 20901/50000, Tr Loss: 0.2984, Tr Acc: 82.0896, Val Loss: 0.4696, Val Acc: 84.8485\n",
      "Val Loss: 0.8184550702571869, Val Acc: 43.18181818181818\n",
      "Epoch 21001/50000, Tr Loss: 0.2744, Tr Acc: 86.5672, Val Loss: 0.4694, Val Acc: 84.8485\n",
      "Val Loss: 0.8327049911022186, Val Acc: 43.18181818181818\n",
      "Epoch 21101/50000, Tr Loss: 0.2928, Tr Acc: 85.0746, Val Loss: 0.4710, Val Acc: 84.8485\n",
      "Val Loss: 0.7810830771923065, Val Acc: 43.18181818181818\n",
      "Epoch 21201/50000, Tr Loss: 0.3002, Tr Acc: 80.5970, Val Loss: 0.4712, Val Acc: 84.8485\n",
      "Val Loss: 0.7852716743946075, Val Acc: 43.18181818181818\n",
      "Epoch 21301/50000, Tr Loss: 0.3077, Tr Acc: 83.5821, Val Loss: 0.4731, Val Acc: 84.8485\n",
      "Val Loss: 0.8312957286834717, Val Acc: 43.18181818181818\n",
      "Epoch 21401/50000, Tr Loss: 0.2928, Tr Acc: 86.5672, Val Loss: 0.4736, Val Acc: 84.8485\n",
      "Val Loss: 0.8377661108970642, Val Acc: 43.18181818181818\n",
      "Epoch 21501/50000, Tr Loss: 0.2775, Tr Acc: 86.5672, Val Loss: 0.4738, Val Acc: 84.8485\n",
      "Val Loss: 0.8085361421108246, Val Acc: 43.18181818181818\n",
      "Epoch 21601/50000, Tr Loss: 0.3025, Tr Acc: 82.0896, Val Loss: 0.4751, Val Acc: 84.8485\n",
      "Val Loss: 0.7872434556484222, Val Acc: 40.909090909090914\n",
      "Epoch 21701/50000, Tr Loss: 0.2732, Tr Acc: 86.5672, Val Loss: 0.4756, Val Acc: 84.8485\n",
      "Val Loss: 0.824192225933075, Val Acc: 40.909090909090914\n",
      "Epoch 21801/50000, Tr Loss: 0.2994, Tr Acc: 83.5821, Val Loss: 0.4775, Val Acc: 84.8485\n",
      "Val Loss: 0.7552032768726349, Val Acc: 40.909090909090914\n",
      "Epoch 21901/50000, Tr Loss: 0.2934, Tr Acc: 83.5821, Val Loss: 0.4776, Val Acc: 84.8485\n",
      "Val Loss: 0.7924101650714874, Val Acc: 40.909090909090914\n",
      "Epoch 22001/50000, Tr Loss: 0.3082, Tr Acc: 82.0896, Val Loss: 0.4784, Val Acc: 81.8182\n",
      "Val Loss: 0.8289037048816681, Val Acc: 40.909090909090914\n",
      "Epoch 22101/50000, Tr Loss: 0.2945, Tr Acc: 88.0597, Val Loss: 0.4792, Val Acc: 81.8182\n",
      "Val Loss: 0.8207343816757202, Val Acc: 40.909090909090914\n",
      "Epoch 22201/50000, Tr Loss: 0.3038, Tr Acc: 82.0896, Val Loss: 0.4805, Val Acc: 81.8182\n",
      "Val Loss: 0.8247726559638977, Val Acc: 40.909090909090914\n",
      "Epoch 22301/50000, Tr Loss: 0.3120, Tr Acc: 79.1045, Val Loss: 0.4809, Val Acc: 81.8182\n",
      "Val Loss: 0.7698611617088318, Val Acc: 40.909090909090914\n",
      "Epoch 22401/50000, Tr Loss: 0.2708, Tr Acc: 91.0448, Val Loss: 0.4820, Val Acc: 81.8182\n",
      "Val Loss: 0.8729119598865509, Val Acc: 40.909090909090914\n",
      "Epoch 22501/50000, Tr Loss: 0.2861, Tr Acc: 83.5821, Val Loss: 0.4831, Val Acc: 81.8182\n",
      "Val Loss: 0.8327096104621887, Val Acc: 40.909090909090914\n",
      "Epoch 22601/50000, Tr Loss: 0.2808, Tr Acc: 82.0896, Val Loss: 0.4845, Val Acc: 81.8182\n",
      "Val Loss: 0.733373612165451, Val Acc: 40.909090909090914\n",
      "Epoch 22701/50000, Tr Loss: 0.2633, Tr Acc: 88.0597, Val Loss: 0.4857, Val Acc: 81.8182\n",
      "Val Loss: 0.8801005184650421, Val Acc: 40.909090909090914\n",
      "Epoch 22801/50000, Tr Loss: 0.2682, Tr Acc: 88.0597, Val Loss: 0.4863, Val Acc: 81.8182\n",
      "Val Loss: 0.8166140019893646, Val Acc: 40.909090909090914\n",
      "Epoch 22901/50000, Tr Loss: 0.2769, Tr Acc: 85.0746, Val Loss: 0.4866, Val Acc: 81.8182\n",
      "Val Loss: 0.819427102804184, Val Acc: 38.63636363636363\n",
      "Epoch 23001/50000, Tr Loss: 0.2847, Tr Acc: 86.5672, Val Loss: 0.4864, Val Acc: 81.8182\n",
      "Val Loss: 0.8233856558799744, Val Acc: 38.63636363636363\n",
      "Epoch 23101/50000, Tr Loss: 0.2718, Tr Acc: 86.5672, Val Loss: 0.4877, Val Acc: 81.8182\n",
      "Val Loss: 0.7762060463428497, Val Acc: 38.63636363636363\n",
      "Epoch 23201/50000, Tr Loss: 0.2668, Tr Acc: 88.0597, Val Loss: 0.4889, Val Acc: 81.8182\n",
      "Val Loss: 0.7606377601623535, Val Acc: 38.63636363636363\n",
      "Epoch 23301/50000, Tr Loss: 0.2905, Tr Acc: 85.0746, Val Loss: 0.4891, Val Acc: 81.8182\n",
      "Val Loss: 0.752564549446106, Val Acc: 38.63636363636363\n",
      "Epoch 23401/50000, Tr Loss: 0.2661, Tr Acc: 89.5522, Val Loss: 0.4905, Val Acc: 81.8182\n",
      "Val Loss: 0.8249286413192749, Val Acc: 38.63636363636363\n",
      "Epoch 23501/50000, Tr Loss: 0.2742, Tr Acc: 94.0298, Val Loss: 0.4916, Val Acc: 81.8182\n",
      "Val Loss: 0.8234851360321045, Val Acc: 38.63636363636363\n",
      "Epoch 23601/50000, Tr Loss: 0.2935, Tr Acc: 82.0896, Val Loss: 0.4928, Val Acc: 81.8182\n",
      "Val Loss: 0.8449410796165466, Val Acc: 38.63636363636363\n",
      "Epoch 23701/50000, Tr Loss: 0.2679, Tr Acc: 92.5373, Val Loss: 0.4929, Val Acc: 81.8182\n",
      "Val Loss: 0.7818931937217712, Val Acc: 38.63636363636363\n",
      "Epoch 23801/50000, Tr Loss: 0.2744, Tr Acc: 85.0746, Val Loss: 0.4940, Val Acc: 81.8182\n",
      "Val Loss: 0.7745873928070068, Val Acc: 38.63636363636363\n",
      "Epoch 23901/50000, Tr Loss: 0.2635, Tr Acc: 86.5672, Val Loss: 0.4942, Val Acc: 81.8182\n",
      "Val Loss: 0.7720910310745239, Val Acc: 38.63636363636363\n",
      "Epoch 24001/50000, Tr Loss: 0.2700, Tr Acc: 82.0896, Val Loss: 0.4953, Val Acc: 81.8182\n",
      "Val Loss: 0.8419753909111023, Val Acc: 36.36363636363637\n",
      "Epoch 24101/50000, Tr Loss: 0.2693, Tr Acc: 85.0746, Val Loss: 0.4967, Val Acc: 81.8182\n",
      "Val Loss: 0.7370893955230713, Val Acc: 36.36363636363637\n",
      "Epoch 24201/50000, Tr Loss: 0.2876, Tr Acc: 88.0597, Val Loss: 0.4976, Val Acc: 81.8182\n",
      "Val Loss: 0.840153843164444, Val Acc: 36.36363636363637\n",
      "Epoch 24301/50000, Tr Loss: 0.2804, Tr Acc: 88.0597, Val Loss: 0.4976, Val Acc: 81.8182\n",
      "Val Loss: 0.8555736839771271, Val Acc: 36.36363636363637\n",
      "Epoch 24401/50000, Tr Loss: 0.2799, Tr Acc: 85.0746, Val Loss: 0.4978, Val Acc: 81.8182\n",
      "Val Loss: 0.8131249248981476, Val Acc: 36.36363636363637\n",
      "Epoch 24501/50000, Tr Loss: 0.2713, Tr Acc: 83.5821, Val Loss: 0.4996, Val Acc: 81.8182\n",
      "Val Loss: 0.8170802295207977, Val Acc: 36.36363636363637\n",
      "Epoch 24601/50000, Tr Loss: 0.2653, Tr Acc: 83.5821, Val Loss: 0.5003, Val Acc: 81.8182\n",
      "Val Loss: 0.8892380595207214, Val Acc: 36.36363636363637\n",
      "Epoch 24701/50000, Tr Loss: 0.2536, Tr Acc: 91.0448, Val Loss: 0.5015, Val Acc: 81.8182\n",
      "Val Loss: 0.8408736288547516, Val Acc: 36.36363636363637\n",
      "Epoch 24801/50000, Tr Loss: 0.2643, Tr Acc: 85.0746, Val Loss: 0.5030, Val Acc: 81.8182\n",
      "Val Loss: 0.7899826467037201, Val Acc: 36.36363636363637\n",
      "Epoch 24901/50000, Tr Loss: 0.2858, Tr Acc: 89.5522, Val Loss: 0.5018, Val Acc: 81.8182\n",
      "Val Loss: 0.8253991007804871, Val Acc: 36.36363636363637\n",
      "Epoch 25001/50000, Tr Loss: 0.2736, Tr Acc: 83.5821, Val Loss: 0.5039, Val Acc: 81.8182\n",
      "Val Loss: 0.7962758839130402, Val Acc: 36.36363636363637\n",
      "Epoch 25101/50000, Tr Loss: 0.2730, Tr Acc: 88.0597, Val Loss: 0.5036, Val Acc: 81.8182\n",
      "Val Loss: 0.8179956674575806, Val Acc: 36.36363636363637\n",
      "Epoch 25201/50000, Tr Loss: 0.2743, Tr Acc: 89.5522, Val Loss: 0.5059, Val Acc: 81.8182\n",
      "Val Loss: 0.8090988397598267, Val Acc: 36.36363636363637\n",
      "Epoch 25301/50000, Tr Loss: 0.2986, Tr Acc: 88.0597, Val Loss: 0.5061, Val Acc: 81.8182\n",
      "Val Loss: 0.8825950622558594, Val Acc: 36.36363636363637\n",
      "Epoch 25401/50000, Tr Loss: 0.2682, Tr Acc: 89.5522, Val Loss: 0.5062, Val Acc: 81.8182\n",
      "Val Loss: 0.8565355241298676, Val Acc: 36.36363636363637\n",
      "Epoch 25501/50000, Tr Loss: 0.2555, Tr Acc: 82.0896, Val Loss: 0.5071, Val Acc: 81.8182\n",
      "Val Loss: 0.8122831284999847, Val Acc: 36.36363636363637\n",
      "Epoch 25601/50000, Tr Loss: 0.2903, Tr Acc: 88.0597, Val Loss: 0.5084, Val Acc: 81.8182\n",
      "Val Loss: 0.8478322327136993, Val Acc: 36.36363636363637\n",
      "Epoch 25701/50000, Tr Loss: 0.2812, Tr Acc: 86.5672, Val Loss: 0.5079, Val Acc: 78.7879\n",
      "Val Loss: 0.780110090970993, Val Acc: 36.36363636363637\n",
      "Epoch 25801/50000, Tr Loss: 0.2611, Tr Acc: 91.0448, Val Loss: 0.5094, Val Acc: 78.7879\n",
      "Val Loss: 0.8509535491466522, Val Acc: 36.36363636363637\n",
      "Epoch 25901/50000, Tr Loss: 0.2576, Tr Acc: 85.0746, Val Loss: 0.5097, Val Acc: 78.7879\n",
      "Val Loss: 0.8238714933395386, Val Acc: 36.36363636363637\n",
      "Epoch 26001/50000, Tr Loss: 0.2719, Tr Acc: 88.0597, Val Loss: 0.5108, Val Acc: 78.7879\n",
      "Val Loss: 0.8095867037773132, Val Acc: 36.36363636363637\n",
      "Epoch 26101/50000, Tr Loss: 0.2466, Tr Acc: 89.5522, Val Loss: 0.5126, Val Acc: 78.7879\n",
      "Val Loss: 0.8005542159080505, Val Acc: 36.36363636363637\n",
      "Epoch 26201/50000, Tr Loss: 0.2691, Tr Acc: 88.0597, Val Loss: 0.5129, Val Acc: 78.7879\n",
      "Val Loss: 0.786112368106842, Val Acc: 36.36363636363637\n",
      "Epoch 26301/50000, Tr Loss: 0.2487, Tr Acc: 91.0448, Val Loss: 0.5146, Val Acc: 78.7879\n",
      "Val Loss: 0.8025523722171783, Val Acc: 36.36363636363637\n",
      "Epoch 26401/50000, Tr Loss: 0.2579, Tr Acc: 89.5522, Val Loss: 0.5151, Val Acc: 78.7879\n",
      "Val Loss: 0.8026807308197021, Val Acc: 36.36363636363637\n",
      "Epoch 26501/50000, Tr Loss: 0.2453, Tr Acc: 92.5373, Val Loss: 0.5158, Val Acc: 78.7879\n",
      "Val Loss: 0.8167749047279358, Val Acc: 36.36363636363637\n",
      "Epoch 26601/50000, Tr Loss: 0.2452, Tr Acc: 92.5373, Val Loss: 0.5160, Val Acc: 78.7879\n",
      "Val Loss: 0.812860518693924, Val Acc: 36.36363636363637\n",
      "Epoch 26701/50000, Tr Loss: 0.2660, Tr Acc: 85.0746, Val Loss: 0.5165, Val Acc: 78.7879\n",
      "Val Loss: 0.7993265390396118, Val Acc: 36.36363636363637\n",
      "Epoch 26801/50000, Tr Loss: 0.2434, Tr Acc: 89.5522, Val Loss: 0.5181, Val Acc: 78.7879\n",
      "Val Loss: 0.8197448253631592, Val Acc: 36.36363636363637\n",
      "Epoch 26901/50000, Tr Loss: 0.2754, Tr Acc: 89.5522, Val Loss: 0.5186, Val Acc: 78.7879\n",
      "Val Loss: 0.8058821856975555, Val Acc: 36.36363636363637\n",
      "Epoch 27001/50000, Tr Loss: 0.2330, Tr Acc: 88.0597, Val Loss: 0.5187, Val Acc: 78.7879\n",
      "Val Loss: 0.834349513053894, Val Acc: 36.36363636363637\n",
      "Epoch 27101/50000, Tr Loss: 0.2795, Tr Acc: 86.5672, Val Loss: 0.5201, Val Acc: 78.7879\n",
      "Val Loss: 0.7970271110534668, Val Acc: 36.36363636363637\n",
      "Epoch 27201/50000, Tr Loss: 0.2615, Tr Acc: 86.5672, Val Loss: 0.5208, Val Acc: 78.7879\n",
      "Val Loss: 0.7445799112319946, Val Acc: 36.36363636363637\n",
      "Epoch 27301/50000, Tr Loss: 0.2715, Tr Acc: 82.0896, Val Loss: 0.5216, Val Acc: 78.7879\n",
      "Val Loss: 0.8095721304416656, Val Acc: 36.36363636363637\n",
      "Epoch 27401/50000, Tr Loss: 0.2570, Tr Acc: 91.0448, Val Loss: 0.5223, Val Acc: 78.7879\n",
      "Val Loss: 0.8444013297557831, Val Acc: 38.63636363636363\n",
      "Epoch 27501/50000, Tr Loss: 0.2514, Tr Acc: 92.5373, Val Loss: 0.5240, Val Acc: 78.7879\n",
      "Val Loss: 0.8147479891777039, Val Acc: 36.36363636363637\n",
      "Epoch 27601/50000, Tr Loss: 0.2793, Tr Acc: 83.5821, Val Loss: 0.5237, Val Acc: 78.7879\n",
      "Val Loss: 0.7681958675384521, Val Acc: 38.63636363636363\n",
      "Epoch 27701/50000, Tr Loss: 0.2618, Tr Acc: 86.5672, Val Loss: 0.5249, Val Acc: 78.7879\n",
      "Val Loss: 0.8085412979125977, Val Acc: 38.63636363636363\n",
      "Epoch 27801/50000, Tr Loss: 0.2774, Tr Acc: 89.5522, Val Loss: 0.5258, Val Acc: 78.7879\n",
      "Val Loss: 0.7697422802448273, Val Acc: 36.36363636363637\n",
      "Epoch 27901/50000, Tr Loss: 0.2360, Tr Acc: 94.0298, Val Loss: 0.5267, Val Acc: 78.7879\n",
      "Val Loss: 0.8170973360538483, Val Acc: 38.63636363636363\n",
      "Epoch 28001/50000, Tr Loss: 0.2678, Tr Acc: 83.5821, Val Loss: 0.5269, Val Acc: 78.7879\n",
      "Val Loss: 0.7808495163917542, Val Acc: 38.63636363636363\n",
      "Epoch 28101/50000, Tr Loss: 0.2616, Tr Acc: 88.0597, Val Loss: 0.5282, Val Acc: 78.7879\n",
      "Val Loss: 0.8503799140453339, Val Acc: 38.63636363636363\n",
      "Epoch 28201/50000, Tr Loss: 0.2654, Tr Acc: 86.5672, Val Loss: 0.5285, Val Acc: 78.7879\n",
      "Val Loss: 0.8388054966926575, Val Acc: 38.63636363636363\n",
      "Epoch 28301/50000, Tr Loss: 0.2606, Tr Acc: 91.0448, Val Loss: 0.5297, Val Acc: 78.7879\n",
      "Val Loss: 0.7809353172779083, Val Acc: 38.63636363636363\n",
      "Epoch 28401/50000, Tr Loss: 0.2436, Tr Acc: 91.0448, Val Loss: 0.5303, Val Acc: 78.7879\n",
      "Val Loss: 0.7908333539962769, Val Acc: 38.63636363636363\n",
      "Epoch 28501/50000, Tr Loss: 0.2438, Tr Acc: 88.0597, Val Loss: 0.5309, Val Acc: 78.7879\n",
      "Val Loss: 0.772011399269104, Val Acc: 38.63636363636363\n",
      "Epoch 28601/50000, Tr Loss: 0.2658, Tr Acc: 89.5522, Val Loss: 0.5310, Val Acc: 78.7879\n",
      "Val Loss: 0.9034052193164825, Val Acc: 38.63636363636363\n",
      "Epoch 28701/50000, Tr Loss: 0.2394, Tr Acc: 94.0298, Val Loss: 0.5327, Val Acc: 78.7879\n",
      "Val Loss: 0.795012354850769, Val Acc: 38.63636363636363\n",
      "Epoch 28801/50000, Tr Loss: 0.2373, Tr Acc: 89.5522, Val Loss: 0.5329, Val Acc: 78.7879\n",
      "Val Loss: 0.8208922445774078, Val Acc: 38.63636363636363\n",
      "Epoch 28901/50000, Tr Loss: 0.2510, Tr Acc: 89.5522, Val Loss: 0.5338, Val Acc: 78.7879\n",
      "Val Loss: 0.7888588309288025, Val Acc: 38.63636363636363\n",
      "Epoch 29001/50000, Tr Loss: 0.2595, Tr Acc: 94.0298, Val Loss: 0.5344, Val Acc: 78.7879\n",
      "Val Loss: 0.8191374242305756, Val Acc: 38.63636363636363\n",
      "Epoch 29101/50000, Tr Loss: 0.2522, Tr Acc: 86.5672, Val Loss: 0.5343, Val Acc: 78.7879\n",
      "Val Loss: 0.8485405743122101, Val Acc: 38.63636363636363\n",
      "Epoch 29201/50000, Tr Loss: 0.2583, Tr Acc: 89.5522, Val Loss: 0.5353, Val Acc: 78.7879\n",
      "Val Loss: 0.8336653411388397, Val Acc: 38.63636363636363\n",
      "Epoch 29301/50000, Tr Loss: 0.2382, Tr Acc: 92.5373, Val Loss: 0.5361, Val Acc: 78.7879\n",
      "Val Loss: 0.8036889731884003, Val Acc: 38.63636363636363\n",
      "Epoch 29401/50000, Tr Loss: 0.2729, Tr Acc: 89.5522, Val Loss: 0.5370, Val Acc: 78.7879\n",
      "Val Loss: 0.7982096672058105, Val Acc: 38.63636363636363\n",
      "Epoch 29501/50000, Tr Loss: 0.2456, Tr Acc: 94.0298, Val Loss: 0.5380, Val Acc: 78.7879\n",
      "Val Loss: 0.8579433858394623, Val Acc: 38.63636363636363\n",
      "Epoch 29601/50000, Tr Loss: 0.2844, Tr Acc: 82.0896, Val Loss: 0.5390, Val Acc: 78.7879\n",
      "Val Loss: 0.8399511575698853, Val Acc: 38.63636363636363\n",
      "Epoch 29701/50000, Tr Loss: 0.2472, Tr Acc: 89.5522, Val Loss: 0.5396, Val Acc: 78.7879\n",
      "Val Loss: 0.8172916769981384, Val Acc: 38.63636363636363\n",
      "Epoch 29801/50000, Tr Loss: 0.2627, Tr Acc: 89.5522, Val Loss: 0.5398, Val Acc: 78.7879\n",
      "Val Loss: 0.7950926125049591, Val Acc: 38.63636363636363\n",
      "Epoch 29901/50000, Tr Loss: 0.2417, Tr Acc: 88.0597, Val Loss: 0.5404, Val Acc: 78.7879\n",
      "Val Loss: 0.8247672617435455, Val Acc: 38.63636363636363\n",
      "Epoch 30001/50000, Tr Loss: 0.2447, Tr Acc: 89.5522, Val Loss: 0.5413, Val Acc: 78.7879\n",
      "Val Loss: 0.7535248696804047, Val Acc: 38.63636363636363\n",
      "Epoch 30101/50000, Tr Loss: 0.2515, Tr Acc: 85.0746, Val Loss: 0.5426, Val Acc: 78.7879\n",
      "Val Loss: 0.7794474959373474, Val Acc: 36.36363636363637\n",
      "Epoch 30201/50000, Tr Loss: 0.2365, Tr Acc: 91.0448, Val Loss: 0.5418, Val Acc: 78.7879\n",
      "Val Loss: 0.8900789618492126, Val Acc: 38.63636363636363\n",
      "Epoch 30301/50000, Tr Loss: 0.2577, Tr Acc: 88.0597, Val Loss: 0.5441, Val Acc: 78.7879\n",
      "Val Loss: 0.9374540448188782, Val Acc: 36.36363636363637\n",
      "Epoch 30401/50000, Tr Loss: 0.2612, Tr Acc: 91.0448, Val Loss: 0.5433, Val Acc: 78.7879\n",
      "Val Loss: 0.7582541704177856, Val Acc: 36.36363636363637\n",
      "Epoch 30501/50000, Tr Loss: 0.2305, Tr Acc: 94.0298, Val Loss: 0.5447, Val Acc: 78.7879\n",
      "Val Loss: 0.8418011963367462, Val Acc: 36.36363636363637\n",
      "Epoch 30601/50000, Tr Loss: 0.2375, Tr Acc: 89.5522, Val Loss: 0.5457, Val Acc: 78.7879\n",
      "Val Loss: 0.8152516782283783, Val Acc: 36.36363636363637\n",
      "Epoch 30701/50000, Tr Loss: 0.2472, Tr Acc: 92.5373, Val Loss: 0.5465, Val Acc: 78.7879\n",
      "Val Loss: 0.9171926975250244, Val Acc: 36.36363636363637\n",
      "Epoch 30801/50000, Tr Loss: 0.2366, Tr Acc: 92.5373, Val Loss: 0.5482, Val Acc: 78.7879\n",
      "Val Loss: 0.8648619055747986, Val Acc: 36.36363636363637\n",
      "Epoch 30901/50000, Tr Loss: 0.2384, Tr Acc: 89.5522, Val Loss: 0.5478, Val Acc: 78.7879\n",
      "Val Loss: 0.826091855764389, Val Acc: 36.36363636363637\n",
      "Epoch 31001/50000, Tr Loss: 0.2419, Tr Acc: 95.5224, Val Loss: 0.5491, Val Acc: 78.7879\n",
      "Val Loss: 0.8331456482410431, Val Acc: 36.36363636363637\n",
      "Epoch 31101/50000, Tr Loss: 0.2237, Tr Acc: 94.0298, Val Loss: 0.5497, Val Acc: 78.7879\n",
      "Val Loss: 0.910435676574707, Val Acc: 36.36363636363637\n",
      "Epoch 31201/50000, Tr Loss: 0.2349, Tr Acc: 89.5522, Val Loss: 0.5505, Val Acc: 78.7879\n",
      "Val Loss: 0.7881882190704346, Val Acc: 36.36363636363637\n",
      "Epoch 31301/50000, Tr Loss: 0.2348, Tr Acc: 88.0597, Val Loss: 0.5511, Val Acc: 78.7879\n",
      "Val Loss: 0.8029942214488983, Val Acc: 36.36363636363637\n",
      "Epoch 31401/50000, Tr Loss: 0.2355, Tr Acc: 89.5522, Val Loss: 0.5511, Val Acc: 78.7879\n",
      "Val Loss: 0.8690110146999359, Val Acc: 36.36363636363637\n",
      "Epoch 31501/50000, Tr Loss: 0.2296, Tr Acc: 89.5522, Val Loss: 0.5535, Val Acc: 78.7879\n",
      "Val Loss: 0.8165101706981659, Val Acc: 36.36363636363637\n",
      "Epoch 31601/50000, Tr Loss: 0.2381, Tr Acc: 91.0448, Val Loss: 0.5545, Val Acc: 78.7879\n",
      "Val Loss: 0.8504745662212372, Val Acc: 36.36363636363637\n",
      "Epoch 31701/50000, Tr Loss: 0.2320, Tr Acc: 94.0298, Val Loss: 0.5545, Val Acc: 78.7879\n",
      "Val Loss: 0.8652999997138977, Val Acc: 36.36363636363637\n",
      "Epoch 31801/50000, Tr Loss: 0.2427, Tr Acc: 89.5522, Val Loss: 0.5546, Val Acc: 78.7879\n",
      "Val Loss: 0.7745075523853302, Val Acc: 36.36363636363637\n",
      "Epoch 31901/50000, Tr Loss: 0.2256, Tr Acc: 94.0298, Val Loss: 0.5551, Val Acc: 78.7879\n",
      "Val Loss: 0.8065319955348969, Val Acc: 36.36363636363637\n",
      "Epoch 32001/50000, Tr Loss: 0.2362, Tr Acc: 91.0448, Val Loss: 0.5563, Val Acc: 78.7879\n",
      "Val Loss: 0.8680505752563477, Val Acc: 36.36363636363637\n",
      "Epoch 32101/50000, Tr Loss: 0.2501, Tr Acc: 89.5522, Val Loss: 0.5562, Val Acc: 78.7879\n",
      "Val Loss: 0.8136056661605835, Val Acc: 36.36363636363637\n",
      "Epoch 32201/50000, Tr Loss: 0.2436, Tr Acc: 88.0597, Val Loss: 0.5574, Val Acc: 78.7879\n",
      "Val Loss: 0.829851508140564, Val Acc: 36.36363636363637\n",
      "Epoch 32301/50000, Tr Loss: 0.2396, Tr Acc: 88.0597, Val Loss: 0.5577, Val Acc: 78.7879\n",
      "Val Loss: 0.8243372738361359, Val Acc: 36.36363636363637\n",
      "Epoch 32401/50000, Tr Loss: 0.2475, Tr Acc: 89.5522, Val Loss: 0.5595, Val Acc: 78.7879\n",
      "Val Loss: 0.835842490196228, Val Acc: 36.36363636363637\n",
      "Epoch 32501/50000, Tr Loss: 0.2421, Tr Acc: 92.5373, Val Loss: 0.5601, Val Acc: 78.7879\n",
      "Val Loss: 0.8014388978481293, Val Acc: 36.36363636363637\n",
      "Epoch 32601/50000, Tr Loss: 0.2452, Tr Acc: 88.0597, Val Loss: 0.5607, Val Acc: 78.7879\n",
      "Val Loss: 0.8294670879840851, Val Acc: 36.36363636363637\n",
      "Epoch 32701/50000, Tr Loss: 0.2390, Tr Acc: 88.0597, Val Loss: 0.5625, Val Acc: 78.7879\n",
      "Val Loss: 0.8452875018119812, Val Acc: 38.63636363636363\n",
      "Epoch 32801/50000, Tr Loss: 0.2436, Tr Acc: 89.5522, Val Loss: 0.5624, Val Acc: 78.7879\n",
      "Val Loss: 0.8258056342601776, Val Acc: 38.63636363636363\n",
      "Epoch 32901/50000, Tr Loss: 0.2249, Tr Acc: 97.0149, Val Loss: 0.5633, Val Acc: 78.7879\n",
      "Val Loss: 0.8279259502887726, Val Acc: 38.63636363636363\n",
      "Epoch 33001/50000, Tr Loss: 0.2399, Tr Acc: 88.0597, Val Loss: 0.5634, Val Acc: 78.7879\n",
      "Val Loss: 0.7804799675941467, Val Acc: 38.63636363636363\n",
      "Epoch 33101/50000, Tr Loss: 0.2327, Tr Acc: 92.5373, Val Loss: 0.5636, Val Acc: 78.7879\n",
      "Val Loss: 0.8479755222797394, Val Acc: 38.63636363636363\n",
      "Epoch 33201/50000, Tr Loss: 0.2425, Tr Acc: 92.5373, Val Loss: 0.5654, Val Acc: 78.7879\n",
      "Val Loss: 0.8334812521934509, Val Acc: 38.63636363636363\n",
      "Epoch 33301/50000, Tr Loss: 0.2489, Tr Acc: 89.5522, Val Loss: 0.5657, Val Acc: 78.7879\n",
      "Val Loss: 0.8181832134723663, Val Acc: 38.63636363636363\n",
      "Epoch 33401/50000, Tr Loss: 0.2324, Tr Acc: 95.5224, Val Loss: 0.5658, Val Acc: 78.7879\n",
      "Val Loss: 0.8129196465015411, Val Acc: 38.63636363636363\n",
      "Epoch 33501/50000, Tr Loss: 0.2450, Tr Acc: 92.5373, Val Loss: 0.5669, Val Acc: 78.7879\n",
      "Val Loss: 0.7594636380672455, Val Acc: 38.63636363636363\n",
      "Epoch 33601/50000, Tr Loss: 0.2509, Tr Acc: 89.5522, Val Loss: 0.5671, Val Acc: 78.7879\n",
      "Val Loss: 0.8439454436302185, Val Acc: 38.63636363636363\n",
      "Epoch 33701/50000, Tr Loss: 0.2452, Tr Acc: 94.0298, Val Loss: 0.5680, Val Acc: 78.7879\n",
      "Val Loss: 0.8040963709354401, Val Acc: 38.63636363636363\n",
      "Epoch 33801/50000, Tr Loss: 0.2123, Tr Acc: 97.0149, Val Loss: 0.5693, Val Acc: 78.7879\n",
      "Val Loss: 0.8592240512371063, Val Acc: 38.63636363636363\n",
      "Epoch 33901/50000, Tr Loss: 0.2630, Tr Acc: 85.0746, Val Loss: 0.5692, Val Acc: 78.7879\n",
      "Val Loss: 0.8294301927089691, Val Acc: 38.63636363636363\n",
      "Epoch 34001/50000, Tr Loss: 0.2224, Tr Acc: 94.0298, Val Loss: 0.5699, Val Acc: 78.7879\n",
      "Val Loss: 0.8879050314426422, Val Acc: 38.63636363636363\n",
      "Epoch 34101/50000, Tr Loss: 0.2373, Tr Acc: 89.5522, Val Loss: 0.5706, Val Acc: 78.7879\n",
      "Val Loss: 0.8335076570510864, Val Acc: 38.63636363636363\n",
      "Epoch 34201/50000, Tr Loss: 0.2244, Tr Acc: 92.5373, Val Loss: 0.5705, Val Acc: 78.7879\n",
      "Val Loss: 0.9171296656131744, Val Acc: 38.63636363636363\n",
      "Epoch 34301/50000, Tr Loss: 0.2380, Tr Acc: 95.5224, Val Loss: 0.5714, Val Acc: 78.7879\n",
      "Val Loss: 0.8334071934223175, Val Acc: 38.63636363636363\n",
      "Epoch 34401/50000, Tr Loss: 0.2258, Tr Acc: 91.0448, Val Loss: 0.5728, Val Acc: 78.7879\n",
      "Val Loss: 0.861656665802002, Val Acc: 38.63636363636363\n",
      "Epoch 34501/50000, Tr Loss: 0.2427, Tr Acc: 91.0448, Val Loss: 0.5738, Val Acc: 78.7879\n",
      "Val Loss: 0.8455581963062286, Val Acc: 38.63636363636363\n",
      "Epoch 34601/50000, Tr Loss: 0.2531, Tr Acc: 85.0746, Val Loss: 0.5732, Val Acc: 78.7879\n",
      "Val Loss: 0.8365789949893951, Val Acc: 38.63636363636363\n",
      "Epoch 34701/50000, Tr Loss: 0.2434, Tr Acc: 91.0448, Val Loss: 0.5743, Val Acc: 78.7879\n",
      "Val Loss: 0.7811361849308014, Val Acc: 38.63636363636363\n",
      "Epoch 34801/50000, Tr Loss: 0.2432, Tr Acc: 86.5672, Val Loss: 0.5750, Val Acc: 78.7879\n",
      "Val Loss: 0.8253822922706604, Val Acc: 38.63636363636363\n",
      "Epoch 34901/50000, Tr Loss: 0.2161, Tr Acc: 95.5224, Val Loss: 0.5761, Val Acc: 78.7879\n",
      "Val Loss: 0.8386080861091614, Val Acc: 38.63636363636363\n",
      "Epoch 35001/50000, Tr Loss: 0.2296, Tr Acc: 92.5373, Val Loss: 0.5769, Val Acc: 78.7879\n",
      "Val Loss: 0.8694385588169098, Val Acc: 38.63636363636363\n",
      "Epoch 35101/50000, Tr Loss: 0.2369, Tr Acc: 92.5373, Val Loss: 0.5766, Val Acc: 78.7879\n",
      "Val Loss: 0.8404161930084229, Val Acc: 38.63636363636363\n",
      "Epoch 35201/50000, Tr Loss: 0.2567, Tr Acc: 86.5672, Val Loss: 0.5784, Val Acc: 78.7879\n",
      "Val Loss: 0.8302990794181824, Val Acc: 38.63636363636363\n",
      "Epoch 35301/50000, Tr Loss: 0.2265, Tr Acc: 92.5373, Val Loss: 0.5788, Val Acc: 78.7879\n",
      "Val Loss: 0.8473000526428223, Val Acc: 38.63636363636363\n",
      "Epoch 35401/50000, Tr Loss: 0.2247, Tr Acc: 94.0298, Val Loss: 0.5798, Val Acc: 78.7879\n",
      "Val Loss: 0.7866604924201965, Val Acc: 38.63636363636363\n",
      "Epoch 35501/50000, Tr Loss: 0.2205, Tr Acc: 89.5522, Val Loss: 0.5800, Val Acc: 78.7879\n",
      "Val Loss: 0.8774023056030273, Val Acc: 38.63636363636363\n",
      "Epoch 35601/50000, Tr Loss: 0.2162, Tr Acc: 92.5373, Val Loss: 0.5812, Val Acc: 78.7879\n",
      "Val Loss: 0.8241952061653137, Val Acc: 38.63636363636363\n",
      "Epoch 35701/50000, Tr Loss: 0.2120, Tr Acc: 95.5224, Val Loss: 0.5823, Val Acc: 78.7879\n",
      "Val Loss: 0.842483788728714, Val Acc: 38.63636363636363\n",
      "Epoch 35801/50000, Tr Loss: 0.2255, Tr Acc: 88.0597, Val Loss: 0.5819, Val Acc: 78.7879\n",
      "Val Loss: 0.801134467124939, Val Acc: 38.63636363636363\n",
      "Epoch 35901/50000, Tr Loss: 0.2249, Tr Acc: 95.5224, Val Loss: 0.5831, Val Acc: 78.7879\n",
      "Val Loss: 0.8565333783626556, Val Acc: 38.63636363636363\n",
      "Epoch 36001/50000, Tr Loss: 0.2477, Tr Acc: 86.5672, Val Loss: 0.5843, Val Acc: 78.7879\n",
      "Val Loss: 0.8408014476299286, Val Acc: 38.63636363636363\n",
      "Epoch 36101/50000, Tr Loss: 0.2223, Tr Acc: 94.0298, Val Loss: 0.5840, Val Acc: 78.7879\n",
      "Val Loss: 0.8569771647453308, Val Acc: 38.63636363636363\n",
      "Epoch 36201/50000, Tr Loss: 0.2409, Tr Acc: 91.0448, Val Loss: 0.5858, Val Acc: 78.7879\n",
      "Val Loss: 0.8875355422496796, Val Acc: 38.63636363636363\n",
      "Epoch 36301/50000, Tr Loss: 0.2226, Tr Acc: 94.0298, Val Loss: 0.5856, Val Acc: 78.7879\n",
      "Val Loss: 0.7429874539375305, Val Acc: 38.63636363636363\n",
      "Epoch 36401/50000, Tr Loss: 0.2215, Tr Acc: 91.0448, Val Loss: 0.5871, Val Acc: 78.7879\n",
      "Val Loss: 0.8840840756893158, Val Acc: 38.63636363636363\n",
      "Epoch 36501/50000, Tr Loss: 0.2317, Tr Acc: 89.5522, Val Loss: 0.5874, Val Acc: 78.7879\n",
      "Val Loss: 0.8946867883205414, Val Acc: 38.63636363636363\n",
      "Epoch 36601/50000, Tr Loss: 0.2205, Tr Acc: 94.0298, Val Loss: 0.5879, Val Acc: 78.7879\n",
      "Val Loss: 0.8492763042449951, Val Acc: 38.63636363636363\n",
      "Epoch 36701/50000, Tr Loss: 0.2314, Tr Acc: 92.5373, Val Loss: 0.5892, Val Acc: 78.7879\n",
      "Val Loss: 0.8303427994251251, Val Acc: 38.63636363636363\n",
      "Epoch 36801/50000, Tr Loss: 0.2257, Tr Acc: 94.0298, Val Loss: 0.5887, Val Acc: 78.7879\n",
      "Val Loss: 0.8013707399368286, Val Acc: 38.63636363636363\n",
      "Epoch 36901/50000, Tr Loss: 0.2329, Tr Acc: 89.5522, Val Loss: 0.5901, Val Acc: 78.7879\n",
      "Val Loss: 0.8802143335342407, Val Acc: 38.63636363636363\n",
      "Epoch 37001/50000, Tr Loss: 0.2154, Tr Acc: 97.0149, Val Loss: 0.5909, Val Acc: 78.7879\n",
      "Val Loss: 0.8920316696166992, Val Acc: 38.63636363636363\n",
      "Epoch 37101/50000, Tr Loss: 0.2411, Tr Acc: 89.5522, Val Loss: 0.5908, Val Acc: 78.7879\n",
      "Val Loss: 0.8355165421962738, Val Acc: 38.63636363636363\n",
      "Epoch 37201/50000, Tr Loss: 0.2331, Tr Acc: 88.0597, Val Loss: 0.5927, Val Acc: 78.7879\n",
      "Val Loss: 0.813310295343399, Val Acc: 38.63636363636363\n",
      "Epoch 37301/50000, Tr Loss: 0.2126, Tr Acc: 95.5224, Val Loss: 0.5931, Val Acc: 78.7879\n",
      "Val Loss: 0.8639231026172638, Val Acc: 38.63636363636363\n",
      "Epoch 37401/50000, Tr Loss: 0.2116, Tr Acc: 94.0298, Val Loss: 0.5945, Val Acc: 78.7879\n",
      "Val Loss: 0.8498908877372742, Val Acc: 38.63636363636363\n",
      "Epoch 37501/50000, Tr Loss: 0.2314, Tr Acc: 94.0298, Val Loss: 0.5934, Val Acc: 78.7879\n",
      "Val Loss: 0.7957900166511536, Val Acc: 38.63636363636363\n",
      "Epoch 37601/50000, Tr Loss: 0.2369, Tr Acc: 92.5373, Val Loss: 0.5949, Val Acc: 78.7879\n",
      "Val Loss: 0.8826780915260315, Val Acc: 38.63636363636363\n",
      "Epoch 37701/50000, Tr Loss: 0.2385, Tr Acc: 92.5373, Val Loss: 0.5948, Val Acc: 78.7879\n",
      "Val Loss: 0.801482081413269, Val Acc: 38.63636363636363\n",
      "Epoch 37801/50000, Tr Loss: 0.2080, Tr Acc: 97.0149, Val Loss: 0.5955, Val Acc: 78.7879\n",
      "Val Loss: 0.8049631416797638, Val Acc: 38.63636363636363\n",
      "Epoch 37901/50000, Tr Loss: 0.2145, Tr Acc: 94.0298, Val Loss: 0.5963, Val Acc: 78.7879\n",
      "Val Loss: 0.8423904776573181, Val Acc: 38.63636363636363\n",
      "Epoch 38001/50000, Tr Loss: 0.2256, Tr Acc: 94.0298, Val Loss: 0.5969, Val Acc: 78.7879\n",
      "Val Loss: 0.8718241155147552, Val Acc: 38.63636363636363\n",
      "Epoch 38101/50000, Tr Loss: 0.2277, Tr Acc: 91.0448, Val Loss: 0.5989, Val Acc: 78.7879\n",
      "Val Loss: 0.8063774406909943, Val Acc: 38.63636363636363\n",
      "Epoch 38201/50000, Tr Loss: 0.2120, Tr Acc: 94.0298, Val Loss: 0.5978, Val Acc: 78.7879\n",
      "Val Loss: 0.7844381332397461, Val Acc: 38.63636363636363\n",
      "Epoch 38301/50000, Tr Loss: 0.2399, Tr Acc: 92.5373, Val Loss: 0.5984, Val Acc: 78.7879\n",
      "Val Loss: 0.8724502921104431, Val Acc: 38.63636363636363\n",
      "Epoch 38401/50000, Tr Loss: 0.2259, Tr Acc: 92.5373, Val Loss: 0.6000, Val Acc: 78.7879\n",
      "Val Loss: 0.7411320507526398, Val Acc: 38.63636363636363\n",
      "Epoch 38501/50000, Tr Loss: 0.2168, Tr Acc: 92.5373, Val Loss: 0.5988, Val Acc: 78.7879\n",
      "Val Loss: 0.8797518014907837, Val Acc: 38.63636363636363\n",
      "Epoch 38601/50000, Tr Loss: 0.2346, Tr Acc: 91.0448, Val Loss: 0.6017, Val Acc: 78.7879\n",
      "Val Loss: 0.8587526381015778, Val Acc: 38.63636363636363\n",
      "Epoch 38701/50000, Tr Loss: 0.2275, Tr Acc: 95.5224, Val Loss: 0.6022, Val Acc: 78.7879\n",
      "Val Loss: 0.832939863204956, Val Acc: 38.63636363636363\n",
      "Epoch 38801/50000, Tr Loss: 0.2272, Tr Acc: 95.5224, Val Loss: 0.6023, Val Acc: 78.7879\n",
      "Val Loss: 0.8802656531333923, Val Acc: 38.63636363636363\n",
      "Epoch 38901/50000, Tr Loss: 0.2145, Tr Acc: 98.5075, Val Loss: 0.6027, Val Acc: 78.7879\n",
      "Val Loss: 0.8339522480964661, Val Acc: 38.63636363636363\n",
      "Epoch 39001/50000, Tr Loss: 0.2056, Tr Acc: 97.0149, Val Loss: 0.6030, Val Acc: 78.7879\n",
      "Val Loss: 0.8324268460273743, Val Acc: 38.63636363636363\n",
      "Epoch 39101/50000, Tr Loss: 0.2007, Tr Acc: 95.5224, Val Loss: 0.6036, Val Acc: 78.7879\n",
      "Val Loss: 0.8897878229618073, Val Acc: 38.63636363636363\n",
      "Epoch 39201/50000, Tr Loss: 0.2281, Tr Acc: 92.5373, Val Loss: 0.6044, Val Acc: 75.7576\n",
      "Val Loss: 0.795128345489502, Val Acc: 38.63636363636363\n",
      "Epoch 39301/50000, Tr Loss: 0.2252, Tr Acc: 91.0448, Val Loss: 0.6050, Val Acc: 75.7576\n",
      "Val Loss: 0.8562892377376556, Val Acc: 38.63636363636363\n",
      "Epoch 39401/50000, Tr Loss: 0.2234, Tr Acc: 92.5373, Val Loss: 0.6067, Val Acc: 75.7576\n",
      "Val Loss: 0.8435576558113098, Val Acc: 38.63636363636363\n",
      "Epoch 39501/50000, Tr Loss: 0.1972, Tr Acc: 94.0298, Val Loss: 0.6069, Val Acc: 75.7576\n",
      "Val Loss: 0.8481026887893677, Val Acc: 38.63636363636363\n",
      "Epoch 39601/50000, Tr Loss: 0.2228, Tr Acc: 91.0448, Val Loss: 0.6067, Val Acc: 75.7576\n",
      "Val Loss: 0.8667311072349548, Val Acc: 38.63636363636363\n",
      "Epoch 39701/50000, Tr Loss: 0.2257, Tr Acc: 91.0448, Val Loss: 0.6091, Val Acc: 75.7576\n",
      "Val Loss: 0.7901120185852051, Val Acc: 38.63636363636363\n",
      "Epoch 39801/50000, Tr Loss: 0.2205, Tr Acc: 91.0448, Val Loss: 0.6080, Val Acc: 75.7576\n",
      "Val Loss: 0.7888035476207733, Val Acc: 38.63636363636363\n",
      "Epoch 39901/50000, Tr Loss: 0.2151, Tr Acc: 94.0298, Val Loss: 0.6099, Val Acc: 75.7576\n",
      "Val Loss: 0.8633480966091156, Val Acc: 38.63636363636363\n",
      "Epoch 40001/50000, Tr Loss: 0.2258, Tr Acc: 88.0597, Val Loss: 0.6104, Val Acc: 75.7576\n",
      "Val Loss: 0.9046705365180969, Val Acc: 38.63636363636363\n",
      "Epoch 40101/50000, Tr Loss: 0.2056, Tr Acc: 95.5224, Val Loss: 0.6090, Val Acc: 75.7576\n",
      "Val Loss: 0.7853034138679504, Val Acc: 38.63636363636363\n",
      "Epoch 40201/50000, Tr Loss: 0.2063, Tr Acc: 95.5224, Val Loss: 0.6118, Val Acc: 75.7576\n",
      "Val Loss: 0.8348420262336731, Val Acc: 38.63636363636363\n",
      "Epoch 40301/50000, Tr Loss: 0.2188, Tr Acc: 94.0298, Val Loss: 0.6125, Val Acc: 75.7576\n",
      "Val Loss: 0.8730656802654266, Val Acc: 38.63636363636363\n",
      "Epoch 40401/50000, Tr Loss: 0.2094, Tr Acc: 94.0298, Val Loss: 0.6124, Val Acc: 75.7576\n",
      "Val Loss: 0.8951629996299744, Val Acc: 38.63636363636363\n",
      "Epoch 40501/50000, Tr Loss: 0.2036, Tr Acc: 97.0149, Val Loss: 0.6131, Val Acc: 75.7576\n",
      "Val Loss: 0.8150394260883331, Val Acc: 38.63636363636363\n",
      "Epoch 40601/50000, Tr Loss: 0.2066, Tr Acc: 95.5224, Val Loss: 0.6135, Val Acc: 75.7576\n",
      "Val Loss: 0.7855242490768433, Val Acc: 38.63636363636363\n",
      "Epoch 40701/50000, Tr Loss: 0.2026, Tr Acc: 92.5373, Val Loss: 0.6146, Val Acc: 72.7273\n",
      "Val Loss: 0.8070061206817627, Val Acc: 38.63636363636363\n",
      "Epoch 40801/50000, Tr Loss: 0.2204, Tr Acc: 92.5373, Val Loss: 0.6151, Val Acc: 72.7273\n",
      "Val Loss: 0.8353777229785919, Val Acc: 38.63636363636363\n",
      "Epoch 40901/50000, Tr Loss: 0.2124, Tr Acc: 92.5373, Val Loss: 0.6154, Val Acc: 72.7273\n",
      "Val Loss: 0.8662458956241608, Val Acc: 38.63636363636363\n",
      "Epoch 41001/50000, Tr Loss: 0.2154, Tr Acc: 91.0448, Val Loss: 0.6162, Val Acc: 72.7273\n",
      "Val Loss: 0.874898225069046, Val Acc: 38.63636363636363\n",
      "Epoch 41101/50000, Tr Loss: 0.2086, Tr Acc: 94.0298, Val Loss: 0.6165, Val Acc: 72.7273\n",
      "Val Loss: 0.8271839916706085, Val Acc: 38.63636363636363\n",
      "Epoch 41201/50000, Tr Loss: 0.1874, Tr Acc: 95.5224, Val Loss: 0.6184, Val Acc: 72.7273\n",
      "Val Loss: 0.8546479046344757, Val Acc: 38.63636363636363\n",
      "Epoch 41301/50000, Tr Loss: 0.2015, Tr Acc: 97.0149, Val Loss: 0.6182, Val Acc: 72.7273\n",
      "Val Loss: 0.8754139244556427, Val Acc: 38.63636363636363\n",
      "Epoch 41401/50000, Tr Loss: 0.1875, Tr Acc: 97.0149, Val Loss: 0.6182, Val Acc: 72.7273\n",
      "Val Loss: 0.8586029708385468, Val Acc: 38.63636363636363\n",
      "Epoch 41501/50000, Tr Loss: 0.2079, Tr Acc: 94.0298, Val Loss: 0.6187, Val Acc: 72.7273\n",
      "Val Loss: 0.8585185706615448, Val Acc: 38.63636363636363\n",
      "Epoch 41601/50000, Tr Loss: 0.2034, Tr Acc: 94.0298, Val Loss: 0.6197, Val Acc: 69.6970\n",
      "Val Loss: 0.8783193528652191, Val Acc: 38.63636363636363\n",
      "Epoch 41701/50000, Tr Loss: 0.2012, Tr Acc: 92.5373, Val Loss: 0.6201, Val Acc: 69.6970\n",
      "Val Loss: 0.8414092063903809, Val Acc: 38.63636363636363\n",
      "Epoch 41801/50000, Tr Loss: 0.2052, Tr Acc: 92.5373, Val Loss: 0.6199, Val Acc: 69.6970\n",
      "Val Loss: 0.8862151801586151, Val Acc: 38.63636363636363\n",
      "Epoch 41901/50000, Tr Loss: 0.2114, Tr Acc: 89.5522, Val Loss: 0.6222, Val Acc: 69.6970\n",
      "Val Loss: 0.8232788145542145, Val Acc: 38.63636363636363\n",
      "Epoch 42001/50000, Tr Loss: 0.2080, Tr Acc: 91.0448, Val Loss: 0.6213, Val Acc: 69.6970\n",
      "Val Loss: 0.8623064160346985, Val Acc: 38.63636363636363\n",
      "Epoch 42101/50000, Tr Loss: 0.2103, Tr Acc: 92.5373, Val Loss: 0.6223, Val Acc: 69.6970\n",
      "Val Loss: 0.8222944736480713, Val Acc: 38.63636363636363\n",
      "Epoch 42201/50000, Tr Loss: 0.2215, Tr Acc: 89.5522, Val Loss: 0.6235, Val Acc: 69.6970\n",
      "Val Loss: 0.8217094540596008, Val Acc: 38.63636363636363\n",
      "Epoch 42301/50000, Tr Loss: 0.1990, Tr Acc: 94.0298, Val Loss: 0.6242, Val Acc: 69.6970\n",
      "Val Loss: 0.8701533079147339, Val Acc: 38.63636363636363\n",
      "Epoch 42401/50000, Tr Loss: 0.1971, Tr Acc: 94.0298, Val Loss: 0.6238, Val Acc: 69.6970\n",
      "Val Loss: 0.8814980387687683, Val Acc: 38.63636363636363\n",
      "Epoch 42501/50000, Tr Loss: 0.2140, Tr Acc: 91.0448, Val Loss: 0.6244, Val Acc: 69.6970\n",
      "Val Loss: 0.8525148630142212, Val Acc: 38.63636363636363\n",
      "Epoch 42601/50000, Tr Loss: 0.2140, Tr Acc: 95.5224, Val Loss: 0.6243, Val Acc: 69.6970\n",
      "Val Loss: 0.818331241607666, Val Acc: 38.63636363636363\n",
      "Epoch 42701/50000, Tr Loss: 0.2033, Tr Acc: 95.5224, Val Loss: 0.6258, Val Acc: 69.6970\n",
      "Val Loss: 0.8515468835830688, Val Acc: 38.63636363636363\n",
      "Epoch 42801/50000, Tr Loss: 0.2016, Tr Acc: 97.0149, Val Loss: 0.6255, Val Acc: 69.6970\n",
      "Val Loss: 0.8848517835140228, Val Acc: 38.63636363636363\n",
      "Epoch 42901/50000, Tr Loss: 0.2058, Tr Acc: 94.0298, Val Loss: 0.6267, Val Acc: 69.6970\n",
      "Val Loss: 0.8922926485538483, Val Acc: 38.63636363636363\n",
      "Epoch 43001/50000, Tr Loss: 0.2109, Tr Acc: 92.5373, Val Loss: 0.6289, Val Acc: 69.6970\n",
      "Val Loss: 0.8238858580589294, Val Acc: 38.63636363636363\n",
      "Epoch 43101/50000, Tr Loss: 0.2028, Tr Acc: 95.5224, Val Loss: 0.6289, Val Acc: 69.6970\n",
      "Val Loss: 0.7917830646038055, Val Acc: 38.63636363636363\n",
      "Epoch 43201/50000, Tr Loss: 0.2046, Tr Acc: 95.5224, Val Loss: 0.6301, Val Acc: 69.6970\n",
      "Val Loss: 0.8333533704280853, Val Acc: 38.63636363636363\n",
      "Epoch 43301/50000, Tr Loss: 0.1974, Tr Acc: 97.0149, Val Loss: 0.6308, Val Acc: 69.6970\n",
      "Val Loss: 0.9045094549655914, Val Acc: 38.63636363636363\n",
      "Epoch 43401/50000, Tr Loss: 0.1945, Tr Acc: 97.0149, Val Loss: 0.6300, Val Acc: 69.6970\n",
      "Val Loss: 0.8789127171039581, Val Acc: 38.63636363636363\n",
      "Epoch 43501/50000, Tr Loss: 0.2101, Tr Acc: 91.0448, Val Loss: 0.6321, Val Acc: 69.6970\n",
      "Val Loss: 0.8582603335380554, Val Acc: 38.63636363636363\n",
      "Epoch 43601/50000, Tr Loss: 0.2154, Tr Acc: 94.0298, Val Loss: 0.6334, Val Acc: 69.6970\n",
      "Val Loss: 0.9067506790161133, Val Acc: 38.63636363636363\n",
      "Epoch 43701/50000, Tr Loss: 0.2296, Tr Acc: 89.5522, Val Loss: 0.6321, Val Acc: 69.6970\n",
      "Val Loss: 0.9028541147708893, Val Acc: 38.63636363636363\n",
      "Epoch 43801/50000, Tr Loss: 0.1996, Tr Acc: 91.0448, Val Loss: 0.6332, Val Acc: 69.6970\n",
      "Val Loss: 0.8305083215236664, Val Acc: 38.63636363636363\n",
      "Epoch 43901/50000, Tr Loss: 0.1978, Tr Acc: 95.5224, Val Loss: 0.6343, Val Acc: 69.6970\n",
      "Val Loss: 0.8900327086448669, Val Acc: 38.63636363636363\n",
      "Epoch 44001/50000, Tr Loss: 0.1852, Tr Acc: 95.5224, Val Loss: 0.6333, Val Acc: 69.6970\n",
      "Val Loss: 0.8264156579971313, Val Acc: 38.63636363636363\n",
      "Epoch 44101/50000, Tr Loss: 0.1960, Tr Acc: 97.0149, Val Loss: 0.6346, Val Acc: 69.6970\n",
      "Val Loss: 0.8533036410808563, Val Acc: 38.63636363636363\n",
      "Epoch 44201/50000, Tr Loss: 0.1961, Tr Acc: 95.5224, Val Loss: 0.6354, Val Acc: 69.6970\n",
      "Val Loss: 0.8098831176757812, Val Acc: 38.63636363636363\n",
      "Epoch 44301/50000, Tr Loss: 0.1993, Tr Acc: 95.5224, Val Loss: 0.6374, Val Acc: 69.6970\n",
      "Val Loss: 0.7932053506374359, Val Acc: 38.63636363636363\n",
      "Epoch 44401/50000, Tr Loss: 0.1889, Tr Acc: 95.5224, Val Loss: 0.6371, Val Acc: 69.6970\n",
      "Val Loss: 0.8128699362277985, Val Acc: 38.63636363636363\n",
      "Epoch 44501/50000, Tr Loss: 0.2208, Tr Acc: 92.5373, Val Loss: 0.6367, Val Acc: 69.6970\n",
      "Val Loss: 0.845291256904602, Val Acc: 38.63636363636363\n",
      "Epoch 44601/50000, Tr Loss: 0.1968, Tr Acc: 95.5224, Val Loss: 0.6374, Val Acc: 69.6970\n",
      "Val Loss: 0.8399680554866791, Val Acc: 38.63636363636363\n",
      "Epoch 44701/50000, Tr Loss: 0.1911, Tr Acc: 95.5224, Val Loss: 0.6380, Val Acc: 69.6970\n",
      "Val Loss: 0.8562611639499664, Val Acc: 38.63636363636363\n",
      "Epoch 44801/50000, Tr Loss: 0.2023, Tr Acc: 95.5224, Val Loss: 0.6400, Val Acc: 69.6970\n",
      "Val Loss: 0.7896835207939148, Val Acc: 38.63636363636363\n",
      "Epoch 44901/50000, Tr Loss: 0.1933, Tr Acc: 92.5373, Val Loss: 0.6398, Val Acc: 69.6970\n",
      "Val Loss: 0.8894383013248444, Val Acc: 38.63636363636363\n",
      "Epoch 45001/50000, Tr Loss: 0.1989, Tr Acc: 94.0298, Val Loss: 0.6412, Val Acc: 69.6970\n",
      "Val Loss: 0.8971186876296997, Val Acc: 38.63636363636363\n",
      "Epoch 45101/50000, Tr Loss: 0.1929, Tr Acc: 95.5224, Val Loss: 0.6401, Val Acc: 69.6970\n",
      "Val Loss: 0.8596959412097931, Val Acc: 38.63636363636363\n",
      "Epoch 45201/50000, Tr Loss: 0.2123, Tr Acc: 95.5224, Val Loss: 0.6416, Val Acc: 69.6970\n",
      "Val Loss: 0.8154290318489075, Val Acc: 38.63636363636363\n",
      "Epoch 45301/50000, Tr Loss: 0.2050, Tr Acc: 92.5373, Val Loss: 0.6422, Val Acc: 69.6970\n",
      "Val Loss: 0.8028215169906616, Val Acc: 38.63636363636363\n",
      "Epoch 45401/50000, Tr Loss: 0.2043, Tr Acc: 92.5373, Val Loss: 0.6427, Val Acc: 69.6970\n",
      "Val Loss: 0.9078422784805298, Val Acc: 38.63636363636363\n",
      "Epoch 45501/50000, Tr Loss: 0.2015, Tr Acc: 92.5373, Val Loss: 0.6431, Val Acc: 69.6970\n",
      "Val Loss: 0.9320069253444672, Val Acc: 38.63636363636363\n",
      "Epoch 45601/50000, Tr Loss: 0.1955, Tr Acc: 95.5224, Val Loss: 0.6435, Val Acc: 69.6970\n",
      "Val Loss: 0.8674175441265106, Val Acc: 38.63636363636363\n",
      "Epoch 45701/50000, Tr Loss: 0.1997, Tr Acc: 95.5224, Val Loss: 0.6450, Val Acc: 69.6970\n",
      "Val Loss: 0.7823324203491211, Val Acc: 38.63636363636363\n",
      "Epoch 45801/50000, Tr Loss: 0.1806, Tr Acc: 95.5224, Val Loss: 0.6441, Val Acc: 69.6970\n",
      "Val Loss: 0.8976216912269592, Val Acc: 38.63636363636363\n",
      "Epoch 45901/50000, Tr Loss: 0.1852, Tr Acc: 98.5075, Val Loss: 0.6442, Val Acc: 69.6970\n",
      "Val Loss: 0.8764298856258392, Val Acc: 38.63636363636363\n",
      "Epoch 46001/50000, Tr Loss: 0.1982, Tr Acc: 92.5373, Val Loss: 0.6451, Val Acc: 69.6970\n",
      "Val Loss: 0.8703233003616333, Val Acc: 38.63636363636363\n",
      "Epoch 46101/50000, Tr Loss: 0.2009, Tr Acc: 94.0298, Val Loss: 0.6468, Val Acc: 69.6970\n",
      "Val Loss: 0.8297739028930664, Val Acc: 38.63636363636363\n",
      "Epoch 46201/50000, Tr Loss: 0.2030, Tr Acc: 92.5373, Val Loss: 0.6467, Val Acc: 69.6970\n",
      "Val Loss: 0.8796034753322601, Val Acc: 38.63636363636363\n",
      "Epoch 46301/50000, Tr Loss: 0.1839, Tr Acc: 95.5224, Val Loss: 0.6473, Val Acc: 69.6970\n",
      "Val Loss: 0.8575266599655151, Val Acc: 38.63636363636363\n",
      "Epoch 46401/50000, Tr Loss: 0.1869, Tr Acc: 100.0000, Val Loss: 0.6480, Val Acc: 69.6970\n",
      "Val Loss: 0.9026314914226532, Val Acc: 38.63636363636363\n",
      "Epoch 46501/50000, Tr Loss: 0.1899, Tr Acc: 92.5373, Val Loss: 0.6486, Val Acc: 69.6970\n",
      "Val Loss: 0.9048552811145782, Val Acc: 38.63636363636363\n",
      "Epoch 46601/50000, Tr Loss: 0.1868, Tr Acc: 95.5224, Val Loss: 0.6487, Val Acc: 69.6970\n",
      "Val Loss: 0.9056149423122406, Val Acc: 38.63636363636363\n",
      "Epoch 46701/50000, Tr Loss: 0.1844, Tr Acc: 97.0149, Val Loss: 0.6500, Val Acc: 69.6970\n",
      "Val Loss: 0.9092155992984772, Val Acc: 38.63636363636363\n",
      "Epoch 46801/50000, Tr Loss: 0.1929, Tr Acc: 94.0298, Val Loss: 0.6495, Val Acc: 69.6970\n",
      "Val Loss: 0.86785027384758, Val Acc: 38.63636363636363\n",
      "Epoch 46901/50000, Tr Loss: 0.1920, Tr Acc: 98.5075, Val Loss: 0.6510, Val Acc: 69.6970\n",
      "Val Loss: 0.7864592969417572, Val Acc: 38.63636363636363\n",
      "Epoch 47001/50000, Tr Loss: 0.1868, Tr Acc: 97.0149, Val Loss: 0.6507, Val Acc: 69.6970\n",
      "Val Loss: 0.9235308468341827, Val Acc: 38.63636363636363\n",
      "Epoch 47101/50000, Tr Loss: 0.1993, Tr Acc: 98.5075, Val Loss: 0.6524, Val Acc: 69.6970\n",
      "Val Loss: 0.9482241868972778, Val Acc: 38.63636363636363\n",
      "Epoch 47201/50000, Tr Loss: 0.1964, Tr Acc: 95.5224, Val Loss: 0.6516, Val Acc: 69.6970\n",
      "Val Loss: 0.8241032361984253, Val Acc: 38.63636363636363\n",
      "Epoch 47301/50000, Tr Loss: 0.2079, Tr Acc: 92.5373, Val Loss: 0.6522, Val Acc: 69.6970\n",
      "Val Loss: 0.804025411605835, Val Acc: 38.63636363636363\n",
      "Epoch 47401/50000, Tr Loss: 0.1907, Tr Acc: 94.0298, Val Loss: 0.6530, Val Acc: 69.6970\n",
      "Val Loss: 0.883901447057724, Val Acc: 38.63636363636363\n",
      "Epoch 47501/50000, Tr Loss: 0.2003, Tr Acc: 95.5224, Val Loss: 0.6541, Val Acc: 69.6970\n",
      "Val Loss: 0.8142437040805817, Val Acc: 38.63636363636363\n",
      "Epoch 47601/50000, Tr Loss: 0.2005, Tr Acc: 92.5373, Val Loss: 0.6551, Val Acc: 69.6970\n",
      "Val Loss: 0.8621821105480194, Val Acc: 38.63636363636363\n",
      "Epoch 47701/50000, Tr Loss: 0.1881, Tr Acc: 95.5224, Val Loss: 0.6544, Val Acc: 69.6970\n",
      "Val Loss: 0.9247589111328125, Val Acc: 38.63636363636363\n",
      "Epoch 47801/50000, Tr Loss: 0.1970, Tr Acc: 94.0298, Val Loss: 0.6539, Val Acc: 69.6970\n",
      "Val Loss: 0.9485864639282227, Val Acc: 38.63636363636363\n",
      "Epoch 47901/50000, Tr Loss: 0.2045, Tr Acc: 92.5373, Val Loss: 0.6556, Val Acc: 69.6970\n",
      "Val Loss: 0.8631861507892609, Val Acc: 38.63636363636363\n",
      "Epoch 48001/50000, Tr Loss: 0.1952, Tr Acc: 91.0448, Val Loss: 0.6581, Val Acc: 69.6970\n",
      "Val Loss: 0.9337836802005768, Val Acc: 38.63636363636363\n",
      "Epoch 48101/50000, Tr Loss: 0.1794, Tr Acc: 94.0298, Val Loss: 0.6564, Val Acc: 69.6970\n",
      "Val Loss: 0.9180919528007507, Val Acc: 38.63636363636363\n",
      "Epoch 48201/50000, Tr Loss: 0.1823, Tr Acc: 97.0149, Val Loss: 0.6549, Val Acc: 69.6970\n",
      "Val Loss: 0.786717027425766, Val Acc: 38.63636363636363\n",
      "Epoch 48301/50000, Tr Loss: 0.1997, Tr Acc: 95.5224, Val Loss: 0.6584, Val Acc: 69.6970\n",
      "Val Loss: 0.8486872017383575, Val Acc: 38.63636363636363\n",
      "Epoch 48401/50000, Tr Loss: 0.1906, Tr Acc: 98.5075, Val Loss: 0.6596, Val Acc: 69.6970\n",
      "Val Loss: 0.8295575082302094, Val Acc: 38.63636363636363\n",
      "Epoch 48501/50000, Tr Loss: 0.1941, Tr Acc: 97.0149, Val Loss: 0.6586, Val Acc: 69.6970\n",
      "Val Loss: 0.8781318068504333, Val Acc: 38.63636363636363\n",
      "Epoch 48601/50000, Tr Loss: 0.1728, Tr Acc: 98.5075, Val Loss: 0.6604, Val Acc: 69.6970\n",
      "Val Loss: 0.8476831614971161, Val Acc: 38.63636363636363\n",
      "Epoch 48701/50000, Tr Loss: 0.1917, Tr Acc: 97.0149, Val Loss: 0.6607, Val Acc: 69.6970\n",
      "Val Loss: 0.8643781542778015, Val Acc: 38.63636363636363\n",
      "Epoch 48801/50000, Tr Loss: 0.1912, Tr Acc: 94.0298, Val Loss: 0.6614, Val Acc: 69.6970\n",
      "Val Loss: 0.8312275409698486, Val Acc: 38.63636363636363\n",
      "Epoch 48901/50000, Tr Loss: 0.1826, Tr Acc: 98.5075, Val Loss: 0.6612, Val Acc: 69.6970\n",
      "Val Loss: 0.8788709044456482, Val Acc: 38.63636363636363\n",
      "Epoch 49001/50000, Tr Loss: 0.1830, Tr Acc: 97.0149, Val Loss: 0.6607, Val Acc: 69.6970\n",
      "Val Loss: 0.8479956686496735, Val Acc: 38.63636363636363\n",
      "Epoch 49101/50000, Tr Loss: 0.1944, Tr Acc: 95.5224, Val Loss: 0.6623, Val Acc: 69.6970\n",
      "Val Loss: 0.8521600365638733, Val Acc: 38.63636363636363\n",
      "Epoch 49201/50000, Tr Loss: 0.1944, Tr Acc: 97.0149, Val Loss: 0.6632, Val Acc: 69.6970\n",
      "Val Loss: 0.9060850441455841, Val Acc: 38.63636363636363\n",
      "Epoch 49301/50000, Tr Loss: 0.1851, Tr Acc: 97.0149, Val Loss: 0.6641, Val Acc: 69.6970\n",
      "Val Loss: 0.9880905151367188, Val Acc: 38.63636363636363\n",
      "Epoch 49401/50000, Tr Loss: 0.1860, Tr Acc: 95.5224, Val Loss: 0.6655, Val Acc: 69.6970\n",
      "Val Loss: 0.8638544082641602, Val Acc: 38.63636363636363\n",
      "Epoch 49501/50000, Tr Loss: 0.2002, Tr Acc: 95.5224, Val Loss: 0.6660, Val Acc: 69.6970\n",
      "Val Loss: 0.8421216309070587, Val Acc: 38.63636363636363\n",
      "Epoch 49601/50000, Tr Loss: 0.1901, Tr Acc: 97.0149, Val Loss: 0.6656, Val Acc: 69.6970\n",
      "Val Loss: 0.9050871729850769, Val Acc: 38.63636363636363\n",
      "Epoch 49701/50000, Tr Loss: 0.1845, Tr Acc: 94.0298, Val Loss: 0.6664, Val Acc: 69.6970\n",
      "Val Loss: 0.9102899134159088, Val Acc: 38.63636363636363\n",
      "Epoch 49801/50000, Tr Loss: 0.1764, Tr Acc: 94.0298, Val Loss: 0.6662, Val Acc: 69.6970\n",
      "Val Loss: 0.9148615598678589, Val Acc: 38.63636363636363\n",
      "Epoch 49901/50000, Tr Loss: 0.1998, Tr Acc: 92.5373, Val Loss: 0.6665, Val Acc: 69.6970\n",
      "Val Loss: 0.8249597549438477, Val Acc: 38.63636363636363\n",
      "Fold 3\n",
      "Epoch 1/50000, Tr Loss: 0.5571, Tr Acc: 80.5970, Val Loss: 0.1100, Val Acc: 100.0000\n",
      "Val Loss: 0.84226855635643, Val Acc: 38.63636363636363\n",
      "Epoch 101/50000, Tr Loss: 0.5060, Tr Acc: 77.6119, Val Loss: 0.1128, Val Acc: 100.0000\n",
      "Val Loss: 0.8425809741020203, Val Acc: 38.63636363636363\n",
      "Epoch 201/50000, Tr Loss: 0.5277, Tr Acc: 80.5970, Val Loss: 0.1129, Val Acc: 100.0000\n",
      "Val Loss: 0.9467223584651947, Val Acc: 38.63636363636363\n",
      "Epoch 301/50000, Tr Loss: 0.4913, Tr Acc: 82.0896, Val Loss: 0.1128, Val Acc: 100.0000\n",
      "Val Loss: 0.8624829649925232, Val Acc: 38.63636363636363\n",
      "Epoch 401/50000, Tr Loss: 0.4346, Tr Acc: 80.5970, Val Loss: 0.1130, Val Acc: 100.0000\n",
      "Val Loss: 0.8436374664306641, Val Acc: 38.63636363636363\n",
      "Epoch 501/50000, Tr Loss: 0.5157, Tr Acc: 85.0746, Val Loss: 0.1132, Val Acc: 100.0000\n",
      "Val Loss: 0.912924200296402, Val Acc: 38.63636363636363\n",
      "Epoch 601/50000, Tr Loss: 0.4857, Tr Acc: 83.5821, Val Loss: 0.1133, Val Acc: 100.0000\n",
      "Val Loss: 0.9620094001293182, Val Acc: 38.63636363636363\n",
      "Epoch 701/50000, Tr Loss: 0.4928, Tr Acc: 83.5821, Val Loss: 0.1135, Val Acc: 100.0000\n",
      "Val Loss: 0.9094078242778778, Val Acc: 38.63636363636363\n",
      "Epoch 801/50000, Tr Loss: 0.4188, Tr Acc: 79.1045, Val Loss: 0.1135, Val Acc: 100.0000\n",
      "Val Loss: 0.8570199012756348, Val Acc: 38.63636363636363\n",
      "Epoch 901/50000, Tr Loss: 0.3961, Tr Acc: 82.0896, Val Loss: 0.1138, Val Acc: 100.0000\n",
      "Val Loss: 0.9539275765419006, Val Acc: 38.63636363636363\n",
      "Epoch 1001/50000, Tr Loss: 0.4467, Tr Acc: 73.1343, Val Loss: 0.1141, Val Acc: 100.0000\n",
      "Val Loss: 0.9292449951171875, Val Acc: 38.63636363636363\n",
      "Epoch 1101/50000, Tr Loss: 0.4277, Tr Acc: 80.5970, Val Loss: 0.1142, Val Acc: 100.0000\n",
      "Val Loss: 0.9551087319850922, Val Acc: 38.63636363636363\n",
      "Epoch 1201/50000, Tr Loss: 0.4340, Tr Acc: 82.0896, Val Loss: 0.1143, Val Acc: 100.0000\n",
      "Val Loss: 0.8817249834537506, Val Acc: 38.63636363636363\n",
      "Epoch 1301/50000, Tr Loss: 0.4650, Tr Acc: 80.5970, Val Loss: 0.1144, Val Acc: 100.0000\n",
      "Val Loss: 0.9090181887149811, Val Acc: 38.63636363636363\n",
      "Epoch 1401/50000, Tr Loss: 0.4540, Tr Acc: 85.0746, Val Loss: 0.1147, Val Acc: 100.0000\n",
      "Val Loss: 0.9134348630905151, Val Acc: 38.63636363636363\n",
      "Epoch 1501/50000, Tr Loss: 0.3938, Tr Acc: 77.6119, Val Loss: 0.1148, Val Acc: 100.0000\n",
      "Val Loss: 0.8199509680271149, Val Acc: 38.63636363636363\n",
      "Epoch 1601/50000, Tr Loss: 0.4590, Tr Acc: 79.1045, Val Loss: 0.1153, Val Acc: 96.9697\n",
      "Val Loss: 0.8954166173934937, Val Acc: 36.36363636363637\n",
      "Epoch 1701/50000, Tr Loss: 0.3913, Tr Acc: 88.0597, Val Loss: 0.1151, Val Acc: 96.9697\n",
      "Val Loss: 0.8812195360660553, Val Acc: 36.36363636363637\n",
      "Epoch 1801/50000, Tr Loss: 0.4184, Tr Acc: 88.0597, Val Loss: 0.1153, Val Acc: 96.9697\n",
      "Val Loss: 0.8217727243900299, Val Acc: 36.36363636363637\n",
      "Epoch 1901/50000, Tr Loss: 0.4714, Tr Acc: 82.0896, Val Loss: 0.1155, Val Acc: 96.9697\n",
      "Val Loss: 0.9729723930358887, Val Acc: 34.090909090909086\n",
      "Epoch 2001/50000, Tr Loss: 0.3905, Tr Acc: 83.5821, Val Loss: 0.1158, Val Acc: 96.9697\n",
      "Val Loss: 0.8582285344600677, Val Acc: 34.090909090909086\n",
      "Epoch 2101/50000, Tr Loss: 0.3741, Tr Acc: 80.5970, Val Loss: 0.1160, Val Acc: 96.9697\n",
      "Val Loss: 0.86520054936409, Val Acc: 34.090909090909086\n",
      "Epoch 2201/50000, Tr Loss: 0.4024, Tr Acc: 83.5821, Val Loss: 0.1162, Val Acc: 96.9697\n",
      "Val Loss: 0.8911789357662201, Val Acc: 34.090909090909086\n",
      "Epoch 2301/50000, Tr Loss: 0.3526, Tr Acc: 85.0746, Val Loss: 0.1164, Val Acc: 96.9697\n",
      "Val Loss: 0.9194262623786926, Val Acc: 34.090909090909086\n",
      "Epoch 2401/50000, Tr Loss: 0.3745, Tr Acc: 82.0896, Val Loss: 0.1166, Val Acc: 96.9697\n",
      "Val Loss: 0.8269493877887726, Val Acc: 34.090909090909086\n",
      "Epoch 2501/50000, Tr Loss: 0.4194, Tr Acc: 85.0746, Val Loss: 0.1167, Val Acc: 96.9697\n",
      "Val Loss: 0.8575082421302795, Val Acc: 34.090909090909086\n",
      "Epoch 2601/50000, Tr Loss: 0.3885, Tr Acc: 82.0896, Val Loss: 0.1170, Val Acc: 96.9697\n",
      "Val Loss: 0.9403993189334869, Val Acc: 34.090909090909086\n",
      "Epoch 2701/50000, Tr Loss: 0.3361, Tr Acc: 83.5821, Val Loss: 0.1170, Val Acc: 96.9697\n",
      "Val Loss: 0.8932440280914307, Val Acc: 34.090909090909086\n",
      "Epoch 2801/50000, Tr Loss: 0.3724, Tr Acc: 80.5970, Val Loss: 0.1173, Val Acc: 96.9697\n",
      "Val Loss: 0.8690363466739655, Val Acc: 34.090909090909086\n",
      "Epoch 2901/50000, Tr Loss: 0.3605, Tr Acc: 83.5821, Val Loss: 0.1176, Val Acc: 96.9697\n",
      "Val Loss: 0.7997072637081146, Val Acc: 34.090909090909086\n",
      "Epoch 3001/50000, Tr Loss: 0.3781, Tr Acc: 82.0896, Val Loss: 0.1178, Val Acc: 96.9697\n",
      "Val Loss: 0.8941371738910675, Val Acc: 34.090909090909086\n",
      "Epoch 3101/50000, Tr Loss: 0.4058, Tr Acc: 80.5970, Val Loss: 0.1178, Val Acc: 96.9697\n",
      "Val Loss: 0.8601467311382294, Val Acc: 34.090909090909086\n",
      "Epoch 3201/50000, Tr Loss: 0.3805, Tr Acc: 83.5821, Val Loss: 0.1179, Val Acc: 96.9697\n",
      "Val Loss: 0.9465067684650421, Val Acc: 34.090909090909086\n",
      "Epoch 3301/50000, Tr Loss: 0.3695, Tr Acc: 79.1045, Val Loss: 0.1184, Val Acc: 96.9697\n",
      "Val Loss: 0.9004397690296173, Val Acc: 34.090909090909086\n",
      "Epoch 3401/50000, Tr Loss: 0.4592, Tr Acc: 79.1045, Val Loss: 0.1183, Val Acc: 96.9697\n",
      "Val Loss: 0.8745799660682678, Val Acc: 34.090909090909086\n",
      "Epoch 3501/50000, Tr Loss: 0.3774, Tr Acc: 85.0746, Val Loss: 0.1187, Val Acc: 96.9697\n",
      "Val Loss: 0.8639008700847626, Val Acc: 34.090909090909086\n",
      "Epoch 3601/50000, Tr Loss: 0.4137, Tr Acc: 86.5672, Val Loss: 0.1188, Val Acc: 96.9697\n",
      "Val Loss: 0.9325660765171051, Val Acc: 34.090909090909086\n",
      "Epoch 3701/50000, Tr Loss: 0.3515, Tr Acc: 82.0896, Val Loss: 0.1189, Val Acc: 96.9697\n",
      "Val Loss: 0.8617425262928009, Val Acc: 34.090909090909086\n",
      "Epoch 3801/50000, Tr Loss: 0.3817, Tr Acc: 76.1194, Val Loss: 0.1191, Val Acc: 96.9697\n",
      "Val Loss: 0.8685974478721619, Val Acc: 34.090909090909086\n",
      "Epoch 3901/50000, Tr Loss: 0.4004, Tr Acc: 82.0896, Val Loss: 0.1194, Val Acc: 96.9697\n",
      "Val Loss: 0.8473396301269531, Val Acc: 34.090909090909086\n",
      "Epoch 4001/50000, Tr Loss: 0.3646, Tr Acc: 80.5970, Val Loss: 0.1198, Val Acc: 96.9697\n",
      "Val Loss: 0.8041276931762695, Val Acc: 34.090909090909086\n",
      "Epoch 4101/50000, Tr Loss: 0.3608, Tr Acc: 80.5970, Val Loss: 0.1199, Val Acc: 96.9697\n",
      "Val Loss: 0.8923209011554718, Val Acc: 34.090909090909086\n",
      "Epoch 4201/50000, Tr Loss: 0.4047, Tr Acc: 79.1045, Val Loss: 0.1199, Val Acc: 96.9697\n",
      "Val Loss: 0.907119870185852, Val Acc: 34.090909090909086\n",
      "Epoch 4301/50000, Tr Loss: 0.3528, Tr Acc: 86.5672, Val Loss: 0.1202, Val Acc: 96.9697\n",
      "Val Loss: 0.8431105613708496, Val Acc: 34.090909090909086\n",
      "Epoch 4401/50000, Tr Loss: 0.3608, Tr Acc: 80.5970, Val Loss: 0.1202, Val Acc: 96.9697\n",
      "Val Loss: 0.8302407562732697, Val Acc: 36.36363636363637\n",
      "Epoch 4501/50000, Tr Loss: 0.3962, Tr Acc: 80.5970, Val Loss: 0.1205, Val Acc: 96.9697\n",
      "Val Loss: 0.9228807687759399, Val Acc: 36.36363636363637\n",
      "Epoch 4601/50000, Tr Loss: 0.3405, Tr Acc: 80.5970, Val Loss: 0.1206, Val Acc: 96.9697\n",
      "Val Loss: 0.8981010019779205, Val Acc: 36.36363636363637\n",
      "Epoch 4701/50000, Tr Loss: 0.3722, Tr Acc: 82.0896, Val Loss: 0.1208, Val Acc: 96.9697\n",
      "Val Loss: 0.8292683959007263, Val Acc: 36.36363636363637\n",
      "Epoch 4801/50000, Tr Loss: 0.3811, Tr Acc: 79.1045, Val Loss: 0.1210, Val Acc: 96.9697\n",
      "Val Loss: 0.8826491832733154, Val Acc: 36.36363636363637\n",
      "Epoch 4901/50000, Tr Loss: 0.3387, Tr Acc: 82.0896, Val Loss: 0.1212, Val Acc: 96.9697\n",
      "Val Loss: 0.9230282008647919, Val Acc: 36.36363636363637\n",
      "Epoch 5001/50000, Tr Loss: 0.3771, Tr Acc: 83.5821, Val Loss: 0.1214, Val Acc: 96.9697\n",
      "Val Loss: 0.8355889618396759, Val Acc: 36.36363636363637\n",
      "Epoch 5101/50000, Tr Loss: 0.3604, Tr Acc: 80.5970, Val Loss: 0.1216, Val Acc: 96.9697\n",
      "Val Loss: 0.8711771965026855, Val Acc: 36.36363636363637\n",
      "Epoch 5201/50000, Tr Loss: 0.3976, Tr Acc: 79.1045, Val Loss: 0.1218, Val Acc: 96.9697\n",
      "Val Loss: 0.8349452912807465, Val Acc: 36.36363636363637\n",
      "Epoch 5301/50000, Tr Loss: 0.3581, Tr Acc: 79.1045, Val Loss: 0.1218, Val Acc: 96.9697\n",
      "Val Loss: 0.9259425103664398, Val Acc: 36.36363636363637\n",
      "Epoch 5401/50000, Tr Loss: 0.3703, Tr Acc: 85.0746, Val Loss: 0.1219, Val Acc: 96.9697\n",
      "Val Loss: 0.8954166769981384, Val Acc: 36.36363636363637\n",
      "Epoch 5501/50000, Tr Loss: 0.3796, Tr Acc: 77.6119, Val Loss: 0.1221, Val Acc: 96.9697\n",
      "Val Loss: 0.9711327850818634, Val Acc: 38.63636363636363\n",
      "Epoch 5601/50000, Tr Loss: 0.3522, Tr Acc: 82.0896, Val Loss: 0.1223, Val Acc: 96.9697\n",
      "Val Loss: 0.8464625477790833, Val Acc: 36.36363636363637\n",
      "Epoch 5701/50000, Tr Loss: 0.3579, Tr Acc: 82.0896, Val Loss: 0.1225, Val Acc: 96.9697\n",
      "Val Loss: 0.8496896326541901, Val Acc: 38.63636363636363\n",
      "Epoch 5801/50000, Tr Loss: 0.3019, Tr Acc: 80.5970, Val Loss: 0.1224, Val Acc: 96.9697\n",
      "Val Loss: 0.8546244204044342, Val Acc: 38.63636363636363\n",
      "Epoch 5901/50000, Tr Loss: 0.3457, Tr Acc: 85.0746, Val Loss: 0.1228, Val Acc: 96.9697\n",
      "Val Loss: 0.9594073295593262, Val Acc: 38.63636363636363\n",
      "Epoch 6001/50000, Tr Loss: 0.3474, Tr Acc: 85.0746, Val Loss: 0.1230, Val Acc: 96.9697\n",
      "Val Loss: 0.9719844460487366, Val Acc: 38.63636363636363\n",
      "Epoch 6101/50000, Tr Loss: 0.3394, Tr Acc: 82.0896, Val Loss: 0.1232, Val Acc: 96.9697\n",
      "Val Loss: 0.9290842711925507, Val Acc: 38.63636363636363\n",
      "Epoch 6201/50000, Tr Loss: 0.3357, Tr Acc: 86.5672, Val Loss: 0.1230, Val Acc: 96.9697\n",
      "Val Loss: 0.8793591260910034, Val Acc: 38.63636363636363\n",
      "Epoch 6301/50000, Tr Loss: 0.3618, Tr Acc: 80.5970, Val Loss: 0.1234, Val Acc: 96.9697\n",
      "Val Loss: 0.8999041616916656, Val Acc: 38.63636363636363\n",
      "Epoch 6401/50000, Tr Loss: 0.3309, Tr Acc: 82.0896, Val Loss: 0.1238, Val Acc: 96.9697\n",
      "Val Loss: 0.8750109672546387, Val Acc: 38.63636363636363\n",
      "Epoch 6501/50000, Tr Loss: 0.3561, Tr Acc: 80.5970, Val Loss: 0.1237, Val Acc: 96.9697\n",
      "Val Loss: 0.9595854878425598, Val Acc: 38.63636363636363\n",
      "Epoch 6601/50000, Tr Loss: 0.3543, Tr Acc: 82.0896, Val Loss: 0.1237, Val Acc: 96.9697\n",
      "Val Loss: 0.9156623184680939, Val Acc: 40.909090909090914\n",
      "Epoch 6701/50000, Tr Loss: 0.3514, Tr Acc: 76.1194, Val Loss: 0.1238, Val Acc: 96.9697\n",
      "Val Loss: 0.8839954137802124, Val Acc: 40.909090909090914\n",
      "Epoch 6801/50000, Tr Loss: 0.3368, Tr Acc: 86.5672, Val Loss: 0.1241, Val Acc: 96.9697\n",
      "Val Loss: 0.8568141758441925, Val Acc: 40.909090909090914\n",
      "Epoch 6901/50000, Tr Loss: 0.3550, Tr Acc: 77.6119, Val Loss: 0.1244, Val Acc: 96.9697\n",
      "Val Loss: 0.9644821882247925, Val Acc: 40.909090909090914\n",
      "Epoch 7001/50000, Tr Loss: 0.2988, Tr Acc: 83.5821, Val Loss: 0.1243, Val Acc: 96.9697\n",
      "Val Loss: 1.009619653224945, Val Acc: 43.18181818181818\n",
      "Epoch 7101/50000, Tr Loss: 0.3364, Tr Acc: 77.6119, Val Loss: 0.1246, Val Acc: 96.9697\n",
      "Val Loss: 0.8407040238380432, Val Acc: 43.18181818181818\n",
      "Epoch 7201/50000, Tr Loss: 0.3518, Tr Acc: 80.5970, Val Loss: 0.1247, Val Acc: 96.9697\n",
      "Val Loss: 0.8527274131774902, Val Acc: 43.18181818181818\n",
      "Epoch 7301/50000, Tr Loss: 0.3417, Tr Acc: 80.5970, Val Loss: 0.1248, Val Acc: 96.9697\n",
      "Val Loss: 0.8839033842086792, Val Acc: 43.18181818181818\n",
      "Epoch 7401/50000, Tr Loss: 0.3278, Tr Acc: 80.5970, Val Loss: 0.1251, Val Acc: 96.9697\n",
      "Val Loss: 0.9823637306690216, Val Acc: 43.18181818181818\n",
      "Epoch 7501/50000, Tr Loss: 0.3451, Tr Acc: 80.5970, Val Loss: 0.1250, Val Acc: 96.9697\n",
      "Val Loss: 0.8132891654968262, Val Acc: 43.18181818181818\n",
      "Epoch 7601/50000, Tr Loss: 0.3653, Tr Acc: 83.5821, Val Loss: 0.1250, Val Acc: 96.9697\n",
      "Val Loss: 0.8543463349342346, Val Acc: 43.18181818181818\n",
      "Epoch 7701/50000, Tr Loss: 0.2908, Tr Acc: 85.0746, Val Loss: 0.1252, Val Acc: 96.9697\n",
      "Val Loss: 0.8810716569423676, Val Acc: 43.18181818181818\n",
      "Epoch 7801/50000, Tr Loss: 0.3754, Tr Acc: 76.1194, Val Loss: 0.1251, Val Acc: 96.9697\n",
      "Val Loss: 0.9109183847904205, Val Acc: 43.18181818181818\n",
      "Epoch 7901/50000, Tr Loss: 0.3330, Tr Acc: 79.1045, Val Loss: 0.1254, Val Acc: 96.9697\n",
      "Val Loss: 0.9503563642501831, Val Acc: 43.18181818181818\n",
      "Epoch 8001/50000, Tr Loss: 0.3313, Tr Acc: 82.0896, Val Loss: 0.1254, Val Acc: 96.9697\n",
      "Val Loss: 0.9477565884590149, Val Acc: 43.18181818181818\n",
      "Epoch 8101/50000, Tr Loss: 0.3489, Tr Acc: 82.0896, Val Loss: 0.1255, Val Acc: 96.9697\n",
      "Val Loss: 0.8558253645896912, Val Acc: 43.18181818181818\n",
      "Epoch 8201/50000, Tr Loss: 0.3181, Tr Acc: 86.5672, Val Loss: 0.1256, Val Acc: 96.9697\n",
      "Val Loss: 0.8360064327716827, Val Acc: 45.45454545454545\n",
      "Epoch 8301/50000, Tr Loss: 0.2937, Tr Acc: 77.6119, Val Loss: 0.1258, Val Acc: 96.9697\n",
      "Val Loss: 0.8824856579303741, Val Acc: 43.18181818181818\n",
      "Epoch 8401/50000, Tr Loss: 0.3142, Tr Acc: 80.5970, Val Loss: 0.1258, Val Acc: 96.9697\n",
      "Val Loss: 0.9163699448108673, Val Acc: 45.45454545454545\n",
      "Epoch 8501/50000, Tr Loss: 0.3498, Tr Acc: 79.1045, Val Loss: 0.1258, Val Acc: 96.9697\n",
      "Val Loss: 0.8937527537345886, Val Acc: 45.45454545454545\n",
      "Epoch 8601/50000, Tr Loss: 0.3119, Tr Acc: 82.0896, Val Loss: 0.1261, Val Acc: 96.9697\n",
      "Val Loss: 0.8478196561336517, Val Acc: 45.45454545454545\n",
      "Epoch 8701/50000, Tr Loss: 0.3129, Tr Acc: 85.0746, Val Loss: 0.1261, Val Acc: 96.9697\n",
      "Val Loss: 0.8595729470252991, Val Acc: 45.45454545454545\n",
      "Epoch 8801/50000, Tr Loss: 0.3104, Tr Acc: 80.5970, Val Loss: 0.1262, Val Acc: 96.9697\n",
      "Val Loss: 0.8586074709892273, Val Acc: 45.45454545454545\n",
      "Epoch 8901/50000, Tr Loss: 0.3304, Tr Acc: 79.1045, Val Loss: 0.1266, Val Acc: 96.9697\n",
      "Val Loss: 0.8394761681556702, Val Acc: 45.45454545454545\n",
      "Epoch 9001/50000, Tr Loss: 0.3220, Tr Acc: 83.5821, Val Loss: 0.1265, Val Acc: 96.9697\n",
      "Val Loss: 0.8720590174198151, Val Acc: 45.45454545454545\n",
      "Epoch 9101/50000, Tr Loss: 0.2836, Tr Acc: 82.0896, Val Loss: 0.1266, Val Acc: 96.9697\n",
      "Val Loss: 0.9155156910419464, Val Acc: 45.45454545454545\n",
      "Epoch 9201/50000, Tr Loss: 0.2950, Tr Acc: 82.0896, Val Loss: 0.1266, Val Acc: 96.9697\n",
      "Val Loss: 0.9022641181945801, Val Acc: 45.45454545454545\n",
      "Epoch 9301/50000, Tr Loss: 0.3074, Tr Acc: 79.1045, Val Loss: 0.1266, Val Acc: 96.9697\n",
      "Val Loss: 0.9549556076526642, Val Acc: 47.72727272727273\n",
      "Epoch 9401/50000, Tr Loss: 0.3063, Tr Acc: 77.6119, Val Loss: 0.1270, Val Acc: 96.9697\n",
      "Val Loss: 0.9476884007453918, Val Acc: 47.72727272727273\n",
      "Epoch 9501/50000, Tr Loss: 0.3271, Tr Acc: 79.1045, Val Loss: 0.1269, Val Acc: 96.9697\n",
      "Val Loss: 0.9738999605178833, Val Acc: 47.72727272727273\n",
      "Epoch 9601/50000, Tr Loss: 0.3244, Tr Acc: 82.0896, Val Loss: 0.1270, Val Acc: 96.9697\n",
      "Val Loss: 0.882402092218399, Val Acc: 47.72727272727273\n",
      "Epoch 9701/50000, Tr Loss: 0.3062, Tr Acc: 80.5970, Val Loss: 0.1270, Val Acc: 96.9697\n",
      "Val Loss: 0.9758321046829224, Val Acc: 47.72727272727273\n",
      "Epoch 9801/50000, Tr Loss: 0.3458, Tr Acc: 83.5821, Val Loss: 0.1271, Val Acc: 96.9697\n",
      "Val Loss: 0.935632973909378, Val Acc: 47.72727272727273\n",
      "Epoch 9901/50000, Tr Loss: 0.3343, Tr Acc: 82.0896, Val Loss: 0.1271, Val Acc: 96.9697\n",
      "Val Loss: 0.954887717962265, Val Acc: 47.72727272727273\n",
      "Epoch 10001/50000, Tr Loss: 0.3084, Tr Acc: 86.5672, Val Loss: 0.1273, Val Acc: 96.9697\n",
      "Val Loss: 0.9240373373031616, Val Acc: 47.72727272727273\n",
      "Epoch 10101/50000, Tr Loss: 0.3507, Tr Acc: 85.0746, Val Loss: 0.1274, Val Acc: 96.9697\n",
      "Val Loss: 0.861120194196701, Val Acc: 47.72727272727273\n",
      "Epoch 10201/50000, Tr Loss: 0.3750, Tr Acc: 79.1045, Val Loss: 0.1275, Val Acc: 96.9697\n",
      "Val Loss: 0.86060431599617, Val Acc: 47.72727272727273\n",
      "Epoch 10301/50000, Tr Loss: 0.2951, Tr Acc: 83.5821, Val Loss: 0.1275, Val Acc: 96.9697\n",
      "Val Loss: 0.9028549194335938, Val Acc: 47.72727272727273\n",
      "Epoch 10401/50000, Tr Loss: 0.3087, Tr Acc: 77.6119, Val Loss: 0.1275, Val Acc: 96.9697\n",
      "Val Loss: 0.9608064293861389, Val Acc: 47.72727272727273\n",
      "Epoch 10501/50000, Tr Loss: 0.3657, Tr Acc: 79.1045, Val Loss: 0.1276, Val Acc: 96.9697\n",
      "Val Loss: 0.8423654139041901, Val Acc: 47.72727272727273\n",
      "Epoch 10601/50000, Tr Loss: 0.3145, Tr Acc: 80.5970, Val Loss: 0.1279, Val Acc: 96.9697\n",
      "Val Loss: 0.8429211974143982, Val Acc: 47.72727272727273\n",
      "Epoch 10701/50000, Tr Loss: 0.3167, Tr Acc: 82.0896, Val Loss: 0.1281, Val Acc: 96.9697\n",
      "Val Loss: 0.8495429754257202, Val Acc: 45.45454545454545\n",
      "Epoch 10801/50000, Tr Loss: 0.2889, Tr Acc: 82.0896, Val Loss: 0.1281, Val Acc: 96.9697\n",
      "Val Loss: 0.8017036318778992, Val Acc: 45.45454545454545\n",
      "Epoch 10901/50000, Tr Loss: 0.2904, Tr Acc: 82.0896, Val Loss: 0.1279, Val Acc: 96.9697\n",
      "Val Loss: 0.9960424602031708, Val Acc: 45.45454545454545\n",
      "Epoch 11001/50000, Tr Loss: 0.2992, Tr Acc: 85.0746, Val Loss: 0.1282, Val Acc: 96.9697\n",
      "Val Loss: 0.8638317286968231, Val Acc: 45.45454545454545\n",
      "Epoch 11101/50000, Tr Loss: 0.3108, Tr Acc: 79.1045, Val Loss: 0.1280, Val Acc: 96.9697\n",
      "Val Loss: 0.90188068151474, Val Acc: 45.45454545454545\n",
      "Epoch 11201/50000, Tr Loss: 0.3137, Tr Acc: 80.5970, Val Loss: 0.1283, Val Acc: 96.9697\n",
      "Val Loss: 0.8706307411193848, Val Acc: 45.45454545454545\n",
      "Epoch 11301/50000, Tr Loss: 0.2757, Tr Acc: 85.0746, Val Loss: 0.1284, Val Acc: 96.9697\n",
      "Val Loss: 0.8538427352905273, Val Acc: 45.45454545454545\n",
      "Epoch 11401/50000, Tr Loss: 0.3093, Tr Acc: 79.1045, Val Loss: 0.1283, Val Acc: 96.9697\n",
      "Val Loss: 0.9306322634220123, Val Acc: 45.45454545454545\n",
      "Epoch 11501/50000, Tr Loss: 0.2898, Tr Acc: 79.1045, Val Loss: 0.1286, Val Acc: 96.9697\n",
      "Val Loss: 0.9501926600933075, Val Acc: 45.45454545454545\n",
      "Epoch 11601/50000, Tr Loss: 0.3093, Tr Acc: 86.5672, Val Loss: 0.1284, Val Acc: 96.9697\n",
      "Val Loss: 0.9786048531532288, Val Acc: 45.45454545454545\n",
      "Epoch 11701/50000, Tr Loss: 0.3214, Tr Acc: 83.5821, Val Loss: 0.1286, Val Acc: 96.9697\n",
      "Val Loss: 0.9141134023666382, Val Acc: 45.45454545454545\n",
      "Epoch 11801/50000, Tr Loss: 0.3053, Tr Acc: 82.0896, Val Loss: 0.1286, Val Acc: 96.9697\n",
      "Val Loss: 0.9482788145542145, Val Acc: 45.45454545454545\n",
      "Epoch 11901/50000, Tr Loss: 0.3021, Tr Acc: 83.5821, Val Loss: 0.1287, Val Acc: 96.9697\n",
      "Val Loss: 0.8189313113689423, Val Acc: 45.45454545454545\n",
      "Epoch 12001/50000, Tr Loss: 0.3057, Tr Acc: 79.1045, Val Loss: 0.1288, Val Acc: 96.9697\n",
      "Val Loss: 0.8899265229701996, Val Acc: 45.45454545454545\n",
      "Epoch 12101/50000, Tr Loss: 0.2822, Tr Acc: 82.0896, Val Loss: 0.1289, Val Acc: 96.9697\n",
      "Val Loss: 0.934623658657074, Val Acc: 43.18181818181818\n",
      "Epoch 12201/50000, Tr Loss: 0.3040, Tr Acc: 86.5672, Val Loss: 0.1290, Val Acc: 96.9697\n",
      "Val Loss: 0.8611637949943542, Val Acc: 43.18181818181818\n",
      "Epoch 12301/50000, Tr Loss: 0.3245, Tr Acc: 83.5821, Val Loss: 0.1292, Val Acc: 96.9697\n",
      "Val Loss: 0.867961972951889, Val Acc: 43.18181818181818\n",
      "Epoch 12401/50000, Tr Loss: 0.2912, Tr Acc: 83.5821, Val Loss: 0.1288, Val Acc: 96.9697\n",
      "Val Loss: 0.8790643811225891, Val Acc: 45.45454545454545\n",
      "Epoch 12501/50000, Tr Loss: 0.2951, Tr Acc: 86.5672, Val Loss: 0.1292, Val Acc: 96.9697\n",
      "Val Loss: 0.8399762213230133, Val Acc: 43.18181818181818\n",
      "Epoch 12601/50000, Tr Loss: 0.2965, Tr Acc: 82.0896, Val Loss: 0.1292, Val Acc: 96.9697\n",
      "Val Loss: 0.8914099037647247, Val Acc: 43.18181818181818\n",
      "Epoch 12701/50000, Tr Loss: 0.2783, Tr Acc: 79.1045, Val Loss: 0.1292, Val Acc: 96.9697\n",
      "Val Loss: 1.0016513168811798, Val Acc: 43.18181818181818\n",
      "Epoch 12801/50000, Tr Loss: 0.2865, Tr Acc: 85.0746, Val Loss: 0.1292, Val Acc: 96.9697\n",
      "Val Loss: 0.9407384693622589, Val Acc: 43.18181818181818\n",
      "Epoch 12901/50000, Tr Loss: 0.2900, Tr Acc: 85.0746, Val Loss: 0.1293, Val Acc: 96.9697\n",
      "Val Loss: 0.8946477472782135, Val Acc: 43.18181818181818\n",
      "Epoch 13001/50000, Tr Loss: 0.2910, Tr Acc: 83.5821, Val Loss: 0.1292, Val Acc: 96.9697\n",
      "Val Loss: 0.8705072700977325, Val Acc: 43.18181818181818\n",
      "Epoch 13101/50000, Tr Loss: 0.2987, Tr Acc: 88.0597, Val Loss: 0.1290, Val Acc: 96.9697\n",
      "Val Loss: 0.8891012072563171, Val Acc: 43.18181818181818\n",
      "Epoch 13201/50000, Tr Loss: 0.3011, Tr Acc: 85.0746, Val Loss: 0.1295, Val Acc: 96.9697\n",
      "Val Loss: 0.8719435334205627, Val Acc: 43.18181818181818\n",
      "Epoch 13301/50000, Tr Loss: 0.3048, Tr Acc: 83.5821, Val Loss: 0.1297, Val Acc: 96.9697\n",
      "Val Loss: 0.8164324462413788, Val Acc: 43.18181818181818\n",
      "Epoch 13401/50000, Tr Loss: 0.2939, Tr Acc: 83.5821, Val Loss: 0.1298, Val Acc: 96.9697\n",
      "Val Loss: 0.8991004526615143, Val Acc: 43.18181818181818\n",
      "Epoch 13501/50000, Tr Loss: 0.2769, Tr Acc: 80.5970, Val Loss: 0.1297, Val Acc: 96.9697\n",
      "Val Loss: 0.8819081485271454, Val Acc: 43.18181818181818\n",
      "Epoch 13601/50000, Tr Loss: 0.2960, Tr Acc: 83.5821, Val Loss: 0.1298, Val Acc: 96.9697\n",
      "Val Loss: 0.9445729553699493, Val Acc: 43.18181818181818\n",
      "Epoch 13701/50000, Tr Loss: 0.2789, Tr Acc: 86.5672, Val Loss: 0.1298, Val Acc: 96.9697\n",
      "Val Loss: 0.9526209831237793, Val Acc: 43.18181818181818\n",
      "Epoch 13801/50000, Tr Loss: 0.3178, Tr Acc: 76.1194, Val Loss: 0.1299, Val Acc: 96.9697\n",
      "Val Loss: 0.84405317902565, Val Acc: 43.18181818181818\n",
      "Epoch 13901/50000, Tr Loss: 0.2789, Tr Acc: 86.5672, Val Loss: 0.1299, Val Acc: 96.9697\n",
      "Val Loss: 0.8815241456031799, Val Acc: 45.45454545454545\n",
      "Epoch 14001/50000, Tr Loss: 0.2784, Tr Acc: 83.5821, Val Loss: 0.1299, Val Acc: 96.9697\n",
      "Val Loss: 0.9119314849376678, Val Acc: 45.45454545454545\n",
      "Epoch 14101/50000, Tr Loss: 0.2444, Tr Acc: 85.0746, Val Loss: 0.1301, Val Acc: 96.9697\n",
      "Val Loss: 0.9682467579841614, Val Acc: 45.45454545454545\n",
      "Epoch 14201/50000, Tr Loss: 0.2814, Tr Acc: 85.0746, Val Loss: 0.1300, Val Acc: 96.9697\n",
      "Val Loss: 0.9769814014434814, Val Acc: 45.45454545454545\n",
      "Epoch 14301/50000, Tr Loss: 0.2705, Tr Acc: 89.5522, Val Loss: 0.1298, Val Acc: 96.9697\n",
      "Val Loss: 0.9128150045871735, Val Acc: 45.45454545454545\n",
      "Epoch 14401/50000, Tr Loss: 0.2708, Tr Acc: 86.5672, Val Loss: 0.1299, Val Acc: 96.9697\n",
      "Val Loss: 0.9469287693500519, Val Acc: 45.45454545454545\n",
      "Epoch 14501/50000, Tr Loss: 0.2839, Tr Acc: 82.0896, Val Loss: 0.1301, Val Acc: 96.9697\n",
      "Val Loss: 0.8211664259433746, Val Acc: 45.45454545454545\n",
      "Epoch 14601/50000, Tr Loss: 0.2603, Tr Acc: 88.0597, Val Loss: 0.1302, Val Acc: 96.9697\n",
      "Val Loss: 0.8180830478668213, Val Acc: 45.45454545454545\n",
      "Epoch 14701/50000, Tr Loss: 0.2766, Tr Acc: 83.5821, Val Loss: 0.1301, Val Acc: 96.9697\n",
      "Val Loss: 0.9799981117248535, Val Acc: 45.45454545454545\n",
      "Epoch 14801/50000, Tr Loss: 0.2826, Tr Acc: 86.5672, Val Loss: 0.1302, Val Acc: 96.9697\n",
      "Val Loss: 0.8163726627826691, Val Acc: 45.45454545454545\n",
      "Epoch 14901/50000, Tr Loss: 0.3027, Tr Acc: 85.0746, Val Loss: 0.1303, Val Acc: 96.9697\n",
      "Val Loss: 0.8784640431404114, Val Acc: 45.45454545454545\n",
      "Epoch 15001/50000, Tr Loss: 0.2695, Tr Acc: 86.5672, Val Loss: 0.1305, Val Acc: 96.9697\n",
      "Val Loss: 0.7740723192691803, Val Acc: 45.45454545454545\n",
      "Epoch 15101/50000, Tr Loss: 0.2877, Tr Acc: 85.0746, Val Loss: 0.1302, Val Acc: 96.9697\n",
      "Val Loss: 0.891726940870285, Val Acc: 45.45454545454545\n",
      "Epoch 15201/50000, Tr Loss: 0.2762, Tr Acc: 86.5672, Val Loss: 0.1303, Val Acc: 96.9697\n",
      "Val Loss: 0.845927357673645, Val Acc: 45.45454545454545\n",
      "Epoch 15301/50000, Tr Loss: 0.2864, Tr Acc: 88.0597, Val Loss: 0.1305, Val Acc: 96.9697\n",
      "Val Loss: 0.8509489595890045, Val Acc: 45.45454545454545\n",
      "Epoch 15401/50000, Tr Loss: 0.2624, Tr Acc: 88.0597, Val Loss: 0.1305, Val Acc: 96.9697\n",
      "Val Loss: 0.9469775557518005, Val Acc: 45.45454545454545\n",
      "Epoch 15501/50000, Tr Loss: 0.2654, Tr Acc: 85.0746, Val Loss: 0.1305, Val Acc: 96.9697\n",
      "Val Loss: 0.9116589426994324, Val Acc: 45.45454545454545\n",
      "Epoch 15601/50000, Tr Loss: 0.2792, Tr Acc: 83.5821, Val Loss: 0.1305, Val Acc: 96.9697\n",
      "Val Loss: 0.8287678360939026, Val Acc: 45.45454545454545\n",
      "Epoch 15701/50000, Tr Loss: 0.2721, Tr Acc: 86.5672, Val Loss: 0.1307, Val Acc: 96.9697\n",
      "Val Loss: 0.8654642105102539, Val Acc: 45.45454545454545\n",
      "Epoch 15801/50000, Tr Loss: 0.2736, Tr Acc: 83.5821, Val Loss: 0.1308, Val Acc: 96.9697\n",
      "Val Loss: 0.8277740478515625, Val Acc: 45.45454545454545\n",
      "Epoch 15901/50000, Tr Loss: 0.2672, Tr Acc: 89.5522, Val Loss: 0.1308, Val Acc: 96.9697\n",
      "Val Loss: 0.840907633304596, Val Acc: 45.45454545454545\n",
      "Epoch 16001/50000, Tr Loss: 0.2814, Tr Acc: 83.5821, Val Loss: 0.1310, Val Acc: 96.9697\n",
      "Val Loss: 0.9336906969547272, Val Acc: 45.45454545454545\n",
      "Epoch 16101/50000, Tr Loss: 0.2833, Tr Acc: 83.5821, Val Loss: 0.1311, Val Acc: 96.9697\n",
      "Val Loss: 0.808086097240448, Val Acc: 45.45454545454545\n",
      "Epoch 16201/50000, Tr Loss: 0.2794, Tr Acc: 82.0896, Val Loss: 0.1308, Val Acc: 96.9697\n",
      "Val Loss: 0.8392930328845978, Val Acc: 45.45454545454545\n",
      "Epoch 16301/50000, Tr Loss: 0.3168, Tr Acc: 76.1194, Val Loss: 0.1308, Val Acc: 96.9697\n",
      "Val Loss: 0.8052325248718262, Val Acc: 45.45454545454545\n",
      "Epoch 16401/50000, Tr Loss: 0.2476, Tr Acc: 88.0597, Val Loss: 0.1308, Val Acc: 96.9697\n",
      "Val Loss: 0.9119958281517029, Val Acc: 45.45454545454545\n",
      "Epoch 16501/50000, Tr Loss: 0.2475, Tr Acc: 91.0448, Val Loss: 0.1312, Val Acc: 96.9697\n",
      "Val Loss: 0.9209093451499939, Val Acc: 45.45454545454545\n",
      "Epoch 16601/50000, Tr Loss: 0.2563, Tr Acc: 88.0597, Val Loss: 0.1314, Val Acc: 96.9697\n",
      "Val Loss: 0.8036736845970154, Val Acc: 45.45454545454545\n",
      "Epoch 16701/50000, Tr Loss: 0.2582, Tr Acc: 86.5672, Val Loss: 0.1311, Val Acc: 96.9697\n",
      "Val Loss: 0.8119142949581146, Val Acc: 45.45454545454545\n",
      "Epoch 16801/50000, Tr Loss: 0.2537, Tr Acc: 88.0597, Val Loss: 0.1314, Val Acc: 96.9697\n",
      "Val Loss: 0.8017002642154694, Val Acc: 45.45454545454545\n",
      "Epoch 16901/50000, Tr Loss: 0.2437, Tr Acc: 89.5522, Val Loss: 0.1315, Val Acc: 96.9697\n",
      "Val Loss: 0.8834122121334076, Val Acc: 45.45454545454545\n",
      "Epoch 17001/50000, Tr Loss: 0.2668, Tr Acc: 86.5672, Val Loss: 0.1314, Val Acc: 96.9697\n",
      "Val Loss: 0.8286415338516235, Val Acc: 45.45454545454545\n",
      "Epoch 17101/50000, Tr Loss: 0.2477, Tr Acc: 88.0597, Val Loss: 0.1315, Val Acc: 96.9697\n",
      "Val Loss: 0.8300170004367828, Val Acc: 45.45454545454545\n",
      "Epoch 17201/50000, Tr Loss: 0.2690, Tr Acc: 88.0597, Val Loss: 0.1314, Val Acc: 96.9697\n",
      "Val Loss: 0.8628791570663452, Val Acc: 45.45454545454545\n",
      "Epoch 17301/50000, Tr Loss: 0.2949, Tr Acc: 82.0896, Val Loss: 0.1317, Val Acc: 96.9697\n",
      "Val Loss: 0.829812228679657, Val Acc: 45.45454545454545\n",
      "Epoch 17401/50000, Tr Loss: 0.2449, Tr Acc: 89.5522, Val Loss: 0.1319, Val Acc: 96.9697\n",
      "Val Loss: 0.8196818828582764, Val Acc: 45.45454545454545\n",
      "Epoch 17501/50000, Tr Loss: 0.2879, Tr Acc: 85.0746, Val Loss: 0.1316, Val Acc: 96.9697\n",
      "Val Loss: 0.9000835418701172, Val Acc: 45.45454545454545\n",
      "Epoch 17601/50000, Tr Loss: 0.2621, Tr Acc: 88.0597, Val Loss: 0.1314, Val Acc: 96.9697\n",
      "Val Loss: 0.8195178806781769, Val Acc: 45.45454545454545\n",
      "Epoch 17701/50000, Tr Loss: 0.2680, Tr Acc: 89.5522, Val Loss: 0.1317, Val Acc: 96.9697\n",
      "Val Loss: 0.7910824418067932, Val Acc: 45.45454545454545\n",
      "Epoch 17801/50000, Tr Loss: 0.2618, Tr Acc: 88.0597, Val Loss: 0.1317, Val Acc: 96.9697\n",
      "Val Loss: 0.8537133038043976, Val Acc: 45.45454545454545\n",
      "Epoch 17901/50000, Tr Loss: 0.2716, Tr Acc: 88.0597, Val Loss: 0.1319, Val Acc: 96.9697\n",
      "Val Loss: 0.8783559799194336, Val Acc: 45.45454545454545\n",
      "Epoch 18001/50000, Tr Loss: 0.2771, Tr Acc: 86.5672, Val Loss: 0.1318, Val Acc: 96.9697\n",
      "Val Loss: 0.8085325062274933, Val Acc: 45.45454545454545\n",
      "Epoch 18101/50000, Tr Loss: 0.2672, Tr Acc: 83.5821, Val Loss: 0.1319, Val Acc: 96.9697\n",
      "Val Loss: 0.7587416172027588, Val Acc: 45.45454545454545\n",
      "Epoch 18201/50000, Tr Loss: 0.2868, Tr Acc: 83.5821, Val Loss: 0.1320, Val Acc: 96.9697\n",
      "Val Loss: 0.8744492530822754, Val Acc: 45.45454545454545\n",
      "Epoch 18301/50000, Tr Loss: 0.2734, Tr Acc: 85.0746, Val Loss: 0.1321, Val Acc: 96.9697\n",
      "Val Loss: 0.8170230686664581, Val Acc: 45.45454545454545\n",
      "Epoch 18401/50000, Tr Loss: 0.2608, Tr Acc: 89.5522, Val Loss: 0.1322, Val Acc: 96.9697\n",
      "Val Loss: 0.8480986654758453, Val Acc: 45.45454545454545\n",
      "Epoch 18501/50000, Tr Loss: 0.2517, Tr Acc: 91.0448, Val Loss: 0.1324, Val Acc: 96.9697\n",
      "Val Loss: 0.8828595578670502, Val Acc: 45.45454545454545\n",
      "Epoch 18601/50000, Tr Loss: 0.2808, Tr Acc: 89.5522, Val Loss: 0.1324, Val Acc: 96.9697\n",
      "Val Loss: 0.8223588168621063, Val Acc: 45.45454545454545\n",
      "Epoch 18701/50000, Tr Loss: 0.2569, Tr Acc: 85.0746, Val Loss: 0.1325, Val Acc: 96.9697\n",
      "Val Loss: 0.8918180167675018, Val Acc: 45.45454545454545\n",
      "Epoch 18801/50000, Tr Loss: 0.2547, Tr Acc: 91.0448, Val Loss: 0.1324, Val Acc: 96.9697\n",
      "Val Loss: 0.8225300312042236, Val Acc: 45.45454545454545\n",
      "Epoch 18901/50000, Tr Loss: 0.2520, Tr Acc: 92.5373, Val Loss: 0.1326, Val Acc: 96.9697\n",
      "Val Loss: 0.8006839752197266, Val Acc: 45.45454545454545\n",
      "Epoch 19001/50000, Tr Loss: 0.2399, Tr Acc: 88.0597, Val Loss: 0.1325, Val Acc: 96.9697\n",
      "Val Loss: 0.8678498566150665, Val Acc: 45.45454545454545\n",
      "Epoch 19101/50000, Tr Loss: 0.2521, Tr Acc: 92.5373, Val Loss: 0.1325, Val Acc: 96.9697\n",
      "Val Loss: 0.7183772772550583, Val Acc: 45.45454545454545\n",
      "Epoch 19201/50000, Tr Loss: 0.2642, Tr Acc: 86.5672, Val Loss: 0.1326, Val Acc: 96.9697\n",
      "Val Loss: 0.8148027956485748, Val Acc: 45.45454545454545\n",
      "Epoch 19301/50000, Tr Loss: 0.2521, Tr Acc: 88.0597, Val Loss: 0.1326, Val Acc: 96.9697\n",
      "Val Loss: 0.8216685354709625, Val Acc: 45.45454545454545\n",
      "Epoch 19401/50000, Tr Loss: 0.2603, Tr Acc: 86.5672, Val Loss: 0.1328, Val Acc: 96.9697\n",
      "Val Loss: 0.8235476613044739, Val Acc: 45.45454545454545\n",
      "Epoch 19501/50000, Tr Loss: 0.2431, Tr Acc: 88.0597, Val Loss: 0.1326, Val Acc: 96.9697\n",
      "Val Loss: 0.8585187196731567, Val Acc: 45.45454545454545\n",
      "Epoch 19601/50000, Tr Loss: 0.2466, Tr Acc: 89.5522, Val Loss: 0.1331, Val Acc: 96.9697\n",
      "Val Loss: 0.8194299340248108, Val Acc: 45.45454545454545\n",
      "Epoch 19701/50000, Tr Loss: 0.2888, Tr Acc: 82.0896, Val Loss: 0.1331, Val Acc: 96.9697\n",
      "Val Loss: 0.889852911233902, Val Acc: 43.18181818181818\n",
      "Epoch 19801/50000, Tr Loss: 0.2496, Tr Acc: 91.0448, Val Loss: 0.1330, Val Acc: 96.9697\n",
      "Val Loss: 0.7977634370326996, Val Acc: 45.45454545454545\n",
      "Epoch 19901/50000, Tr Loss: 0.2549, Tr Acc: 92.5373, Val Loss: 0.1330, Val Acc: 96.9697\n",
      "Val Loss: 0.9100359976291656, Val Acc: 45.45454545454545\n",
      "Epoch 20001/50000, Tr Loss: 0.2354, Tr Acc: 86.5672, Val Loss: 0.1331, Val Acc: 96.9697\n",
      "Val Loss: 0.8507279753684998, Val Acc: 47.72727272727273\n",
      "Epoch 20101/50000, Tr Loss: 0.2804, Tr Acc: 86.5672, Val Loss: 0.1330, Val Acc: 96.9697\n",
      "Val Loss: 0.8110305368900299, Val Acc: 45.45454545454545\n",
      "Epoch 20201/50000, Tr Loss: 0.2564, Tr Acc: 89.5522, Val Loss: 0.1332, Val Acc: 96.9697\n",
      "Val Loss: 0.8982876241207123, Val Acc: 47.72727272727273\n",
      "Epoch 20301/50000, Tr Loss: 0.2820, Tr Acc: 85.0746, Val Loss: 0.1331, Val Acc: 96.9697\n",
      "Val Loss: 0.8619461953639984, Val Acc: 47.72727272727273\n",
      "Epoch 20401/50000, Tr Loss: 0.2617, Tr Acc: 91.0448, Val Loss: 0.1333, Val Acc: 96.9697\n",
      "Val Loss: 0.8992864489555359, Val Acc: 45.45454545454545\n",
      "Epoch 20501/50000, Tr Loss: 0.2652, Tr Acc: 89.5522, Val Loss: 0.1333, Val Acc: 96.9697\n",
      "Val Loss: 0.8115046620368958, Val Acc: 45.45454545454545\n",
      "Epoch 20601/50000, Tr Loss: 0.2445, Tr Acc: 91.0448, Val Loss: 0.1333, Val Acc: 96.9697\n",
      "Val Loss: 0.8964109718799591, Val Acc: 45.45454545454545\n",
      "Epoch 20701/50000, Tr Loss: 0.2784, Tr Acc: 86.5672, Val Loss: 0.1332, Val Acc: 96.9697\n",
      "Val Loss: 0.8105875253677368, Val Acc: 45.45454545454545\n",
      "Epoch 20801/50000, Tr Loss: 0.2479, Tr Acc: 89.5522, Val Loss: 0.1338, Val Acc: 96.9697\n",
      "Val Loss: 0.7824856042861938, Val Acc: 45.45454545454545\n",
      "Epoch 20901/50000, Tr Loss: 0.2592, Tr Acc: 88.0597, Val Loss: 0.1336, Val Acc: 96.9697\n",
      "Val Loss: 0.8562597632408142, Val Acc: 45.45454545454545\n",
      "Epoch 21001/50000, Tr Loss: 0.2560, Tr Acc: 88.0597, Val Loss: 0.1339, Val Acc: 96.9697\n",
      "Val Loss: 0.8732538521289825, Val Acc: 45.45454545454545\n",
      "Epoch 21101/50000, Tr Loss: 0.2395, Tr Acc: 88.0597, Val Loss: 0.1335, Val Acc: 96.9697\n",
      "Val Loss: 0.7860981822013855, Val Acc: 45.45454545454545\n",
      "Epoch 21201/50000, Tr Loss: 0.2601, Tr Acc: 88.0597, Val Loss: 0.1337, Val Acc: 96.9697\n",
      "Val Loss: 0.8633216619491577, Val Acc: 45.45454545454545\n",
      "Epoch 21301/50000, Tr Loss: 0.2298, Tr Acc: 89.5522, Val Loss: 0.1335, Val Acc: 96.9697\n",
      "Val Loss: 0.852489560842514, Val Acc: 45.45454545454545\n",
      "Epoch 21401/50000, Tr Loss: 0.2494, Tr Acc: 91.0448, Val Loss: 0.1338, Val Acc: 96.9697\n",
      "Val Loss: 0.814855694770813, Val Acc: 45.45454545454545\n",
      "Epoch 21501/50000, Tr Loss: 0.2443, Tr Acc: 89.5522, Val Loss: 0.1338, Val Acc: 96.9697\n",
      "Val Loss: 0.8508348166942596, Val Acc: 45.45454545454545\n",
      "Epoch 21601/50000, Tr Loss: 0.2631, Tr Acc: 86.5672, Val Loss: 0.1340, Val Acc: 96.9697\n",
      "Val Loss: 0.8010309636592865, Val Acc: 47.72727272727273\n",
      "Epoch 21701/50000, Tr Loss: 0.2496, Tr Acc: 88.0597, Val Loss: 0.1343, Val Acc: 96.9697\n",
      "Val Loss: 0.9381835758686066, Val Acc: 47.72727272727273\n",
      "Epoch 21801/50000, Tr Loss: 0.2466, Tr Acc: 83.5821, Val Loss: 0.1343, Val Acc: 96.9697\n",
      "Val Loss: 0.8760189116001129, Val Acc: 47.72727272727273\n",
      "Epoch 21901/50000, Tr Loss: 0.2442, Tr Acc: 85.0746, Val Loss: 0.1343, Val Acc: 96.9697\n",
      "Val Loss: 0.8607929348945618, Val Acc: 47.72727272727273\n",
      "Epoch 22001/50000, Tr Loss: 0.2429, Tr Acc: 86.5672, Val Loss: 0.1341, Val Acc: 96.9697\n",
      "Val Loss: 0.7727188467979431, Val Acc: 47.72727272727273\n",
      "Epoch 22101/50000, Tr Loss: 0.2377, Tr Acc: 89.5522, Val Loss: 0.1341, Val Acc: 96.9697\n",
      "Val Loss: 0.8496650159358978, Val Acc: 47.72727272727273\n",
      "Epoch 22201/50000, Tr Loss: 0.2511, Tr Acc: 88.0597, Val Loss: 0.1340, Val Acc: 96.9697\n",
      "Val Loss: 0.8543550074100494, Val Acc: 47.72727272727273\n",
      "Epoch 22301/50000, Tr Loss: 0.2929, Tr Acc: 88.0597, Val Loss: 0.1344, Val Acc: 96.9697\n",
      "Val Loss: 0.8076459169387817, Val Acc: 47.72727272727273\n",
      "Epoch 22401/50000, Tr Loss: 0.2604, Tr Acc: 85.0746, Val Loss: 0.1344, Val Acc: 96.9697\n",
      "Val Loss: 0.8849623501300812, Val Acc: 47.72727272727273\n",
      "Epoch 22501/50000, Tr Loss: 0.2528, Tr Acc: 85.0746, Val Loss: 0.1345, Val Acc: 96.9697\n",
      "Val Loss: 0.9152898788452148, Val Acc: 47.72727272727273\n",
      "Epoch 22601/50000, Tr Loss: 0.2622, Tr Acc: 91.0448, Val Loss: 0.1347, Val Acc: 96.9697\n",
      "Val Loss: 0.7797509729862213, Val Acc: 47.72727272727273\n",
      "Epoch 22701/50000, Tr Loss: 0.2489, Tr Acc: 89.5522, Val Loss: 0.1345, Val Acc: 96.9697\n",
      "Val Loss: 0.8456971347332001, Val Acc: 47.72727272727273\n",
      "Epoch 22801/50000, Tr Loss: 0.2346, Tr Acc: 91.0448, Val Loss: 0.1346, Val Acc: 96.9697\n",
      "Val Loss: 0.9492871463298798, Val Acc: 47.72727272727273\n",
      "Epoch 22901/50000, Tr Loss: 0.2523, Tr Acc: 88.0597, Val Loss: 0.1347, Val Acc: 96.9697\n",
      "Val Loss: 0.8918796181678772, Val Acc: 47.72727272727273\n",
      "Epoch 23001/50000, Tr Loss: 0.2511, Tr Acc: 88.0597, Val Loss: 0.1349, Val Acc: 96.9697\n",
      "Val Loss: 0.8077641725540161, Val Acc: 47.72727272727273\n",
      "Epoch 23101/50000, Tr Loss: 0.2323, Tr Acc: 91.0448, Val Loss: 0.1348, Val Acc: 96.9697\n",
      "Val Loss: 0.9325332641601562, Val Acc: 47.72727272727273\n",
      "Epoch 23201/50000, Tr Loss: 0.2321, Tr Acc: 91.0448, Val Loss: 0.1350, Val Acc: 96.9697\n",
      "Val Loss: 0.8788976669311523, Val Acc: 47.72727272727273\n",
      "Epoch 23301/50000, Tr Loss: 0.2558, Tr Acc: 89.5522, Val Loss: 0.1350, Val Acc: 96.9697\n",
      "Val Loss: 0.8100477755069733, Val Acc: 47.72727272727273\n",
      "Epoch 23401/50000, Tr Loss: 0.2327, Tr Acc: 88.0597, Val Loss: 0.1350, Val Acc: 96.9697\n",
      "Val Loss: 0.7840552031993866, Val Acc: 47.72727272727273\n",
      "Epoch 23501/50000, Tr Loss: 0.2873, Tr Acc: 89.5522, Val Loss: 0.1351, Val Acc: 96.9697\n",
      "Val Loss: 0.7297635823488235, Val Acc: 47.72727272727273\n",
      "Epoch 23601/50000, Tr Loss: 0.2460, Tr Acc: 88.0597, Val Loss: 0.1352, Val Acc: 96.9697\n",
      "Val Loss: 0.824587345123291, Val Acc: 47.72727272727273\n",
      "Epoch 23701/50000, Tr Loss: 0.2471, Tr Acc: 91.0448, Val Loss: 0.1352, Val Acc: 96.9697\n",
      "Val Loss: 0.841569721698761, Val Acc: 47.72727272727273\n",
      "Epoch 23801/50000, Tr Loss: 0.2665, Tr Acc: 86.5672, Val Loss: 0.1353, Val Acc: 96.9697\n",
      "Val Loss: 0.7868354618549347, Val Acc: 47.72727272727273\n",
      "Epoch 23901/50000, Tr Loss: 0.2418, Tr Acc: 91.0448, Val Loss: 0.1352, Val Acc: 96.9697\n",
      "Val Loss: 0.8441720008850098, Val Acc: 47.72727272727273\n",
      "Epoch 24001/50000, Tr Loss: 0.2221, Tr Acc: 89.5522, Val Loss: 0.1354, Val Acc: 96.9697\n",
      "Val Loss: 0.8273701071739197, Val Acc: 47.72727272727273\n",
      "Epoch 24101/50000, Tr Loss: 0.2303, Tr Acc: 91.0448, Val Loss: 0.1357, Val Acc: 96.9697\n",
      "Val Loss: 0.821902334690094, Val Acc: 47.72727272727273\n",
      "Epoch 24201/50000, Tr Loss: 0.2257, Tr Acc: 89.5522, Val Loss: 0.1356, Val Acc: 96.9697\n",
      "Val Loss: 0.7993331849575043, Val Acc: 47.72727272727273\n",
      "Epoch 24301/50000, Tr Loss: 0.2412, Tr Acc: 86.5672, Val Loss: 0.1358, Val Acc: 96.9697\n",
      "Val Loss: 0.8072851598262787, Val Acc: 47.72727272727273\n",
      "Epoch 24401/50000, Tr Loss: 0.2302, Tr Acc: 91.0448, Val Loss: 0.1358, Val Acc: 96.9697\n",
      "Val Loss: 0.7607702314853668, Val Acc: 47.72727272727273\n",
      "Epoch 24501/50000, Tr Loss: 0.2328, Tr Acc: 94.0298, Val Loss: 0.1360, Val Acc: 96.9697\n",
      "Val Loss: 0.8552566766738892, Val Acc: 45.45454545454545\n",
      "Epoch 24601/50000, Tr Loss: 0.2358, Tr Acc: 92.5373, Val Loss: 0.1359, Val Acc: 96.9697\n",
      "Val Loss: 0.7733331918716431, Val Acc: 47.72727272727273\n",
      "Epoch 24701/50000, Tr Loss: 0.2237, Tr Acc: 89.5522, Val Loss: 0.1358, Val Acc: 96.9697\n",
      "Val Loss: 0.7499858140945435, Val Acc: 47.72727272727273\n",
      "Epoch 24801/50000, Tr Loss: 0.2215, Tr Acc: 91.0448, Val Loss: 0.1361, Val Acc: 96.9697\n",
      "Val Loss: 0.8104521334171295, Val Acc: 47.72727272727273\n",
      "Epoch 24901/50000, Tr Loss: 0.2260, Tr Acc: 92.5373, Val Loss: 0.1359, Val Acc: 96.9697\n",
      "Val Loss: 0.807168036699295, Val Acc: 47.72727272727273\n",
      "Epoch 25001/50000, Tr Loss: 0.2548, Tr Acc: 89.5522, Val Loss: 0.1363, Val Acc: 96.9697\n",
      "Val Loss: 0.7919217050075531, Val Acc: 47.72727272727273\n",
      "Epoch 25101/50000, Tr Loss: 0.2262, Tr Acc: 89.5522, Val Loss: 0.1362, Val Acc: 96.9697\n",
      "Val Loss: 0.8381563723087311, Val Acc: 45.45454545454545\n",
      "Epoch 25201/50000, Tr Loss: 0.2168, Tr Acc: 92.5373, Val Loss: 0.1363, Val Acc: 96.9697\n",
      "Val Loss: 0.862642914056778, Val Acc: 45.45454545454545\n",
      "Epoch 25301/50000, Tr Loss: 0.2216, Tr Acc: 91.0448, Val Loss: 0.1363, Val Acc: 96.9697\n",
      "Val Loss: 0.7682082653045654, Val Acc: 47.72727272727273\n",
      "Epoch 25401/50000, Tr Loss: 0.2446, Tr Acc: 86.5672, Val Loss: 0.1365, Val Acc: 96.9697\n",
      "Val Loss: 0.77419513463974, Val Acc: 45.45454545454545\n",
      "Epoch 25501/50000, Tr Loss: 0.2189, Tr Acc: 92.5373, Val Loss: 0.1365, Val Acc: 96.9697\n",
      "Val Loss: 0.8938846588134766, Val Acc: 45.45454545454545\n",
      "Epoch 25601/50000, Tr Loss: 0.2316, Tr Acc: 91.0448, Val Loss: 0.1367, Val Acc: 96.9697\n",
      "Val Loss: 0.7781929969787598, Val Acc: 45.45454545454545\n",
      "Epoch 25701/50000, Tr Loss: 0.2125, Tr Acc: 92.5373, Val Loss: 0.1364, Val Acc: 96.9697\n",
      "Val Loss: 0.7314868122339249, Val Acc: 45.45454545454545\n",
      "Epoch 25801/50000, Tr Loss: 0.2254, Tr Acc: 94.0298, Val Loss: 0.1367, Val Acc: 96.9697\n",
      "Val Loss: 0.7968868911266327, Val Acc: 45.45454545454545\n",
      "Epoch 25901/50000, Tr Loss: 0.2139, Tr Acc: 92.5373, Val Loss: 0.1367, Val Acc: 96.9697\n",
      "Val Loss: 0.8436223268508911, Val Acc: 45.45454545454545\n",
      "Epoch 26001/50000, Tr Loss: 0.2316, Tr Acc: 91.0448, Val Loss: 0.1366, Val Acc: 96.9697\n",
      "Val Loss: 0.7969965636730194, Val Acc: 45.45454545454545\n",
      "Epoch 26101/50000, Tr Loss: 0.2043, Tr Acc: 89.5522, Val Loss: 0.1367, Val Acc: 96.9697\n",
      "Val Loss: 0.9542523324489594, Val Acc: 45.45454545454545\n",
      "Epoch 26201/50000, Tr Loss: 0.2252, Tr Acc: 91.0448, Val Loss: 0.1369, Val Acc: 96.9697\n",
      "Val Loss: 0.8383093774318695, Val Acc: 45.45454545454545\n",
      "Epoch 26301/50000, Tr Loss: 0.2306, Tr Acc: 86.5672, Val Loss: 0.1372, Val Acc: 96.9697\n",
      "Val Loss: 0.9756048321723938, Val Acc: 45.45454545454545\n",
      "Epoch 26401/50000, Tr Loss: 0.2137, Tr Acc: 91.0448, Val Loss: 0.1371, Val Acc: 96.9697\n",
      "Val Loss: 0.9239723980426788, Val Acc: 45.45454545454545\n",
      "Epoch 26501/50000, Tr Loss: 0.2387, Tr Acc: 89.5522, Val Loss: 0.1370, Val Acc: 96.9697\n",
      "Val Loss: 0.8751380145549774, Val Acc: 45.45454545454545\n",
      "Epoch 26601/50000, Tr Loss: 0.2234, Tr Acc: 88.0597, Val Loss: 0.1373, Val Acc: 96.9697\n",
      "Val Loss: 0.8502039611339569, Val Acc: 45.45454545454545\n",
      "Epoch 26701/50000, Tr Loss: 0.2342, Tr Acc: 92.5373, Val Loss: 0.1372, Val Acc: 96.9697\n",
      "Val Loss: 0.7887663245201111, Val Acc: 45.45454545454545\n",
      "Epoch 26801/50000, Tr Loss: 0.2310, Tr Acc: 89.5522, Val Loss: 0.1372, Val Acc: 96.9697\n",
      "Val Loss: 0.8632637858390808, Val Acc: 45.45454545454545\n",
      "Epoch 26901/50000, Tr Loss: 0.2325, Tr Acc: 91.0448, Val Loss: 0.1373, Val Acc: 96.9697\n",
      "Val Loss: 0.7883571982383728, Val Acc: 45.45454545454545\n",
      "Epoch 27001/50000, Tr Loss: 0.2301, Tr Acc: 94.0298, Val Loss: 0.1376, Val Acc: 96.9697\n",
      "Val Loss: 0.8101718723773956, Val Acc: 45.45454545454545\n",
      "Epoch 27101/50000, Tr Loss: 0.2439, Tr Acc: 91.0448, Val Loss: 0.1375, Val Acc: 96.9697\n",
      "Val Loss: 0.8142992556095123, Val Acc: 45.45454545454545\n",
      "Epoch 27201/50000, Tr Loss: 0.2619, Tr Acc: 85.0746, Val Loss: 0.1375, Val Acc: 96.9697\n",
      "Val Loss: 0.7960565686225891, Val Acc: 45.45454545454545\n",
      "Epoch 27301/50000, Tr Loss: 0.2489, Tr Acc: 85.0746, Val Loss: 0.1374, Val Acc: 96.9697\n",
      "Val Loss: 0.8408595323562622, Val Acc: 45.45454545454545\n",
      "Epoch 27401/50000, Tr Loss: 0.2136, Tr Acc: 94.0298, Val Loss: 0.1375, Val Acc: 96.9697\n",
      "Val Loss: 0.8329848647117615, Val Acc: 45.45454545454545\n",
      "Epoch 27501/50000, Tr Loss: 0.2150, Tr Acc: 89.5522, Val Loss: 0.1379, Val Acc: 96.9697\n",
      "Val Loss: 0.8005558848381042, Val Acc: 45.45454545454545\n",
      "Epoch 27601/50000, Tr Loss: 0.2226, Tr Acc: 89.5522, Val Loss: 0.1381, Val Acc: 96.9697\n",
      "Val Loss: 0.8866413831710815, Val Acc: 45.45454545454545\n",
      "Epoch 27701/50000, Tr Loss: 0.2295, Tr Acc: 89.5522, Val Loss: 0.1380, Val Acc: 96.9697\n",
      "Val Loss: 0.8743744790554047, Val Acc: 45.45454545454545\n",
      "Epoch 27801/50000, Tr Loss: 0.2256, Tr Acc: 92.5373, Val Loss: 0.1378, Val Acc: 96.9697\n",
      "Val Loss: 0.7363464832305908, Val Acc: 45.45454545454545\n",
      "Epoch 27901/50000, Tr Loss: 0.2241, Tr Acc: 91.0448, Val Loss: 0.1382, Val Acc: 96.9697\n",
      "Val Loss: 0.8641296923160553, Val Acc: 45.45454545454545\n",
      "Epoch 28001/50000, Tr Loss: 0.2429, Tr Acc: 88.0597, Val Loss: 0.1381, Val Acc: 96.9697\n",
      "Val Loss: 0.8809048533439636, Val Acc: 45.45454545454545\n",
      "Epoch 28101/50000, Tr Loss: 0.2171, Tr Acc: 91.0448, Val Loss: 0.1381, Val Acc: 96.9697\n",
      "Val Loss: 0.9126356840133667, Val Acc: 45.45454545454545\n",
      "Epoch 28201/50000, Tr Loss: 0.2127, Tr Acc: 89.5522, Val Loss: 0.1381, Val Acc: 96.9697\n",
      "Val Loss: 0.8740787506103516, Val Acc: 45.45454545454545\n",
      "Epoch 28301/50000, Tr Loss: 0.2198, Tr Acc: 91.0448, Val Loss: 0.1382, Val Acc: 96.9697\n",
      "Val Loss: 0.7848125100135803, Val Acc: 45.45454545454545\n",
      "Epoch 28401/50000, Tr Loss: 0.2160, Tr Acc: 94.0298, Val Loss: 0.1382, Val Acc: 96.9697\n",
      "Val Loss: 0.900073230266571, Val Acc: 45.45454545454545\n",
      "Epoch 28501/50000, Tr Loss: 0.2142, Tr Acc: 92.5373, Val Loss: 0.1385, Val Acc: 96.9697\n",
      "Val Loss: 0.8892748057842255, Val Acc: 45.45454545454545\n",
      "Epoch 28601/50000, Tr Loss: 0.2133, Tr Acc: 92.5373, Val Loss: 0.1385, Val Acc: 96.9697\n",
      "Val Loss: 0.8813967406749725, Val Acc: 45.45454545454545\n",
      "Epoch 28701/50000, Tr Loss: 0.2294, Tr Acc: 89.5522, Val Loss: 0.1385, Val Acc: 96.9697\n",
      "Val Loss: 0.7841098308563232, Val Acc: 45.45454545454545\n",
      "Epoch 28801/50000, Tr Loss: 0.2463, Tr Acc: 86.5672, Val Loss: 0.1384, Val Acc: 96.9697\n",
      "Val Loss: 0.906759113073349, Val Acc: 45.45454545454545\n",
      "Epoch 28901/50000, Tr Loss: 0.2384, Tr Acc: 89.5522, Val Loss: 0.1386, Val Acc: 96.9697\n",
      "Val Loss: 0.920341968536377, Val Acc: 45.45454545454545\n",
      "Epoch 29001/50000, Tr Loss: 0.2199, Tr Acc: 91.0448, Val Loss: 0.1389, Val Acc: 96.9697\n",
      "Val Loss: 0.8818157017230988, Val Acc: 45.45454545454545\n",
      "Epoch 29101/50000, Tr Loss: 0.2340, Tr Acc: 89.5522, Val Loss: 0.1390, Val Acc: 96.9697\n",
      "Val Loss: 0.848565936088562, Val Acc: 45.45454545454545\n",
      "Epoch 29201/50000, Tr Loss: 0.2070, Tr Acc: 94.0298, Val Loss: 0.1391, Val Acc: 96.9697\n",
      "Val Loss: 0.8227342665195465, Val Acc: 45.45454545454545\n",
      "Epoch 29301/50000, Tr Loss: 0.2226, Tr Acc: 89.5522, Val Loss: 0.1389, Val Acc: 96.9697\n",
      "Val Loss: 0.8617578744888306, Val Acc: 45.45454545454545\n",
      "Epoch 29401/50000, Tr Loss: 0.2246, Tr Acc: 89.5522, Val Loss: 0.1390, Val Acc: 96.9697\n",
      "Val Loss: 0.8149890005588531, Val Acc: 45.45454545454545\n",
      "Epoch 29501/50000, Tr Loss: 0.2107, Tr Acc: 92.5373, Val Loss: 0.1389, Val Acc: 96.9697\n",
      "Val Loss: 0.7570196986198425, Val Acc: 45.45454545454545\n",
      "Epoch 29601/50000, Tr Loss: 0.2164, Tr Acc: 91.0448, Val Loss: 0.1389, Val Acc: 96.9697\n",
      "Val Loss: 0.8019262552261353, Val Acc: 45.45454545454545\n",
      "Epoch 29701/50000, Tr Loss: 0.2204, Tr Acc: 91.0448, Val Loss: 0.1390, Val Acc: 96.9697\n",
      "Val Loss: 0.7897936403751373, Val Acc: 45.45454545454545\n",
      "Epoch 29801/50000, Tr Loss: 0.2227, Tr Acc: 94.0298, Val Loss: 0.1393, Val Acc: 96.9697\n",
      "Val Loss: 0.8415324985980988, Val Acc: 45.45454545454545\n",
      "Epoch 29901/50000, Tr Loss: 0.1853, Tr Acc: 92.5373, Val Loss: 0.1394, Val Acc: 96.9697\n",
      "Val Loss: 0.883041650056839, Val Acc: 45.45454545454545\n",
      "Epoch 30001/50000, Tr Loss: 0.2159, Tr Acc: 88.0597, Val Loss: 0.1394, Val Acc: 96.9697\n",
      "Val Loss: 0.9089697003364563, Val Acc: 45.45454545454545\n",
      "Epoch 30101/50000, Tr Loss: 0.2225, Tr Acc: 89.5522, Val Loss: 0.1395, Val Acc: 96.9697\n",
      "Val Loss: 0.8525824844837189, Val Acc: 45.45454545454545\n",
      "Epoch 30201/50000, Tr Loss: 0.2190, Tr Acc: 92.5373, Val Loss: 0.1393, Val Acc: 96.9697\n",
      "Val Loss: 0.7684542238712311, Val Acc: 45.45454545454545\n",
      "Epoch 30301/50000, Tr Loss: 0.2320, Tr Acc: 91.0448, Val Loss: 0.1394, Val Acc: 96.9697\n",
      "Val Loss: 0.8606144785881042, Val Acc: 45.45454545454545\n",
      "Epoch 30401/50000, Tr Loss: 0.2174, Tr Acc: 89.5522, Val Loss: 0.1396, Val Acc: 96.9697\n",
      "Val Loss: 0.8064942955970764, Val Acc: 45.45454545454545\n",
      "Epoch 30501/50000, Tr Loss: 0.2102, Tr Acc: 94.0298, Val Loss: 0.1396, Val Acc: 96.9697\n",
      "Val Loss: 0.8546459674835205, Val Acc: 45.45454545454545\n",
      "Epoch 30601/50000, Tr Loss: 0.2167, Tr Acc: 92.5373, Val Loss: 0.1397, Val Acc: 96.9697\n",
      "Val Loss: 0.8408075273036957, Val Acc: 45.45454545454545\n",
      "Epoch 30701/50000, Tr Loss: 0.2225, Tr Acc: 92.5373, Val Loss: 0.1398, Val Acc: 96.9697\n",
      "Val Loss: 0.7532261908054352, Val Acc: 45.45454545454545\n",
      "Epoch 30801/50000, Tr Loss: 0.1986, Tr Acc: 94.0298, Val Loss: 0.1399, Val Acc: 96.9697\n",
      "Val Loss: 0.8465566635131836, Val Acc: 45.45454545454545\n",
      "Epoch 30901/50000, Tr Loss: 0.2153, Tr Acc: 88.0597, Val Loss: 0.1398, Val Acc: 96.9697\n",
      "Val Loss: 0.8026977181434631, Val Acc: 45.45454545454545\n",
      "Epoch 31001/50000, Tr Loss: 0.2246, Tr Acc: 92.5373, Val Loss: 0.1401, Val Acc: 96.9697\n",
      "Val Loss: 0.8096463680267334, Val Acc: 45.45454545454545\n",
      "Epoch 31101/50000, Tr Loss: 0.2353, Tr Acc: 89.5522, Val Loss: 0.1402, Val Acc: 96.9697\n",
      "Val Loss: 0.9205621778964996, Val Acc: 45.45454545454545\n",
      "Epoch 31201/50000, Tr Loss: 0.2170, Tr Acc: 85.0746, Val Loss: 0.1402, Val Acc: 96.9697\n",
      "Val Loss: 0.8028604388237, Val Acc: 45.45454545454545\n",
      "Epoch 31301/50000, Tr Loss: 0.2111, Tr Acc: 92.5373, Val Loss: 0.1401, Val Acc: 96.9697\n",
      "Val Loss: 0.8504436314105988, Val Acc: 45.45454545454545\n",
      "Epoch 31401/50000, Tr Loss: 0.2295, Tr Acc: 91.0448, Val Loss: 0.1402, Val Acc: 96.9697\n",
      "Val Loss: 0.7368308007717133, Val Acc: 45.45454545454545\n",
      "Epoch 31501/50000, Tr Loss: 0.2103, Tr Acc: 91.0448, Val Loss: 0.1404, Val Acc: 96.9697\n",
      "Val Loss: 0.7477112710475922, Val Acc: 45.45454545454545\n",
      "Epoch 31601/50000, Tr Loss: 0.2027, Tr Acc: 92.5373, Val Loss: 0.1404, Val Acc: 96.9697\n",
      "Val Loss: 0.8953245282173157, Val Acc: 45.45454545454545\n",
      "Epoch 31701/50000, Tr Loss: 0.2145, Tr Acc: 94.0298, Val Loss: 0.1403, Val Acc: 96.9697\n",
      "Val Loss: 0.7215920835733414, Val Acc: 45.45454545454545\n",
      "Epoch 31801/50000, Tr Loss: 0.2194, Tr Acc: 91.0448, Val Loss: 0.1404, Val Acc: 96.9697\n",
      "Val Loss: 0.8096001744270325, Val Acc: 45.45454545454545\n",
      "Epoch 31901/50000, Tr Loss: 0.2019, Tr Acc: 92.5373, Val Loss: 0.1406, Val Acc: 96.9697\n",
      "Val Loss: 0.8126561343669891, Val Acc: 45.45454545454545\n",
      "Epoch 32001/50000, Tr Loss: 0.2153, Tr Acc: 91.0448, Val Loss: 0.1406, Val Acc: 96.9697\n",
      "Val Loss: 0.8616696000099182, Val Acc: 45.45454545454545\n",
      "Epoch 32101/50000, Tr Loss: 0.1925, Tr Acc: 95.5224, Val Loss: 0.1407, Val Acc: 96.9697\n",
      "Val Loss: 0.9050518274307251, Val Acc: 45.45454545454545\n",
      "Epoch 32201/50000, Tr Loss: 0.1973, Tr Acc: 92.5373, Val Loss: 0.1407, Val Acc: 96.9697\n",
      "Val Loss: 0.8486667275428772, Val Acc: 45.45454545454545\n",
      "Epoch 32301/50000, Tr Loss: 0.2001, Tr Acc: 92.5373, Val Loss: 0.1409, Val Acc: 96.9697\n",
      "Val Loss: 0.8209516406059265, Val Acc: 45.45454545454545\n",
      "Epoch 32401/50000, Tr Loss: 0.2127, Tr Acc: 95.5224, Val Loss: 0.1410, Val Acc: 96.9697\n",
      "Val Loss: 0.8732452094554901, Val Acc: 45.45454545454545\n",
      "Epoch 32501/50000, Tr Loss: 0.2272, Tr Acc: 97.0149, Val Loss: 0.1409, Val Acc: 96.9697\n",
      "Val Loss: 0.8199433088302612, Val Acc: 45.45454545454545\n",
      "Epoch 32601/50000, Tr Loss: 0.2062, Tr Acc: 94.0298, Val Loss: 0.1408, Val Acc: 96.9697\n",
      "Val Loss: 0.8368391990661621, Val Acc: 45.45454545454545\n",
      "Epoch 32701/50000, Tr Loss: 0.1966, Tr Acc: 94.0298, Val Loss: 0.1408, Val Acc: 96.9697\n",
      "Val Loss: 0.854218453168869, Val Acc: 45.45454545454545\n",
      "Epoch 32801/50000, Tr Loss: 0.2206, Tr Acc: 92.5373, Val Loss: 0.1411, Val Acc: 96.9697\n",
      "Val Loss: 0.7931528389453888, Val Acc: 45.45454545454545\n",
      "Epoch 32901/50000, Tr Loss: 0.2126, Tr Acc: 89.5522, Val Loss: 0.1412, Val Acc: 96.9697\n",
      "Val Loss: 0.8701413869857788, Val Acc: 45.45454545454545\n",
      "Epoch 33001/50000, Tr Loss: 0.1940, Tr Acc: 92.5373, Val Loss: 0.1412, Val Acc: 96.9697\n",
      "Val Loss: 0.8919739425182343, Val Acc: 45.45454545454545\n",
      "Epoch 33101/50000, Tr Loss: 0.1988, Tr Acc: 94.0298, Val Loss: 0.1412, Val Acc: 96.9697\n",
      "Val Loss: 0.8038694858551025, Val Acc: 45.45454545454545\n",
      "Epoch 33201/50000, Tr Loss: 0.2025, Tr Acc: 94.0298, Val Loss: 0.1413, Val Acc: 96.9697\n",
      "Val Loss: 0.8112307488918304, Val Acc: 45.45454545454545\n",
      "Epoch 33301/50000, Tr Loss: 0.2038, Tr Acc: 94.0298, Val Loss: 0.1415, Val Acc: 96.9697\n",
      "Val Loss: 0.8136694431304932, Val Acc: 45.45454545454545\n",
      "Epoch 33401/50000, Tr Loss: 0.2033, Tr Acc: 86.5672, Val Loss: 0.1417, Val Acc: 96.9697\n",
      "Val Loss: 0.8501226603984833, Val Acc: 45.45454545454545\n",
      "Epoch 33501/50000, Tr Loss: 0.2035, Tr Acc: 94.0298, Val Loss: 0.1416, Val Acc: 96.9697\n",
      "Val Loss: 0.860000729560852, Val Acc: 45.45454545454545\n",
      "Epoch 33601/50000, Tr Loss: 0.2154, Tr Acc: 91.0448, Val Loss: 0.1416, Val Acc: 96.9697\n",
      "Val Loss: 0.8000226318836212, Val Acc: 45.45454545454545\n",
      "Epoch 33701/50000, Tr Loss: 0.2073, Tr Acc: 92.5373, Val Loss: 0.1417, Val Acc: 96.9697\n",
      "Val Loss: 0.8343007862567902, Val Acc: 45.45454545454545\n",
      "Epoch 33801/50000, Tr Loss: 0.1828, Tr Acc: 95.5224, Val Loss: 0.1420, Val Acc: 96.9697\n",
      "Val Loss: 0.8644527792930603, Val Acc: 45.45454545454545\n",
      "Epoch 33901/50000, Tr Loss: 0.2296, Tr Acc: 85.0746, Val Loss: 0.1420, Val Acc: 96.9697\n",
      "Val Loss: 0.9422739446163177, Val Acc: 45.45454545454545\n",
      "Epoch 34001/50000, Tr Loss: 0.2115, Tr Acc: 92.5373, Val Loss: 0.1423, Val Acc: 96.9697\n",
      "Val Loss: 0.7689991295337677, Val Acc: 45.45454545454545\n",
      "Epoch 34101/50000, Tr Loss: 0.1888, Tr Acc: 92.5373, Val Loss: 0.1421, Val Acc: 96.9697\n",
      "Val Loss: 0.8856797218322754, Val Acc: 45.45454545454545\n",
      "Epoch 34201/50000, Tr Loss: 0.2001, Tr Acc: 91.0448, Val Loss: 0.1424, Val Acc: 96.9697\n",
      "Val Loss: 0.8513801097869873, Val Acc: 45.45454545454545\n",
      "Epoch 34301/50000, Tr Loss: 0.1896, Tr Acc: 94.0298, Val Loss: 0.1426, Val Acc: 96.9697\n",
      "Val Loss: 0.8820014894008636, Val Acc: 45.45454545454545\n",
      "Epoch 34401/50000, Tr Loss: 0.2006, Tr Acc: 94.0298, Val Loss: 0.1422, Val Acc: 96.9697\n",
      "Val Loss: 0.7695978879928589, Val Acc: 45.45454545454545\n",
      "Epoch 34501/50000, Tr Loss: 0.2035, Tr Acc: 91.0448, Val Loss: 0.1423, Val Acc: 96.9697\n",
      "Val Loss: 0.8943292498588562, Val Acc: 45.45454545454545\n",
      "Epoch 34601/50000, Tr Loss: 0.1905, Tr Acc: 92.5373, Val Loss: 0.1424, Val Acc: 96.9697\n",
      "Val Loss: 0.8124032616615295, Val Acc: 45.45454545454545\n",
      "Epoch 34701/50000, Tr Loss: 0.1994, Tr Acc: 94.0298, Val Loss: 0.1422, Val Acc: 96.9697\n",
      "Val Loss: 0.9076873362064362, Val Acc: 45.45454545454545\n",
      "Epoch 34801/50000, Tr Loss: 0.1950, Tr Acc: 94.0298, Val Loss: 0.1429, Val Acc: 96.9697\n",
      "Val Loss: 0.8562337458133698, Val Acc: 45.45454545454545\n",
      "Epoch 34901/50000, Tr Loss: 0.1628, Tr Acc: 95.5224, Val Loss: 0.1427, Val Acc: 96.9697\n",
      "Val Loss: 0.8555707633495331, Val Acc: 45.45454545454545\n",
      "Epoch 35001/50000, Tr Loss: 0.2256, Tr Acc: 94.0298, Val Loss: 0.1426, Val Acc: 96.9697\n",
      "Val Loss: 0.7549630403518677, Val Acc: 45.45454545454545\n",
      "Epoch 35101/50000, Tr Loss: 0.1973, Tr Acc: 94.0298, Val Loss: 0.1430, Val Acc: 96.9697\n",
      "Val Loss: 0.8205505311489105, Val Acc: 45.45454545454545\n",
      "Epoch 35201/50000, Tr Loss: 0.1946, Tr Acc: 95.5224, Val Loss: 0.1426, Val Acc: 96.9697\n",
      "Val Loss: 0.8575096130371094, Val Acc: 45.45454545454545\n",
      "Epoch 35301/50000, Tr Loss: 0.2065, Tr Acc: 94.0298, Val Loss: 0.1430, Val Acc: 96.9697\n",
      "Val Loss: 0.7712661027908325, Val Acc: 45.45454545454545\n",
      "Epoch 35401/50000, Tr Loss: 0.2157, Tr Acc: 89.5522, Val Loss: 0.1428, Val Acc: 96.9697\n",
      "Val Loss: 0.8307314813137054, Val Acc: 45.45454545454545\n",
      "Epoch 35501/50000, Tr Loss: 0.1752, Tr Acc: 95.5224, Val Loss: 0.1431, Val Acc: 96.9697\n",
      "Val Loss: 0.8765827417373657, Val Acc: 45.45454545454545\n",
      "Epoch 35601/50000, Tr Loss: 0.2133, Tr Acc: 91.0448, Val Loss: 0.1429, Val Acc: 96.9697\n",
      "Val Loss: 0.8147456049919128, Val Acc: 45.45454545454545\n",
      "Epoch 35701/50000, Tr Loss: 0.1903, Tr Acc: 91.0448, Val Loss: 0.1430, Val Acc: 96.9697\n",
      "Val Loss: 0.8179604709148407, Val Acc: 45.45454545454545\n",
      "Epoch 35801/50000, Tr Loss: 0.1917, Tr Acc: 97.0149, Val Loss: 0.1432, Val Acc: 96.9697\n",
      "Val Loss: 0.7397668659687042, Val Acc: 45.45454545454545\n",
      "Epoch 35901/50000, Tr Loss: 0.2041, Tr Acc: 94.0298, Val Loss: 0.1433, Val Acc: 96.9697\n",
      "Val Loss: 0.8796648681163788, Val Acc: 45.45454545454545\n",
      "Epoch 36001/50000, Tr Loss: 0.2241, Tr Acc: 91.0448, Val Loss: 0.1432, Val Acc: 96.9697\n",
      "Val Loss: 0.8671354651451111, Val Acc: 45.45454545454545\n",
      "Epoch 36101/50000, Tr Loss: 0.2045, Tr Acc: 94.0298, Val Loss: 0.1432, Val Acc: 96.9697\n",
      "Val Loss: 0.8676696717739105, Val Acc: 45.45454545454545\n",
      "Epoch 36201/50000, Tr Loss: 0.1790, Tr Acc: 97.0149, Val Loss: 0.1434, Val Acc: 96.9697\n",
      "Val Loss: 0.8143805265426636, Val Acc: 45.45454545454545\n",
      "Epoch 36301/50000, Tr Loss: 0.1860, Tr Acc: 92.5373, Val Loss: 0.1434, Val Acc: 93.9394\n",
      "Val Loss: 0.8205266296863556, Val Acc: 45.45454545454545\n",
      "Epoch 36401/50000, Tr Loss: 0.1857, Tr Acc: 95.5224, Val Loss: 0.1436, Val Acc: 93.9394\n",
      "Val Loss: 0.7861146628856659, Val Acc: 45.45454545454545\n",
      "Epoch 36501/50000, Tr Loss: 0.1891, Tr Acc: 94.0298, Val Loss: 0.1434, Val Acc: 93.9394\n",
      "Val Loss: 0.8070245683193207, Val Acc: 45.45454545454545\n",
      "Epoch 36601/50000, Tr Loss: 0.1974, Tr Acc: 94.0298, Val Loss: 0.1436, Val Acc: 93.9394\n",
      "Val Loss: 0.8431859910488129, Val Acc: 45.45454545454545\n",
      "Epoch 36701/50000, Tr Loss: 0.1801, Tr Acc: 95.5224, Val Loss: 0.1437, Val Acc: 93.9394\n",
      "Val Loss: 0.8820980191230774, Val Acc: 45.45454545454545\n",
      "Epoch 36801/50000, Tr Loss: 0.1954, Tr Acc: 89.5522, Val Loss: 0.1438, Val Acc: 93.9394\n",
      "Val Loss: 0.8459491729736328, Val Acc: 45.45454545454545\n",
      "Epoch 36901/50000, Tr Loss: 0.2021, Tr Acc: 94.0298, Val Loss: 0.1437, Val Acc: 93.9394\n",
      "Val Loss: 0.8041320145130157, Val Acc: 45.45454545454545\n",
      "Epoch 37001/50000, Tr Loss: 0.1953, Tr Acc: 92.5373, Val Loss: 0.1437, Val Acc: 93.9394\n",
      "Val Loss: 0.8158935904502869, Val Acc: 45.45454545454545\n",
      "Epoch 37101/50000, Tr Loss: 0.2045, Tr Acc: 95.5224, Val Loss: 0.1439, Val Acc: 93.9394\n",
      "Val Loss: 0.8866163492202759, Val Acc: 45.45454545454545\n",
      "Epoch 37201/50000, Tr Loss: 0.1886, Tr Acc: 95.5224, Val Loss: 0.1440, Val Acc: 93.9394\n",
      "Val Loss: 0.8033209443092346, Val Acc: 45.45454545454545\n",
      "Epoch 37301/50000, Tr Loss: 0.1824, Tr Acc: 94.0298, Val Loss: 0.1443, Val Acc: 93.9394\n",
      "Val Loss: 0.9103861749172211, Val Acc: 45.45454545454545\n",
      "Epoch 37401/50000, Tr Loss: 0.1826, Tr Acc: 92.5373, Val Loss: 0.1440, Val Acc: 93.9394\n",
      "Val Loss: 0.8434960842132568, Val Acc: 45.45454545454545\n",
      "Epoch 37501/50000, Tr Loss: 0.2087, Tr Acc: 92.5373, Val Loss: 0.1441, Val Acc: 93.9394\n",
      "Val Loss: 0.7096370458602905, Val Acc: 45.45454545454545\n",
      "Epoch 37601/50000, Tr Loss: 0.1774, Tr Acc: 97.0149, Val Loss: 0.1438, Val Acc: 93.9394\n",
      "Val Loss: 0.880757600069046, Val Acc: 45.45454545454545\n",
      "Epoch 37701/50000, Tr Loss: 0.1918, Tr Acc: 92.5373, Val Loss: 0.1442, Val Acc: 93.9394\n",
      "Val Loss: 0.9047366976737976, Val Acc: 45.45454545454545\n",
      "Epoch 37801/50000, Tr Loss: 0.1843, Tr Acc: 92.5373, Val Loss: 0.1442, Val Acc: 93.9394\n",
      "Val Loss: 0.8176327347755432, Val Acc: 45.45454545454545\n",
      "Epoch 37901/50000, Tr Loss: 0.1751, Tr Acc: 97.0149, Val Loss: 0.1443, Val Acc: 93.9394\n",
      "Val Loss: 0.7872044742107391, Val Acc: 45.45454545454545\n",
      "Epoch 38001/50000, Tr Loss: 0.2029, Tr Acc: 95.5224, Val Loss: 0.1444, Val Acc: 93.9394\n",
      "Val Loss: 0.786726176738739, Val Acc: 45.45454545454545\n",
      "Epoch 38101/50000, Tr Loss: 0.1993, Tr Acc: 91.0448, Val Loss: 0.1445, Val Acc: 93.9394\n",
      "Val Loss: 0.9125666618347168, Val Acc: 45.45454545454545\n",
      "Epoch 38201/50000, Tr Loss: 0.1831, Tr Acc: 94.0298, Val Loss: 0.1446, Val Acc: 93.9394\n",
      "Val Loss: 0.8260872066020966, Val Acc: 45.45454545454545\n",
      "Epoch 38301/50000, Tr Loss: 0.1929, Tr Acc: 95.5224, Val Loss: 0.1447, Val Acc: 93.9394\n",
      "Val Loss: 0.8167338371276855, Val Acc: 45.45454545454545\n",
      "Epoch 38401/50000, Tr Loss: 0.1978, Tr Acc: 95.5224, Val Loss: 0.1447, Val Acc: 93.9394\n",
      "Val Loss: 0.7707113027572632, Val Acc: 45.45454545454545\n",
      "Epoch 38501/50000, Tr Loss: 0.2043, Tr Acc: 92.5373, Val Loss: 0.1448, Val Acc: 93.9394\n",
      "Val Loss: 0.8010005354881287, Val Acc: 45.45454545454545\n",
      "Epoch 38601/50000, Tr Loss: 0.1896, Tr Acc: 92.5373, Val Loss: 0.1450, Val Acc: 93.9394\n",
      "Val Loss: 0.7868080139160156, Val Acc: 45.45454545454545\n",
      "Epoch 38701/50000, Tr Loss: 0.2132, Tr Acc: 89.5522, Val Loss: 0.1449, Val Acc: 93.9394\n",
      "Val Loss: 0.7676217555999756, Val Acc: 45.45454545454545\n",
      "Epoch 38801/50000, Tr Loss: 0.2049, Tr Acc: 95.5224, Val Loss: 0.1451, Val Acc: 93.9394\n",
      "Val Loss: 0.8406062424182892, Val Acc: 45.45454545454545\n",
      "Epoch 38901/50000, Tr Loss: 0.1773, Tr Acc: 97.0149, Val Loss: 0.1452, Val Acc: 93.9394\n",
      "Val Loss: 0.7876079082489014, Val Acc: 45.45454545454545\n",
      "Epoch 39001/50000, Tr Loss: 0.1724, Tr Acc: 95.5224, Val Loss: 0.1451, Val Acc: 93.9394\n",
      "Val Loss: 0.8648939728736877, Val Acc: 45.45454545454545\n",
      "Epoch 39101/50000, Tr Loss: 0.1857, Tr Acc: 95.5224, Val Loss: 0.1450, Val Acc: 93.9394\n",
      "Val Loss: 0.7290787994861603, Val Acc: 45.45454545454545\n",
      "Epoch 39201/50000, Tr Loss: 0.1820, Tr Acc: 95.5224, Val Loss: 0.1454, Val Acc: 93.9394\n",
      "Val Loss: 0.9112792015075684, Val Acc: 45.45454545454545\n",
      "Epoch 39301/50000, Tr Loss: 0.1892, Tr Acc: 92.5373, Val Loss: 0.1454, Val Acc: 93.9394\n",
      "Val Loss: 0.7823034226894379, Val Acc: 45.45454545454545\n",
      "Epoch 39401/50000, Tr Loss: 0.2000, Tr Acc: 91.0448, Val Loss: 0.1451, Val Acc: 93.9394\n",
      "Val Loss: 0.8302074670791626, Val Acc: 47.72727272727273\n",
      "Epoch 39501/50000, Tr Loss: 0.1802, Tr Acc: 92.5373, Val Loss: 0.1454, Val Acc: 93.9394\n",
      "Val Loss: 0.8548043966293335, Val Acc: 45.45454545454545\n",
      "Epoch 39601/50000, Tr Loss: 0.1869, Tr Acc: 98.5075, Val Loss: 0.1457, Val Acc: 93.9394\n",
      "Val Loss: 0.7820038795471191, Val Acc: 45.45454545454545\n",
      "Epoch 39701/50000, Tr Loss: 0.1937, Tr Acc: 91.0448, Val Loss: 0.1456, Val Acc: 93.9394\n",
      "Val Loss: 0.7680128812789917, Val Acc: 45.45454545454545\n",
      "Epoch 39801/50000, Tr Loss: 0.1943, Tr Acc: 94.0298, Val Loss: 0.1456, Val Acc: 93.9394\n",
      "Val Loss: 0.7985288798809052, Val Acc: 45.45454545454545\n",
      "Epoch 39901/50000, Tr Loss: 0.2007, Tr Acc: 95.5224, Val Loss: 0.1457, Val Acc: 93.9394\n",
      "Val Loss: 0.8618695735931396, Val Acc: 45.45454545454545\n",
      "Epoch 40001/50000, Tr Loss: 0.1752, Tr Acc: 97.0149, Val Loss: 0.1461, Val Acc: 93.9394\n",
      "Val Loss: 0.7587980329990387, Val Acc: 45.45454545454545\n",
      "Epoch 40101/50000, Tr Loss: 0.1835, Tr Acc: 95.5224, Val Loss: 0.1459, Val Acc: 93.9394\n",
      "Val Loss: 0.801689624786377, Val Acc: 45.45454545454545\n",
      "Epoch 40201/50000, Tr Loss: 0.1763, Tr Acc: 94.0298, Val Loss: 0.1458, Val Acc: 93.9394\n",
      "Val Loss: 0.7939901649951935, Val Acc: 47.72727272727273\n",
      "Epoch 40301/50000, Tr Loss: 0.1881, Tr Acc: 92.5373, Val Loss: 0.1458, Val Acc: 93.9394\n",
      "Val Loss: 0.8094742596149445, Val Acc: 47.72727272727273\n",
      "Epoch 40401/50000, Tr Loss: 0.1693, Tr Acc: 94.0298, Val Loss: 0.1460, Val Acc: 93.9394\n",
      "Val Loss: 0.9032634496688843, Val Acc: 47.72727272727273\n",
      "Epoch 40501/50000, Tr Loss: 0.1838, Tr Acc: 94.0298, Val Loss: 0.1461, Val Acc: 93.9394\n",
      "Val Loss: 0.9556423425674438, Val Acc: 47.72727272727273\n",
      "Epoch 40601/50000, Tr Loss: 0.1642, Tr Acc: 92.5373, Val Loss: 0.1458, Val Acc: 93.9394\n",
      "Val Loss: 0.8466374278068542, Val Acc: 47.72727272727273\n",
      "Epoch 40701/50000, Tr Loss: 0.1907, Tr Acc: 92.5373, Val Loss: 0.1461, Val Acc: 93.9394\n",
      "Val Loss: 0.9244253933429718, Val Acc: 47.72727272727273\n",
      "Epoch 40801/50000, Tr Loss: 0.1836, Tr Acc: 95.5224, Val Loss: 0.1464, Val Acc: 93.9394\n",
      "Val Loss: 0.8077019453048706, Val Acc: 47.72727272727273\n",
      "Epoch 40901/50000, Tr Loss: 0.1894, Tr Acc: 92.5373, Val Loss: 0.1462, Val Acc: 93.9394\n",
      "Val Loss: 0.9403734803199768, Val Acc: 47.72727272727273\n",
      "Epoch 41001/50000, Tr Loss: 0.1822, Tr Acc: 94.0298, Val Loss: 0.1466, Val Acc: 93.9394\n",
      "Val Loss: 0.8633666932582855, Val Acc: 47.72727272727273\n",
      "Epoch 41101/50000, Tr Loss: 0.1809, Tr Acc: 95.5224, Val Loss: 0.1462, Val Acc: 93.9394\n",
      "Val Loss: 0.7619282603263855, Val Acc: 47.72727272727273\n",
      "Epoch 41201/50000, Tr Loss: 0.1770, Tr Acc: 97.0149, Val Loss: 0.1462, Val Acc: 93.9394\n",
      "Val Loss: 0.8758285641670227, Val Acc: 47.72727272727273\n",
      "Epoch 41301/50000, Tr Loss: 0.1718, Tr Acc: 97.0149, Val Loss: 0.1464, Val Acc: 93.9394\n",
      "Val Loss: 0.7969359457492828, Val Acc: 47.72727272727273\n",
      "Epoch 41401/50000, Tr Loss: 0.1801, Tr Acc: 95.5224, Val Loss: 0.1465, Val Acc: 93.9394\n",
      "Val Loss: 0.7435879111289978, Val Acc: 47.72727272727273\n",
      "Epoch 41501/50000, Tr Loss: 0.1893, Tr Acc: 95.5224, Val Loss: 0.1471, Val Acc: 93.9394\n",
      "Val Loss: 0.8537333309650421, Val Acc: 47.72727272727273\n",
      "Epoch 41601/50000, Tr Loss: 0.1722, Tr Acc: 95.5224, Val Loss: 0.1470, Val Acc: 93.9394\n",
      "Val Loss: 0.7990652322769165, Val Acc: 47.72727272727273\n",
      "Epoch 41701/50000, Tr Loss: 0.1597, Tr Acc: 97.0149, Val Loss: 0.1471, Val Acc: 93.9394\n",
      "Val Loss: 0.8373876214027405, Val Acc: 47.72727272727273\n",
      "Epoch 41801/50000, Tr Loss: 0.2062, Tr Acc: 92.5373, Val Loss: 0.1468, Val Acc: 93.9394\n",
      "Val Loss: 0.8288268148899078, Val Acc: 47.72727272727273\n",
      "Epoch 41901/50000, Tr Loss: 0.1768, Tr Acc: 97.0149, Val Loss: 0.1470, Val Acc: 93.9394\n",
      "Val Loss: 0.7742764353752136, Val Acc: 47.72727272727273\n",
      "Epoch 42001/50000, Tr Loss: 0.1641, Tr Acc: 97.0149, Val Loss: 0.1471, Val Acc: 93.9394\n",
      "Val Loss: 0.8185200691223145, Val Acc: 47.72727272727273\n",
      "Epoch 42101/50000, Tr Loss: 0.1870, Tr Acc: 95.5224, Val Loss: 0.1471, Val Acc: 93.9394\n",
      "Val Loss: 0.7347472012042999, Val Acc: 47.72727272727273\n",
      "Epoch 42201/50000, Tr Loss: 0.1828, Tr Acc: 95.5224, Val Loss: 0.1471, Val Acc: 93.9394\n",
      "Val Loss: 0.8891286551952362, Val Acc: 47.72727272727273\n",
      "Epoch 42301/50000, Tr Loss: 0.1868, Tr Acc: 94.0298, Val Loss: 0.1470, Val Acc: 93.9394\n",
      "Val Loss: 0.8296252489089966, Val Acc: 47.72727272727273\n",
      "Epoch 42401/50000, Tr Loss: 0.1631, Tr Acc: 95.5224, Val Loss: 0.1472, Val Acc: 93.9394\n",
      "Val Loss: 0.8165650367736816, Val Acc: 47.72727272727273\n",
      "Epoch 42501/50000, Tr Loss: 0.1841, Tr Acc: 91.0448, Val Loss: 0.1472, Val Acc: 93.9394\n",
      "Val Loss: 0.845915675163269, Val Acc: 47.72727272727273\n",
      "Epoch 42601/50000, Tr Loss: 0.1791, Tr Acc: 92.5373, Val Loss: 0.1473, Val Acc: 93.9394\n",
      "Val Loss: 0.8438980579376221, Val Acc: 47.72727272727273\n",
      "Epoch 42701/50000, Tr Loss: 0.1809, Tr Acc: 95.5224, Val Loss: 0.1476, Val Acc: 93.9394\n",
      "Val Loss: 0.9307146966457367, Val Acc: 47.72727272727273\n",
      "Epoch 42801/50000, Tr Loss: 0.1739, Tr Acc: 95.5224, Val Loss: 0.1475, Val Acc: 93.9394\n",
      "Val Loss: 0.8734090030193329, Val Acc: 47.72727272727273\n",
      "Epoch 42901/50000, Tr Loss: 0.1911, Tr Acc: 91.0448, Val Loss: 0.1474, Val Acc: 93.9394\n",
      "Val Loss: 0.7888370752334595, Val Acc: 47.72727272727273\n",
      "Epoch 43001/50000, Tr Loss: 0.1871, Tr Acc: 94.0298, Val Loss: 0.1473, Val Acc: 93.9394\n",
      "Val Loss: 0.7798329889774323, Val Acc: 47.72727272727273\n",
      "Epoch 43101/50000, Tr Loss: 0.1880, Tr Acc: 94.0298, Val Loss: 0.1474, Val Acc: 93.9394\n",
      "Val Loss: 0.757940024137497, Val Acc: 47.72727272727273\n",
      "Epoch 43201/50000, Tr Loss: 0.1861, Tr Acc: 94.0298, Val Loss: 0.1478, Val Acc: 93.9394\n",
      "Val Loss: 0.852257251739502, Val Acc: 47.72727272727273\n",
      "Epoch 43301/50000, Tr Loss: 0.1760, Tr Acc: 94.0298, Val Loss: 0.1477, Val Acc: 93.9394\n",
      "Val Loss: 0.7809523940086365, Val Acc: 47.72727272727273\n",
      "Epoch 43401/50000, Tr Loss: 0.1791, Tr Acc: 94.0298, Val Loss: 0.1476, Val Acc: 93.9394\n",
      "Val Loss: 0.9549897909164429, Val Acc: 47.72727272727273\n",
      "Epoch 43501/50000, Tr Loss: 0.1803, Tr Acc: 94.0298, Val Loss: 0.1476, Val Acc: 93.9394\n",
      "Val Loss: 0.8472148180007935, Val Acc: 47.72727272727273\n",
      "Epoch 43601/50000, Tr Loss: 0.1822, Tr Acc: 95.5224, Val Loss: 0.1481, Val Acc: 93.9394\n",
      "Val Loss: 0.799036830663681, Val Acc: 47.72727272727273\n",
      "Epoch 43701/50000, Tr Loss: 0.1938, Tr Acc: 92.5373, Val Loss: 0.1480, Val Acc: 93.9394\n",
      "Val Loss: 0.930651068687439, Val Acc: 47.72727272727273\n",
      "Epoch 43801/50000, Tr Loss: 0.1905, Tr Acc: 95.5224, Val Loss: 0.1481, Val Acc: 93.9394\n",
      "Val Loss: 0.8207147717475891, Val Acc: 47.72727272727273\n",
      "Epoch 43901/50000, Tr Loss: 0.1803, Tr Acc: 95.5224, Val Loss: 0.1482, Val Acc: 93.9394\n",
      "Val Loss: 0.7812959849834442, Val Acc: 47.72727272727273\n",
      "Epoch 44001/50000, Tr Loss: 0.1879, Tr Acc: 92.5373, Val Loss: 0.1484, Val Acc: 93.9394\n",
      "Val Loss: 0.6908278316259384, Val Acc: 47.72727272727273\n",
      "Epoch 44101/50000, Tr Loss: 0.1873, Tr Acc: 94.0298, Val Loss: 0.1485, Val Acc: 93.9394\n",
      "Val Loss: 0.8326654434204102, Val Acc: 47.72727272727273\n",
      "Epoch 44201/50000, Tr Loss: 0.1702, Tr Acc: 95.5224, Val Loss: 0.1485, Val Acc: 90.9091\n",
      "Val Loss: 0.8301903605461121, Val Acc: 47.72727272727273\n",
      "Epoch 44301/50000, Tr Loss: 0.1682, Tr Acc: 95.5224, Val Loss: 0.1486, Val Acc: 90.9091\n",
      "Val Loss: 0.8920040726661682, Val Acc: 47.72727272727273\n",
      "Epoch 44401/50000, Tr Loss: 0.1708, Tr Acc: 97.0149, Val Loss: 0.1487, Val Acc: 90.9091\n",
      "Val Loss: 0.8627091646194458, Val Acc: 47.72727272727273\n",
      "Epoch 44501/50000, Tr Loss: 0.1669, Tr Acc: 97.0149, Val Loss: 0.1486, Val Acc: 90.9091\n",
      "Val Loss: 0.7905943691730499, Val Acc: 47.72727272727273\n",
      "Epoch 44601/50000, Tr Loss: 0.1877, Tr Acc: 94.0298, Val Loss: 0.1486, Val Acc: 90.9091\n",
      "Val Loss: 0.9382626116275787, Val Acc: 47.72727272727273\n",
      "Epoch 44701/50000, Tr Loss: 0.1629, Tr Acc: 97.0149, Val Loss: 0.1488, Val Acc: 90.9091\n",
      "Val Loss: 0.9694763720035553, Val Acc: 47.72727272727273\n",
      "Epoch 44801/50000, Tr Loss: 0.1663, Tr Acc: 92.5373, Val Loss: 0.1488, Val Acc: 90.9091\n",
      "Val Loss: 0.9140933156013489, Val Acc: 47.72727272727273\n",
      "Epoch 44901/50000, Tr Loss: 0.1642, Tr Acc: 95.5224, Val Loss: 0.1489, Val Acc: 90.9091\n",
      "Val Loss: 0.9161312878131866, Val Acc: 47.72727272727273\n",
      "Epoch 45001/50000, Tr Loss: 0.1550, Tr Acc: 98.5075, Val Loss: 0.1486, Val Acc: 90.9091\n",
      "Val Loss: 0.8819342851638794, Val Acc: 47.72727272727273\n",
      "Epoch 45101/50000, Tr Loss: 0.1517, Tr Acc: 97.0149, Val Loss: 0.1486, Val Acc: 90.9091\n",
      "Val Loss: 0.8367521464824677, Val Acc: 47.72727272727273\n",
      "Epoch 45201/50000, Tr Loss: 0.1742, Tr Acc: 95.5224, Val Loss: 0.1490, Val Acc: 90.9091\n",
      "Val Loss: 0.8614037930965424, Val Acc: 47.72727272727273\n",
      "Epoch 45301/50000, Tr Loss: 0.1729, Tr Acc: 95.5224, Val Loss: 0.1490, Val Acc: 90.9091\n",
      "Val Loss: 0.8698456585407257, Val Acc: 47.72727272727273\n",
      "Epoch 45901/50000, Tr Loss: 0.1733, Tr Acc: 94.0298, Val Loss: 0.1492, Val Acc: 90.9091\n",
      "Val Loss: 0.8757866621017456, Val Acc: 47.72727272727273\n",
      "Epoch 46001/50000, Tr Loss: 0.1758, Tr Acc: 94.0298, Val Loss: 0.1497, Val Acc: 90.9091\n",
      "Val Loss: 0.873310387134552, Val Acc: 47.72727272727273\n",
      "Epoch 46101/50000, Tr Loss: 0.1722, Tr Acc: 94.0298, Val Loss: 0.1497, Val Acc: 90.9091\n",
      "Val Loss: 0.8498803079128265, Val Acc: 47.72727272727273\n",
      "Epoch 46201/50000, Tr Loss: 0.1806, Tr Acc: 94.0298, Val Loss: 0.1497, Val Acc: 90.9091\n",
      "Val Loss: 0.867402970790863, Val Acc: 47.72727272727273\n",
      "Epoch 46301/50000, Tr Loss: 0.1665, Tr Acc: 97.0149, Val Loss: 0.1495, Val Acc: 90.9091\n",
      "Val Loss: 0.8374156355857849, Val Acc: 47.72727272727273\n",
      "Epoch 46401/50000, Tr Loss: 0.1593, Tr Acc: 97.0149, Val Loss: 0.1497, Val Acc: 90.9091\n",
      "Val Loss: 0.8372375071048737, Val Acc: 47.72727272727273\n",
      "Epoch 46501/50000, Tr Loss: 0.1712, Tr Acc: 97.0149, Val Loss: 0.1498, Val Acc: 90.9091\n",
      "Val Loss: 0.8139075636863708, Val Acc: 50.0\n",
      "Epoch 46601/50000, Tr Loss: 0.1789, Tr Acc: 92.5373, Val Loss: 0.1498, Val Acc: 90.9091\n",
      "Val Loss: 0.8143357038497925, Val Acc: 50.0\n",
      "Epoch 46701/50000, Tr Loss: 0.1655, Tr Acc: 94.0298, Val Loss: 0.1498, Val Acc: 90.9091\n",
      "Val Loss: 0.9273402988910675, Val Acc: 50.0\n",
      "Epoch 46801/50000, Tr Loss: 0.1500, Tr Acc: 97.0149, Val Loss: 0.1500, Val Acc: 90.9091\n",
      "Val Loss: 0.876621276140213, Val Acc: 50.0\n",
      "Epoch 46901/50000, Tr Loss: 0.1664, Tr Acc: 95.5224, Val Loss: 0.1501, Val Acc: 90.9091\n",
      "Val Loss: 0.7867114543914795, Val Acc: 50.0\n",
      "Epoch 47001/50000, Tr Loss: 0.1652, Tr Acc: 94.0298, Val Loss: 0.1502, Val Acc: 90.9091\n",
      "Val Loss: 0.7805591225624084, Val Acc: 50.0\n",
      "Epoch 47101/50000, Tr Loss: 0.1739, Tr Acc: 95.5224, Val Loss: 0.1502, Val Acc: 90.9091\n",
      "Val Loss: 0.7884770333766937, Val Acc: 47.72727272727273\n",
      "Epoch 47201/50000, Tr Loss: 0.1672, Tr Acc: 92.5373, Val Loss: 0.1502, Val Acc: 90.9091\n",
      "Val Loss: 0.8117192685604095, Val Acc: 50.0\n",
      "Epoch 47301/50000, Tr Loss: 0.1736, Tr Acc: 95.5224, Val Loss: 0.1504, Val Acc: 90.9091\n",
      "Val Loss: 0.863382875919342, Val Acc: 50.0\n",
      "Epoch 47401/50000, Tr Loss: 0.1591, Tr Acc: 97.0149, Val Loss: 0.1503, Val Acc: 90.9091\n",
      "Val Loss: 0.8701744377613068, Val Acc: 50.0\n",
      "Epoch 47501/50000, Tr Loss: 0.1572, Tr Acc: 95.5224, Val Loss: 0.1503, Val Acc: 90.9091\n",
      "Val Loss: 0.7207646369934082, Val Acc: 50.0\n",
      "Epoch 47601/50000, Tr Loss: 0.1642, Tr Acc: 97.0149, Val Loss: 0.1504, Val Acc: 90.9091\n",
      "Val Loss: 0.8236499130725861, Val Acc: 50.0\n",
      "Epoch 47701/50000, Tr Loss: 0.1567, Tr Acc: 97.0149, Val Loss: 0.1507, Val Acc: 90.9091\n",
      "Val Loss: 0.8320013582706451, Val Acc: 50.0\n",
      "Epoch 47801/50000, Tr Loss: 0.1574, Tr Acc: 97.0149, Val Loss: 0.1506, Val Acc: 90.9091\n",
      "Val Loss: 0.8534605801105499, Val Acc: 50.0\n",
      "Epoch 47901/50000, Tr Loss: 0.1808, Tr Acc: 97.0149, Val Loss: 0.1506, Val Acc: 90.9091\n",
      "Val Loss: 0.7718585133552551, Val Acc: 50.0\n",
      "Epoch 48001/50000, Tr Loss: 0.1889, Tr Acc: 95.5224, Val Loss: 0.1507, Val Acc: 90.9091\n",
      "Val Loss: 0.7487165927886963, Val Acc: 50.0\n",
      "Epoch 48101/50000, Tr Loss: 0.1595, Tr Acc: 94.0298, Val Loss: 0.1509, Val Acc: 90.9091\n",
      "Val Loss: 0.7999868988990784, Val Acc: 50.0\n",
      "Epoch 48201/50000, Tr Loss: 0.1723, Tr Acc: 94.0298, Val Loss: 0.1510, Val Acc: 90.9091\n",
      "Val Loss: 0.8484913110733032, Val Acc: 50.0\n",
      "Epoch 48301/50000, Tr Loss: 0.1601, Tr Acc: 95.5224, Val Loss: 0.1511, Val Acc: 90.9091\n",
      "Val Loss: 0.8649860322475433, Val Acc: 50.0\n",
      "Epoch 48401/50000, Tr Loss: 0.1679, Tr Acc: 95.5224, Val Loss: 0.1510, Val Acc: 90.9091\n",
      "Val Loss: 0.8670792281627655, Val Acc: 50.0\n",
      "Epoch 48501/50000, Tr Loss: 0.1748, Tr Acc: 94.0298, Val Loss: 0.1511, Val Acc: 90.9091\n",
      "Val Loss: 0.8423721492290497, Val Acc: 50.0\n",
      "Epoch 48601/50000, Tr Loss: 0.1616, Tr Acc: 97.0149, Val Loss: 0.1512, Val Acc: 90.9091\n",
      "Val Loss: 0.8620730340480804, Val Acc: 50.0\n",
      "Epoch 48701/50000, Tr Loss: 0.1607, Tr Acc: 97.0149, Val Loss: 0.1511, Val Acc: 90.9091\n",
      "Val Loss: 0.8479324579238892, Val Acc: 50.0\n",
      "Epoch 48801/50000, Tr Loss: 0.1689, Tr Acc: 95.5224, Val Loss: 0.1515, Val Acc: 90.9091\n",
      "Val Loss: 0.8560463488101959, Val Acc: 50.0\n",
      "Epoch 48901/50000, Tr Loss: 0.1603, Tr Acc: 97.0149, Val Loss: 0.1516, Val Acc: 90.9091\n",
      "Val Loss: 0.8609783947467804, Val Acc: 50.0\n",
      "Epoch 49001/50000, Tr Loss: 0.1665, Tr Acc: 95.5224, Val Loss: 0.1513, Val Acc: 90.9091\n",
      "Val Loss: 0.7948673367500305, Val Acc: 50.0\n",
      "Epoch 49101/50000, Tr Loss: 0.1737, Tr Acc: 97.0149, Val Loss: 0.1515, Val Acc: 90.9091\n",
      "Val Loss: 0.8616814613342285, Val Acc: 50.0\n",
      "Epoch 49201/50000, Tr Loss: 0.1734, Tr Acc: 97.0149, Val Loss: 0.1516, Val Acc: 90.9091\n",
      "Val Loss: 0.8648650348186493, Val Acc: 50.0\n",
      "Epoch 49301/50000, Tr Loss: 0.1454, Tr Acc: 97.0149, Val Loss: 0.1515, Val Acc: 90.9091\n",
      "Val Loss: 0.7240734845399857, Val Acc: 50.0\n",
      "Epoch 49401/50000, Tr Loss: 0.1606, Tr Acc: 97.0149, Val Loss: 0.1518, Val Acc: 90.9091\n",
      "Val Loss: 0.7864840626716614, Val Acc: 50.0\n",
      "Epoch 49501/50000, Tr Loss: 0.1584, Tr Acc: 97.0149, Val Loss: 0.1519, Val Acc: 90.9091\n",
      "Val Loss: 0.7600579857826233, Val Acc: 50.0\n",
      "Epoch 49601/50000, Tr Loss: 0.1684, Tr Acc: 94.0298, Val Loss: 0.1517, Val Acc: 90.9091\n",
      "Val Loss: 0.8238933384418488, Val Acc: 50.0\n",
      "Epoch 49701/50000, Tr Loss: 0.1441, Tr Acc: 97.0149, Val Loss: 0.1519, Val Acc: 90.9091\n",
      "Val Loss: 0.7838124632835388, Val Acc: 50.0\n",
      "Epoch 49801/50000, Tr Loss: 0.1502, Tr Acc: 97.0149, Val Loss: 0.1517, Val Acc: 90.9091\n",
      "Val Loss: 0.8702911734580994, Val Acc: 50.0\n",
      "Epoch 49901/50000, Tr Loss: 0.1699, Tr Acc: 98.5075, Val Loss: 0.1519, Val Acc: 90.9091\n",
      "Val Loss: 0.7741153240203857, Val Acc: 50.0\n"
     ]
    }
   ],
   "source": [
    "results_per_fold = []\n",
    "batch_size = 32\n",
    "from common import train\n",
    "\n",
    "\n",
    "\n",
    "for epoch_B in range(1):\n",
    "    print(\"Epoch Num {}\".format(epoch_B))\n",
    "    net = ConvNet().cuda(0)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    wand = wandb.init(\n",
    "\n",
    "          # Set the project where this run will be logged\n",
    "          project=\"Motor-Imagery\", \n",
    "          # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "          name=f\"ourdata_CNN_2ch_2class_cross_ourdata_s4\", \n",
    "          # Track hyperparameters and run metadata\n",
    "          config={\n",
    "          \"learning_rate\": 0.00000001,\n",
    "          \"architecture\": \"CNN\",\n",
    "          \"dataset\": \"Nutapol T.\",\n",
    "          \"epochs\": 50000,\n",
    "          \"weightname\":\"ourdata_CNN_2ch_2class_cross_update_s4\",\n",
    "\n",
    "\n",
    "          }\n",
    "        )\n",
    "\n",
    "    config = wand.config\n",
    "    #print(config.num_step_per_epoch)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for fold_idx, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "\n",
    "        print('Fold {}'.format(fold_idx + 1))\n",
    "\n",
    "        train_loader, test_loader = setup_dataflow(X_tensor_train,y_tensor_train, train_idx, val_idx)\n",
    "        num_step =math.ceil(len(train_loader.dataset) / batch_size)\n",
    "        config.num_step_per_epoch=num_step\n",
    "        train_loss,valid_loss,train_accuracy,valid_accuracy =train(\n",
    "                                                                model = net,\n",
    "                                                                gpu_num = 0,\n",
    "                                                                train_loader = train_loader,\n",
    "                                                                test_loader = test_loader,\n",
    "                                                                vail_loader = vail_loader,\n",
    "                                                                optimizer = optimizer  ,\n",
    "                                                                criterion = criterion ,\n",
    "                                                                wand = wand,\n",
    "                                                                cross = True\n",
    "                                                                     )\n",
    "        results_per_fold.append([train_accuracy, valid_accuracy])\n",
    "wandb.alert(\n",
    "            title='Finish',\n",
    "            text=f'Finishing training',\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41f393-f99e-4774-bcd9-f42279f4b195",
   "metadata": {},
   "source": [
    "### GAMENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b75311fb-331b-4a9c-9f96-c6f222f5e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gamenet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(gamenet,self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Sequential(\n",
    "            #in_channel = 16\n",
    "            #out_channel or Filter size = 100\n",
    "            #kernel size = (1,25)\n",
    "            #stride = 1\n",
    "            #padding = Same\n",
    "            #Relu\n",
    "            nn.Conv2d(1,100,kernel_size=(1,2),stride=1,padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(100)\n",
    "        )\n",
    "        self.l2 = nn.Sequential(\n",
    "            #in_channel = 100\n",
    "            #out_channel or Filter size = 100\n",
    "            #kernel size = (16,1)\n",
    "            #stride = 1\n",
    "            #padding = Valid\n",
    "            #Relu\n",
    "            nn.Conv2d(100,100,kernel_size=(1,16),stride=1,padding='valid'),\n",
    "            #nn.Conv2d(100,100,kernel_size=(64,1),stride=1,padding='valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(100)\n",
    "        )\n",
    "        self.l3 = nn.Sequential(\n",
    "            #in_channel = 100\n",
    "            #out_channel = 50\n",
    "            #kernel size = (1,30)\n",
    "            #stride = 1\n",
    "            #padding = Same\n",
    "            #Relu\n",
    "            nn.Conv2d(100,50,kernel_size=(1,30),stride=1,padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(50)\n",
    "        )\n",
    "        self.maxpooling1 = nn.MaxPool2d(kernel_size=(1,7),stride=5)\n",
    "        self.l4 = nn.Sequential(\n",
    "            #in_channel = 50\n",
    "            #out_channel = 50\n",
    "            #kernel size = (1,30)\n",
    "            #stride = 1\n",
    "            #padding = Same\n",
    "            #Relu\n",
    "            nn.Conv2d(50,50,kernel_size=(1,30),stride=1,padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(50)\n",
    "        )\n",
    "        self.maxpooling2 = nn.MaxPool2d(kernel_size=(1,3),stride=2)\n",
    "        \n",
    "\n",
    "        self.flatten = nn.Flatten()   #Sequential(nn.Flatten(),nn.BatchNorm1d(3050),nn.Dropout(0.15))\n",
    "        #mat1 and mat2 shapes cannot be multiplied (32x6100 and 3050x1024)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(6100,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc5 = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc6 = nn.Sequential(\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.softmax = nn.Sequential(\n",
    "            nn.Linear(32,2),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        \n",
    "        out = self.l1(x)\n",
    "        out = self.l2(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.maxpooling1(out)\n",
    "        out = self.l4(out)\n",
    "        out = self.maxpooling2(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7dba703b-48bc-49b3-aea3-f5474218b2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 events found\n",
      "Event IDs: [1 2 4]\n",
      "Not setting metadata\n",
      "72 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 72 events and 1251 original time points ...\n",
      "0 bad epochs dropped\n",
      "(72, 2, 1251) (72,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14181/1373606278.py:92: RuntimeWarning: No matching events found for 3 (event id 3)\n",
      "  epochs = mne.Epochs(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = eeg.get_X_y()\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "#X = X[:,:,:]\n",
    "X.shape\n",
    "y=y-1\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "692bb76e-272a-4bfe-80c9-2bb2be7041b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 1, 2, 1251)\n"
     ]
    }
   ],
   "source": [
    "X = X[:,np.newaxis,:,:]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "562792ae-75b4-477d-9862-6ceea1e470df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size (50, 1, 2, 1251) (50,)\n",
      "Test size (22, 1, 2, 1251) (22,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print('train size',X_train.shape, y_train.shape)\n",
    "print('Test size',X_test.shape, y_test.shape)\n",
    "\n",
    "X_tensor = torch.tensor(X_test).float()\n",
    "y_tensor = torch.tensor(y_test).long()\n",
    "\n",
    "dataset_vail = TensorDataset(X_tensor, y_tensor)\n",
    "vail_loader = torch.utils.data.DataLoader(dataset_vail, batch_size=32, shuffle=True)\n",
    "\n",
    "X_tensor_train = torch.tensor(X_train).float()\n",
    "y_tensor_train = torch.tensor(y_train).long()\n",
    "\n",
    "\n",
    "\n",
    "dataset = TensorDataset(X_tensor_train, y_tensor_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd57ab05-f027-4df3-8bf0-f18b826dc319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:fs9ba7de) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/train_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/train_loss</td><td>0.74796</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">physionet_gamenet_2ch_2class_cross_vail</strong>: <a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery/runs/fs9ba7de\" target=\"_blank\">https://wandb.ai/nutapol-1997/Motor-Imagery/runs/fs9ba7de</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220907_090731-fs9ba7de/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:fs9ba7de). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nutapolt/eeg_mi/wandb/run-20220907_091122-3g7xgrv0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery/runs/3g7xgrv0\" target=\"_blank\">physionet_gamenet_2ch_2class_cross_vail</a></strong> to <a href=\"https://wandb.ai/nutapol-1997/Motor-Imagery\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "wand = wandb.init(\n",
    "        \n",
    "      # Set the project where this run will be logged\n",
    "      project=\"Motor-Imagery\", \n",
    "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"physionet_gamenet_2ch_2class_cross_vail\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      config={\n",
    "      \"learning_rate\": 0.00001,\n",
    "      \"architecture\": \"gamenet\",\n",
    "      \"dataset\": \"Nutapol T.\",\n",
    "      \"epochs\": 10,\n",
    "      \"weightname\":\"physionet_gamenet_2ch_2class_cross_vail\",\n",
    "       \n",
    "        \n",
    "      }\n",
    "    )\n",
    "\n",
    "config = wand.config\n",
    "#print(config.num_step_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f00c68f4-c3c7-49a3-b150-a929cf68206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net = gamenet().cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1fbfed80-d631-413f-bbe3-a690e40b4de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Num 0\n",
      "Fold 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 1024])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         num_step \u001b[38;5;241m=\u001b[39mmath\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;241m/\u001b[39m batch_size)\n\u001b[1;32m     16\u001b[0m         config\u001b[38;5;241m.\u001b[39mnum_step_per_epoch\u001b[38;5;241m=\u001b[39mnum_step\n\u001b[0;32m---> 17\u001b[0m         train_loss,valid_loss,train_accuracy,valid_accuracy \u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mgpu_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mvail_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvail_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mwand\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwand\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                                                                     \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m         results_per_fold\u001b[38;5;241m.\u001b[39mappend([train_accuracy, valid_accuracy])\n\u001b[1;32m     28\u001b[0m wandb\u001b[38;5;241m.\u001b[39malert(\n\u001b[1;32m     29\u001b[0m             title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinish\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     30\u001b[0m             text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinishing training\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m         )\n",
      "File \u001b[0;32m~/eeg_mi/common.py:78\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, gpu_num, train_loader, test_loader, vail_loader, optimizer, criterion, wand)\u001b[0m\n\u001b[1;32m     75\u001b[0m     classes \u001b[38;5;241m=\u001b[39m classes\u001b[38;5;241m.\u001b[39mcuda(gpu_num)\n\u001b[1;32m     77\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 78\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, classes)\n\u001b[1;32m     81\u001b[0m iter_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [75]\u001b[0m, in \u001b[0;36mgamenet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpooling2(out)\n\u001b[1;32m    111\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(out)\n\u001b[0;32m--> 112\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(out)\n\u001b[1;32m    114\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:2436\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2424\u001b[0m         batch_norm,\n\u001b[1;32m   2425\u001b[0m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2433\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2434\u001b[0m     )\n\u001b[1;32m   2435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m-> 2436\u001b[0m     \u001b[43m_verify_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2439\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[1;32m   2440\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:2404\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2402\u001b[0m     size_prods \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m size[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   2403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_prods \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(size))\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 1024])"
     ]
    }
   ],
   "source": [
    "results_per_fold = []\n",
    "batch_size = 32\n",
    "from common import train\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch_B in range(1):\n",
    "    print(\"Epoch Num {}\".format(epoch_B))\n",
    "    for fold_idx, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "\n",
    "        print('Fold {}'.format(fold_idx + 1))\n",
    "\n",
    "        train_loader, test_loader = setup_dataflow(X_tensor_train,y_tensor_train, train_idx, val_idx)\n",
    "        num_step =math.ceil(len(train_loader.dataset) / batch_size)\n",
    "        config.num_step_per_epoch=num_step\n",
    "        train_loss,valid_loss,train_accuracy,valid_accuracy =train(\n",
    "                                                                model = net,\n",
    "                                                                gpu_num = 0,\n",
    "                                                                train_loader = train_loader,\n",
    "                                                                test_loader = test_loader,\n",
    "                                                                vail_loader = vail_loader,\n",
    "                                                                optimizer = optimizer  ,\n",
    "                                                                criterion = criterion ,\n",
    "                                                                wand = wand\n",
    "                                                                     )\n",
    "        results_per_fold.append([train_accuracy, valid_accuracy])\n",
    "wandb.alert(\n",
    "            title='Finish',\n",
    "            text=f'Finishing training',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee728d9-5484-472b-b052-5edb26585493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
